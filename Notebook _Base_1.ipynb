{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T10:03:11.504852Z",
     "iopub.status.busy": "2025-03-12T10:03:11.504446Z",
     "iopub.status.idle": "2025-03-12T10:03:14.754828Z",
     "shell.execute_reply": "2025-03-12T10:03:14.753274Z",
     "shell.execute_reply.started": "2025-03-12T10:03:11.504802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Hate-Check-Necessity-Suffisciency'...\n",
      "remote: Enumerating objects: 130, done.\u001b[K\n",
      "remote: Counting objects: 100% (130/130), done.\u001b[K\n",
      "remote: Compressing objects: 100% (104/104), done.\u001b[K\n",
      "remote: Total 130 (delta 17), reused 130 (delta 17), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (130/130), 22.52 MiB | 19.38 MiB/s, done.\n",
      "Resolving deltas: 100% (17/17), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone https://github.com/hackimm11/Hate-Check-Necessity-Suffisciency.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T10:02:55.406479Z",
     "iopub.status.busy": "2025-03-12T10:02:55.406124Z",
     "iopub.status.idle": "2025-03-12T10:02:55.526074Z",
     "shell.execute_reply": "2025-03-12T10:02:55.524759Z",
     "shell.execute_reply.started": "2025-03-12T10:02:55.406445Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:23:14.829235Z",
     "iopub.status.busy": "2025-03-18T12:23:14.828939Z",
     "iopub.status.idle": "2025-03-18T12:23:14.834283Z",
     "shell.execute_reply": "2025-03-18T12:23:14.833390Z",
     "shell.execute_reply.started": "2025-03-18T12:23:14.829214Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Hate-Check-Necessity-Suffisciency'\n",
      "/kaggle/working/Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "%cd Hate-Check-Necessity-Suffisciency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preapre data for ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:31:10.974143Z",
     "iopub.status.busy": "2025-03-11T13:31:10.973862Z",
     "iopub.status.idle": "2025-03-11T13:31:10.979781Z",
     "shell.execute_reply": "2025-03-11T13:31:10.978880Z",
     "shell.execute_reply.started": "2025-03-11T13:31:10.974121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/French-Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:34:07.012832Z",
     "iopub.status.busy": "2025-03-07T15:34:07.012407Z",
     "iopub.status.idle": "2025-03-07T15:34:07.017730Z",
     "shell.execute_reply": "2025-03-07T15:34:07.016232Z",
     "shell.execute_reply.started": "2025-03-07T15:34:07.012801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install -r ilm/requirements.txt  # conflict with Train ILM PArt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:30:58.964738Z",
     "iopub.status.busy": "2025-03-11T13:30:58.964419Z",
     "iopub.status.idle": "2025-03-11T13:31:02.858694Z",
     "shell.execute_reply": "2025-03-11T13:31:02.857906Z",
     "shell.execute_reply.started": "2025-03-11T13:30:58.964711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordsegment\n",
      "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wordsegment\n",
      "Successfully installed wordsegment-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:31:18.854374Z",
     "iopub.status.busy": "2025-03-11T13:31:18.854072Z",
     "iopub.status.idle": "2025-03-11T13:31:20.031186Z",
     "shell.execute_reply": "2025-03-11T13:31:20.030426Z",
     "shell.execute_reply.started": "2025-03-11T13:31:18.854346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# #import preprocessor\n",
    "# import pickle\n",
    "# import wordsegment as ws\n",
    "# from html import unescape\n",
    "# import re\n",
    "# import string\n",
    "# ws.load() # load vocab for word segmentation\n",
    "\n",
    "# random_seed = 42\n",
    "\n",
    "# # Cleaning functions from hatecheck-experiments\n",
    "# # Define helper function for segmenting hashtags found through regex\n",
    "# def regex_match_segmentation(match):\n",
    "#     return ' '.join(ws.segment(match.group(0)))\n",
    "\n",
    "# # Define function for cleaning text\n",
    "# def clean_text(text):\n",
    "    \n",
    "#     # convert HTML codes\n",
    "#     text = unescape(text)\n",
    "    \n",
    "#     # lowercase text\n",
    "#     text = text.lower()\n",
    "    \n",
    "#     # replace mentions, URLs and emojis with special token\n",
    "#     text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "#     text = re.sub(r\"u/[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "#     text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    \n",
    "#     # find and split hashtags into words\n",
    "#     text = re.sub(r\"#[A-Za-z0-9]+\", regex_match_segmentation, text)\n",
    "\n",
    "#     # remove punctuation at beginning of string (quirk in Davidson data)\n",
    "#     text = text.lstrip(\"!\")\n",
    "#     text = text.lstrip(\":\")\n",
    "    \n",
    "#     # remove newline and tab characters\n",
    "#     text = text.replace('\\n',' ')\n",
    "#     text = text.replace('\\t',' ')\n",
    "#     text = text.replace('[linebreak]', ' ')\n",
    "    \n",
    "#     return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:31:27.664530Z",
     "iopub.status.busy": "2025-03-11T13:31:27.664117Z",
     "iopub.status.idle": "2025-03-11T13:31:27.734315Z",
     "shell.execute_reply": "2025-03-11T13:31:27.733503Z",
     "shell.execute_reply.started": "2025-03-11T13:31:27.664507Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>dataset</th>\n",
       "      <th>nb_annotators</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>On ne peut pas faire confiance aux musulmans c'est tous des escrocs.</td>\n",
       "      <td>1</td>\n",
       "      <td>islam</td>\n",
       "      <td>NGO</td>\n",
       "      <td>CONAN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Les musulmans ne sont pas dignes de confiance.</td>\n",
       "      <td>1</td>\n",
       "      <td>islam</td>\n",
       "      <td>NGO</td>\n",
       "      <td>CONAN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vous ne devriez pas croire les musulmans.</td>\n",
       "      <td>1</td>\n",
       "      <td>islam</td>\n",
       "      <td>NGO</td>\n",
       "      <td>CONAN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Les musulmans empêchent les gens de manger pendant le ramadan.</td>\n",
       "      <td>1</td>\n",
       "      <td>islam</td>\n",
       "      <td>NGO</td>\n",
       "      <td>CONAN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Les musulmans ne laissent pas les gens s'en prendre à cause du ramadan.</td>\n",
       "      <td>1</td>\n",
       "      <td>islam</td>\n",
       "      <td>NGO</td>\n",
       "      <td>CONAN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      text  \\\n",
       "0     On ne peut pas faire confiance aux musulmans c'est tous des escrocs.   \n",
       "1                           Les musulmans ne sont pas dignes de confiance.   \n",
       "2                                Vous ne devriez pas croire les musulmans.   \n",
       "3           Les musulmans empêchent les gens de manger pendant le ramadan.   \n",
       "4  Les musulmans ne laissent pas les gens s'en prendre à cause du ramadan.   \n",
       "\n",
       "   labels target source dataset  nb_annotators  tweet_id  \n",
       "0       1  islam    NGO   CONAN              1       NaN  \n",
       "1       1  islam    NGO   CONAN              1       NaN  \n",
       "2       1  islam    NGO   CONAN              1       NaN  \n",
       "3       1  islam    NGO   CONAN              1       NaN  \n",
       "4       1  islam    NGO   CONAN              1       NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # from datasets import load_dataset\n",
    "\n",
    "# # ds = load_dataset(\"manueltonneau/french-hate-speech-superset\")\n",
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV file (adjust the encoding if needed)\n",
    "# file_path = \"DATASET/french_dataset.csv\"  # Replace with the actual path\n",
    "\n",
    "# french_train = pd.read_csv(file_path, encoding=\"utf-8\", delimiter=\",\")\n",
    "# french_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:31:33.165757Z",
     "iopub.status.busy": "2025-03-11T13:31:33.165473Z",
     "iopub.status.idle": "2025-03-11T13:31:33.183582Z",
     "shell.execute_reply": "2025-03-11T13:31:33.182614Z",
     "shell.execute_reply.started": "2025-03-11T13:31:33.165736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "      <th>target</th>\n",
       "      <th>source</th>\n",
       "      <th>dataset</th>\n",
       "      <th>nb_annotators</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>Alors julie t'ai dégeulasse de sortir avec une gadji bande de salopes</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>role_playing_game</td>\n",
       "      <td>cyberado</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>allo mme est homophobe</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>role_playing_game</td>\n",
       "      <td>cyberado</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>on est pas ensemble déjà puis même si c'était le cas,  ça ne te regarde pas et je vois pas en quoi ça te concerne</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>role_playing_game</td>\n",
       "      <td>cyberado</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                   text  \\\n",
       "151                                               Alors julie t'ai dégeulasse de sortir avec une gadji bande de salopes   \n",
       "152                                                                                              allo mme est homophobe   \n",
       "154   on est pas ensemble déjà puis même si c'était le cas,  ça ne te regarde pas et je vois pas en quoi ça te concerne   \n",
       "\n",
       "     labels target             source   dataset  nb_annotators  tweet_id  \n",
       "151       0    NaN  role_playing_game  cyberado              2       NaN  \n",
       "152       0    NaN  role_playing_game  cyberado              2       NaN  \n",
       "154       0    NaN  role_playing_game  cyberado              2       NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# french_train_neutral = french_train[french_train['labels'] == 0]\n",
    "# french_train_neutral[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:32:08.805145Z",
     "iopub.status.busy": "2025-03-11T13:32:08.804830Z",
     "iopub.status.idle": "2025-03-11T13:32:08.810088Z",
     "shell.execute_reply": "2025-03-11T13:32:08.809297Z",
     "shell.execute_reply.started": "2025-03-11T13:32:08.805117Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18071"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(french_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:32:59.782048Z",
     "iopub.status.busy": "2025-03-11T13:32:59.781740Z",
     "iopub.status.idle": "2025-03-11T13:33:05.023451Z",
     "shell.execute_reply": "2025-03-11T13:33:05.022548Z",
     "shell.execute_reply.started": "2025-03-11T13:32:59.782026Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# founta_texts = [clean_text(tt) for tt in founta_train_neutral['text'].tolist()]\n",
    "# cad_texts = [clean_text(str(tt)) for tt in cad_train_neutral['text'].tolist()]\n",
    "# french_texts = [clean_text(str(tt)) for tt in french_train_neutral['text'].tolist()]\n",
    "# wiki_texts = [clean_text(tt) for tt in wiki_train_neutral['comment'].tolist()]\n",
    "# civil_texts = [clean_text(tt) for tt in civil_comments_sampled['comment_text'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:33:09.864969Z",
     "iopub.status.busy": "2025-03-11T13:33:09.864634Z",
     "iopub.status.idle": "2025-03-11T13:33:09.873187Z",
     "shell.execute_reply": "2025-03-11T13:33:09.872426Z",
     "shell.execute_reply.started": "2025-03-11T13:33:09.864942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from random import Random\n",
    "\n",
    "# # founta_train, founta_valid = train_test_split(founta_texts, test_size=0.05, random_state=random_seed+1)\n",
    "# # cad_train, cad_valid = train_test_split(cad_texts, test_size=0.05, random_state=random_seed+2)\n",
    "# # wiki_train, wiki_valid = train_test_split(wiki_texts, test_size=0.05, random_state=random_seed+3)\n",
    "# # civil_train, civil_valid = train_test_split(wiki_texts, test_size=0.05, random_state=random_seed+4)\n",
    "# fr_train, fr_valid = train_test_split(french_texts, test_size=0.2, random_state=random_seed+2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:45:57.475932Z",
     "iopub.status.busy": "2025-03-07T15:45:57.475600Z",
     "iopub.status.idle": "2025-03-07T15:45:57.488295Z",
     "shell.execute_reply": "2025-03-07T15:45:57.487481Z",
     "shell.execute_reply.started": "2025-03-07T15:45:57.475906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# compound_train = founta_train + cad_train + wiki_train + civil_train\n",
    "# compound_valid = founta_valid + cad_valid + wiki_valid + civil_valid\n",
    "# Random(random_seed+5).shuffle(compound_train)\n",
    "# Random(random_seed+6).shuffle(compound_valid)\n",
    "# compound_train = cad_train\n",
    "\n",
    "# compound_valid = cad_valid\n",
    "\n",
    "# Random(random_seed+5).shuffle(fr_train)\n",
    "# Random(random_seed+6).shuffle(fr_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:33:17.483360Z",
     "iopub.status.busy": "2025-03-11T13:33:17.483019Z",
     "iopub.status.idle": "2025-03-11T13:33:17.494703Z",
     "shell.execute_reply": "2025-03-11T13:33:17.493884Z",
     "shell.execute_reply.started": "2025-03-11T13:33:17.483333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# with open(\"ilm/data/french_dataset/train.txt\", \"w\") as ff:\n",
    "#     ff.write(\"\\n\\n\\n\".join(fr_train))\n",
    "    \n",
    "# with open(\"ilm/data/french_dataset/valid.txt\", \"w\") as ff:\n",
    "#     ff.write(\"\\n\\n\\n\".join(fr_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ILM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T14:02:41.606472Z",
     "iopub.status.busy": "2025-03-07T14:02:41.606142Z",
     "iopub.status.idle": "2025-03-07T14:02:44.090240Z",
     "shell.execute_reply": "2025-03-07T14:02:44.089111Z",
     "shell.execute_reply.started": "2025-03-07T14:02:41.606435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'French-Hate-Check-Necessity-Suffisciency'...\n",
      "remote: Enumerating objects: 97, done.\u001b[K\n",
      "remote: Counting objects: 100% (97/97), done.\u001b[K\n",
      "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
      "remote: Total 97 (delta 9), reused 97 (delta 9), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (97/97), 10.51 MiB | 15.51 MiB/s, done.\n",
      "Resolving deltas: 100% (9/9), done.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T23:35:33.692826Z",
     "iopub.status.busy": "2025-03-07T23:35:33.692627Z",
     "iopub.status.idle": "2025-03-07T23:35:33.698559Z",
     "shell.execute_reply": "2025-03-07T23:35:33.697725Z",
     "shell.execute_reply.started": "2025-03-07T23:35:33.692807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/French-Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "# !ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T12:28:50.129211Z",
     "iopub.status.busy": "2025-03-12T12:28:50.128877Z",
     "iopub.status.idle": "2025-03-12T12:28:50.134568Z",
     "shell.execute_reply": "2025-03-12T12:28:50.133455Z",
     "shell.execute_reply.started": "2025-03-12T12:28:50.129164Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Hate-Check-Necessity-Suffisciency/ilm\n"
     ]
    }
   ],
   "source": [
    "%cd ilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:36:04.967501Z",
     "iopub.status.busy": "2025-03-07T15:36:04.966979Z",
     "iopub.status.idle": "2025-03-07T15:36:04.972011Z",
     "shell.execute_reply": "2025-03-07T15:36:04.970708Z",
     "shell.execute_reply.started": "2025-03-07T15:36:04.967438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T12:28:52.007112Z",
     "iopub.status.busy": "2025-03-12T12:28:52.006842Z",
     "iopub.status.idle": "2025-03-12T12:28:53.536018Z",
     "shell.execute_reply": "2025-03-12T12:28:53.535243Z",
     "shell.execute_reply.started": "2025-03-12T12:28:52.007093Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import nltk; nltk.download('punkt')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T12:28:56.215260Z",
     "iopub.status.busy": "2025-03-12T12:28:56.214923Z",
     "iopub.status.idle": "2025-03-12T12:29:04.085739Z",
     "shell.execute_reply": "2025-03-12T12:29:04.084942Z",
     "shell.execute_reply.started": "2025-03-12T12:28:56.215230Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///kaggle/working/Hate-Check-Necessity-Suffisciency/ilm\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Installing collected packages: ilm\n",
      "  Running setup.py develop for ilm\n",
      "Successfully installed ilm-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:55:07.039678Z",
     "iopub.status.busy": "2025-03-12T11:55:07.039394Z",
     "iopub.status.idle": "2025-03-12T11:55:15.654248Z",
     "shell.execute_reply": "2025-03-12T11:55:15.653274Z",
     "shell.execute_reply.started": "2025-03-12T11:55:07.039654Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 353065\n",
      " 32%|███████████▍                        | 3352/10519 [00:02<00:04, 1519.83it/s]/kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/tokenize_util.py:182: UserWarning: Encountered empty token\n",
      "  warnings.warn('Encountered empty token')\n",
      "100%|███████████████████████████████████| 10519/10519 [00:07<00:00, 1485.70it/s]\n",
      "Processed 10519 documents and created 15.221218747029186 examples per document (expected 16)\n",
      "Errors which caused retries:\n",
      "* (236334 retries) Issue with example: Mask is not unique\n",
      "Mask rate (characters): 0.2036\n"
     ]
    }
   ],
   "source": [
    "!python create_ilm_examples.py train data/masks/compound_dataset --data_name custom --data_dir \"data/compound_dataset\" --data_split train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T11:55:32.521779Z",
     "iopub.status.busy": "2025-03-12T11:55:32.521457Z",
     "iopub.status.idle": "2025-03-12T11:55:33.819633Z",
     "shell.execute_reply": "2025-03-12T11:55:33.818789Z",
     "shell.execute_reply.started": "2025-03-12T11:55:32.521753Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 591680\n",
      "100%|███████████████████████████████████████| 554/554 [00:00<00:00, 1434.37it/s]\n",
      "Processed 554 documents and created 15.312274368231048 examples per document (expected 16)\n",
      "Errors which caused retries:\n",
      "* (11720 retries) Issue with example: Mask is not unique\n",
      "Mask rate (characters): 0.2031\n"
     ]
    }
   ],
   "source": [
    "!python create_ilm_examples.py valid data/masks/compound_dataset --data_name custom --data_dir \"data/compound_dataset\" --data_split valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:40:37.448160Z",
     "iopub.status.busy": "2025-03-07T15:40:37.447839Z",
     "iopub.status.idle": "2025-03-07T15:40:37.568089Z",
     "shell.execute_reply": "2025-03-07T15:40:37.566928Z",
     "shell.execute_reply.started": "2025-03-07T15:40:37.448132Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 root root 6115419 Mar  7 15:36 data/masks/french_dataset/train.pkl\n"
     ]
    }
   ],
   "source": [
    "# !ls -l data/masks/french_dataset/train.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T15:44:52.553950Z",
     "iopub.status.busy": "2025-03-07T15:44:52.553578Z",
     "iopub.status.idle": "2025-03-07T15:44:52.672157Z",
     "shell.execute_reply": "2025-03-07T15:44:52.670861Z",
     "shell.execute_reply.started": "2025-03-07T15:44:52.553917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm data/masks/my_experiment_output.pkl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T13:36:49.764644Z",
     "iopub.status.busy": "2025-03-11T13:36:49.764363Z",
     "iopub.status.idle": "2025-03-11T13:36:49.891395Z",
     "shell.execute_reply": "2025-03-11T13:36:49.890368Z",
     "shell.execute_reply.started": "2025-03-11T13:36:49.764621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm ../Weights/ILM/eval_num_docs.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T14:54:27.837822Z",
     "iopub.status.busy": "2025-03-07T14:54:27.837141Z",
     "iopub.status.idle": "2025-03-07T14:54:27.959583Z",
     "shell.execute_reply": "2025-03-07T14:54:27.958549Z",
     "shell.execute_reply.started": "2025-03-07T14:54:27.837767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create_ilm_examples.py\t     ilm.egg-info\t      setup.py\n",
      "create_ilm_examples_test.py  preview_ilm_examples.py  train_ilm.py\n",
      "data\t\t\t     README.md\t\t      train_ilm_test.py\n",
      "ilm\t\t\t     requirements.txt\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-07T16:13:08.898160Z",
     "iopub.status.busy": "2025-03-07T16:13:08.897737Z",
     "iopub.status.idle": "2025-03-07T16:13:09.032833Z",
     "shell.execute_reply": "2025-03-07T16:13:09.031563Z",
     "shell.execute_reply.started": "2025-03-07T16:13:08.898130Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train_ilm.py (French adaptation)\n",
      "from enum import Enum\n",
      "from collections import defaultdict\n",
      "import multiprocessing\n",
      "import os\n",
      "import pickle\n",
      "import random\n",
      "import time\n",
      "import warnings\n",
      "\n",
      "import numpy as np\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
      "from tqdm import tqdm\n",
      "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPT2Config, AdamW, CONFIG_NAME, WEIGHTS_NAME\n",
      "try:\n",
      "    import wandb\n",
      "except ImportError:\n",
      "    pass\n",
      "\n",
      "import ilm.constants\n",
      "import ilm.mask\n",
      "import ilm.mask.util\n",
      "import ilm.tokenize_util\n",
      "\n",
      "# Extend the Tokenizer enum to include French option:\n",
      "# (Make sure this matches the changes in tokenize_util.py)\n",
      "# If using our new GPT2_FRENCH, then in the command-line we should set --tokenizer_name \"gpt2_french\"\n",
      "# (our code converts it to upper-case so it must match \"GPT2_FRENCH\")\n",
      "\n",
      "class Task(Enum):\n",
      "    ILM = 0\n",
      "    NO_CONTEXT_ILM = 1\n",
      "    NAIVE = 2\n",
      "    LM = 3\n",
      "    REVERSE_LM = 4\n",
      "\n",
      "class TargetType(Enum):\n",
      "    PAD = 0\n",
      "    CONTEXT = 1\n",
      "    CONTEXT_SPECIAL = 2\n",
      "    CONTEXT_INFILL_SEP = 3\n",
      "    INFILL = 4\n",
      "    INFILL_SPECIAL = 5\n",
      "    INFILL_REDUNDANT = 6\n",
      "\n",
      "def set_random_seed(seed):\n",
      "    random.seed(seed)\n",
      "    np.random.seed(seed)\n",
      "    torch.manual_seed(seed)\n",
      "    torch.cuda.manual_seed_all(seed)\n",
      "\n",
      "_GLOBAL_WORKER_TARGET = None\n",
      "def _worker_target(doc):\n",
      "    return _GLOBAL_WORKER_TARGET(doc)\n",
      "def worker_target_factory(tokenizer, start_infill_id, end_infill_id, mask_type_to_id, sequence_length, task, skip_naive_incomplete):\n",
      "    def fn(doc_and_char_masks):\n",
      "        doc, char_masks = doc_and_char_masks\n",
      "        try:\n",
      "            return doc_and_char_masks_to_input_and_tt(doc, char_masks, tokenizer, start_infill_id, end_infill_id, mask_type_to_id, task, sequence_length, skip_naive_incomplete)\n",
      "        except Exception as e:\n",
      "            print(e)\n",
      "            return None\n",
      "    return fn\n",
      "\n",
      "def doc_and_char_masks_to_input_and_tt(doc, char_masks, tokenizer, start_infill_id, end_infill_id, mask_type_to_id, task, sequence_length, skip_naive_incomplete):\n",
      "    try:\n",
      "        doc_tokens = ilm.tokenize_util.tokenize(doc, tokenizer=tokenizer)\n",
      "        doc_tokens_ids = ilm.tokenize_util.tokens_to_ids(doc_tokens, tokenizer=tokenizer)\n",
      "    except:\n",
      "        doc_tokens = None\n",
      "    tok_masks = []\n",
      "    if doc_tokens is not None:\n",
      "        for char_mask in char_masks:\n",
      "            try:\n",
      "                tok_mask = ilm.mask.util.align_char_mask_to_tokens(doc, doc_tokens, char_mask)\n",
      "            except:\n",
      "                continue\n",
      "            tok_masks.append(tok_mask)\n",
      "    contexts_and_answers = []\n",
      "    for tok_mask in tok_masks:\n",
      "        try:\n",
      "            ca = ilm.mask.util.apply_masked_spans(doc_tokens_ids, tok_mask, mask_type_to_id)\n",
      "        except:\n",
      "            continue\n",
      "        contexts_and_answers.append((tok_mask, ca))\n",
      "    if skip_naive_incomplete:\n",
      "        contexts_and_answers = [(m, (c, a)) for m, (c, a) in contexts_and_answers if (len(c) + 1 + len(doc_tokens_ids) + 1) <= sequence_length]\n",
      "    special_ids = set([start_infill_id, end_infill_id] + list(mask_type_to_id.values()))\n",
      "    inputs = np.zeros((len(contexts_and_answers), sequence_length), dtype=np.uint16)\n",
      "    tts = np.full((len(contexts_and_answers), sequence_length), TargetType.PAD.value, dtype=np.uint8)\n",
      "    for i, (mask, (context, answers)) in enumerate(contexts_and_answers):\n",
      "        example = []\n",
      "        if task in [Task.ILM, Task.NAIVE]:\n",
      "            example += context\n",
      "        context_len = len(example)\n",
      "        example += [start_infill_id]\n",
      "        if task in [Task.ILM, Task.NO_CONTEXT_ILM]:\n",
      "            for mask_type, answer in answers:\n",
      "                example += answer\n",
      "                example += [end_infill_id]\n",
      "        elif task in [Task.NAIVE, Task.LM]:\n",
      "            example += doc_tokens_ids\n",
      "            example += [end_infill_id]\n",
      "        elif task == Task.REVERSE_LM:\n",
      "            example += doc_tokens_ids[::-1]\n",
      "            example += [end_infill_id]\n",
      "        else:\n",
      "            assert False\n",
      "        if len(example) > sequence_length:\n",
      "            example = example[:sequence_length]\n",
      "        context_special_idxs = [l for l, t in enumerate(example) if l < context_len and t in special_ids]\n",
      "        infill_special_idxs = [l for l, t in enumerate(example) if l > context_len and t in special_ids]\n",
      "        if len(example) > 0 and (min(example) < np.iinfo(inputs.dtype).min or max(example) > np.iinfo(inputs.dtype).max):\n",
      "            raise ValueError('Example cannot be stored in numpy array')\n",
      "        inputs[i, :len(example)] = example\n",
      "        tts[i, :context_len] = TargetType.CONTEXT.value\n",
      "        for l in context_special_idxs:\n",
      "            tts[i, l] = TargetType.CONTEXT_SPECIAL.value\n",
      "        tts[i, context_len:context_len+1] = TargetType.CONTEXT_INFILL_SEP.value\n",
      "        if task in [Task.NAIVE, Task.LM, Task.REVERSE_LM]:\n",
      "            tts[i, context_len+1:len(example)] = TargetType.INFILL_REDUNDANT.value\n",
      "            if task == Task.REVERSE_LM:\n",
      "                mask = mask[::-1]\n",
      "            for (_, tok_off, tok_len) in mask:\n",
      "                if task == Task.REVERSE_LM:\n",
      "                    tok_off = (len(doc_tokens_ids) - 1) - (tok_off + tok_len - 1)\n",
      "                tts[i, context_len+1+tok_off:context_len+1+tok_off+tok_len] = TargetType.INFILL.value\n",
      "                tts[i, context_len+1+tok_off+tok_len:context_len+1+tok_off+tok_len+1] = TargetType.INFILL_SPECIAL.value\n",
      "        else:\n",
      "            tts[i, context_len+1:len(example)] = TargetType.INFILL.value\n",
      "            for l in infill_special_idxs:\n",
      "                tts[i, l] = TargetType.INFILL_SPECIAL.value\n",
      "    return inputs, tts\n",
      "\n",
      "def masked_dataset_to_inputs_and_tts(split, tokenizer, start_infill_id, end_infill_id, mask_type_to_id, args):\n",
      "    assert split in ['train', 'eval']\n",
      "    if split == 'train':\n",
      "        examples_tag = args.train_examples_tag\n",
      "        sequence_length = args.train_sequence_length\n",
      "        max_num_examples = args.train_max_num_examples\n",
      "        skip_naive_incomplete = args.train_skip_naive_incomplete\n",
      "    else:\n",
      "        examples_tag = args.eval_examples_tag\n",
      "        sequence_length = args.eval_sequence_length\n",
      "        max_num_examples = args.eval_max_num_examples\n",
      "        skip_naive_incomplete = args.eval_skip_naive_incomplete\n",
      "\n",
      "    with open(os.path.join(args.examples_dir, '{}.pkl'.format(examples_tag)), 'rb') as f:\n",
      "        dataset = pickle.load(f)\n",
      "    num_docs = len(dataset)\n",
      "    global _GLOBAL_WORKER_TARGET\n",
      "    _GLOBAL_WORKER_TARGET = worker_target_factory(tokenizer, start_infill_id, end_infill_id, mask_type_to_id, sequence_length, Task[args.task.upper()], skip_naive_incomplete)\n",
      "    with multiprocessing.Pool(args.data_loader_num_workers) as p:\n",
      "        docs_inputs_and_tts = list(tqdm(p.imap(_worker_target, dataset), total=len(dataset)))\n",
      "    inputs = np.concatenate([i for i, _ in docs_inputs_and_tts], axis=0)\n",
      "    tts = np.concatenate([t for _, t in docs_inputs_and_tts], axis=0)\n",
      "    if max_num_examples is not None:\n",
      "        set_random_seed(args.seed)\n",
      "        example_ids = random.sample(list(range(inputs.shape[0])), max_num_examples)\n",
      "        inputs = np.take(inputs, example_ids, axis=0)\n",
      "        tts = np.take(tts, example_ids, axis=0)\n",
      "    return inputs, tts, num_docs\n",
      "\n",
      "def tts_to_labels(inputs, tts, label_tts):\n",
      "    selector = torch.zeros_like(inputs, dtype=torch.bool)\n",
      "    for tt in label_tts:\n",
      "        selector |= tts == tt.value\n",
      "    return torch.where(selector, inputs, torch.full_like(inputs, -1))\n",
      "\n",
      "def train(args):\n",
      "    n_gpu = torch.cuda.device_count()\n",
      "    if n_gpu == 0:\n",
      "        warnings.warn('No GPU detected. Training on CPU will be very slow')\n",
      "    elif n_gpu > 1:\n",
      "        warnings.warn('This codebase is not optimized for multi GPU usage')\n",
      "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "    example_tag_to_fp = lambda tag: os.path.join(args.examples_dir, '{}.pkl'.format(tag))\n",
      "    out_fn_to_fp = lambda fn: os.path.join(args.train_dir, fn)\n",
      "    os.makedirs(args.train_dir, exist_ok=True)\n",
      "    resuming = os.path.exists(out_fn_to_fp('step.pkl'))\n",
      "    tokenizer = ilm.tokenize_util.Tokenizer[args.tokenizer_name.upper()]\n",
      "    if tokenizer == ilm.tokenize_util.Tokenizer.CUSTOM:\n",
      "        ilm.tokenize_util.set_custom_vocab_fp(args.tokenizer_custom_vocab_fp)\n",
      "    base_vocab_size = ilm.tokenize_util.vocab_size(tokenizer)\n",
      "    start_infill_id = base_vocab_size + 0\n",
      "    end_infill_id = base_vocab_size + 1\n",
      "    additional_ids_to_tokens = {\n",
      "        start_infill_id: '<|startofinfill|>',\n",
      "        end_infill_id: '<|endofinfill|>'\n",
      "    }\n",
      "    mask_cls = ilm.mask.util.mask_cls_str_to_type(args.mask_cls)\n",
      "    mask_types = mask_cls.mask_types()\n",
      "    mask_type_to_id = {}\n",
      "    for i, t in enumerate(mask_types):\n",
      "        t_id = base_vocab_size + 2 + i\n",
      "        t_tok = '<|infill_{}|>'.format(mask_cls.mask_type_serialize(t))\n",
      "        additional_ids_to_tokens[t_id] = t_tok\n",
      "        mask_type_to_id[t] = t_id\n",
      "    print(additional_ids_to_tokens)\n",
      "    vocab_size = ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
      "    with open(out_fn_to_fp('additional_ids_to_tokens.pkl'), 'wb') as f:\n",
      "        pickle.dump(additional_ids_to_tokens, f)\n",
      "    if not args.eval_only:\n",
      "        print('Loading training data')\n",
      "        loaded_from_cache = False\n",
      "        if args.data_cache:\n",
      "            try:\n",
      "                train_inputs = np.load(out_fn_to_fp('train_inp.npy'))\n",
      "                train_tts = np.load(out_fn_to_fp('train_tts.npy'))\n",
      "                with open(out_fn_to_fp('train_num_docs.pkl'), 'rb') as f:\n",
      "                    train_num_docs = pickle.load(f)\n",
      "                loaded_from_cache = True\n",
      "            except:\n",
      "                pass\n",
      "        if not loaded_from_cache:\n",
      "            train_inputs, train_tts, train_num_docs = masked_dataset_to_inputs_and_tts('train', tokenizer, start_infill_id, end_infill_id, mask_type_to_id, args)\n",
      "            if args.data_cache:\n",
      "                np.save(out_fn_to_fp('train_inp.npy'), train_inputs)\n",
      "                np.save(out_fn_to_fp('train_tts.npy'), train_tts)\n",
      "                with open(out_fn_to_fp('train_num_docs.pkl'), 'wb') as f:\n",
      "                    pickle.dump(train_num_docs, f)\n",
      "        train_tt_to_count = {TargetType(k):v for k, v in zip(*np.unique(train_tts, return_counts=True))}\n",
      "        print(train_tt_to_count)\n",
      "        num_unmasked = train_tt_to_count.get(TargetType.CONTEXT, 0)\n",
      "        num_masked = train_tt_to_count.get(TargetType.INFILL, 0)\n",
      "        print('Mask rate (tokens): {:.4f}'.format(num_masked / (num_unmasked + num_masked)))\n",
      "        print('{} documents, {} examples'.format(train_num_docs, train_inputs.shape[0]))\n",
      "        print(train_inputs.shape, train_inputs.dtype, train_tts.shape, train_tts.dtype)\n",
      "        train_data = TensorDataset(torch.from_numpy(train_inputs.astype(np.int64)), torch.from_numpy(train_tts))\n",
      "        del train_inputs\n",
      "        del train_tts\n",
      "    print('Loading eval data')\n",
      "    loaded_from_cache = False\n",
      "    if args.data_cache:\n",
      "        try:\n",
      "            eval_inputs = np.load(out_fn_to_fp('eval_inp.npy'))\n",
      "            eval_tts = np.load(out_fn_to_fp('eval_tts.npy'))\n",
      "            with open(out_fn_to_fp('eval_num_docs.pkl'), 'rb') as f:\n",
      "                eval_num_docs = pickle.load(f)\n",
      "            loaded_from_cache = True\n",
      "        except:\n",
      "            pass\n",
      "    if not loaded_from_cache:\n",
      "        eval_inputs, eval_tts, eval_num_docs = masked_dataset_to_inputs_and_tts('eval', tokenizer, start_infill_id, end_infill_id, mask_type_to_id, args)\n",
      "        if args.data_cache:\n",
      "            np.save(out_fn_to_fp('eval_inp.npy'), eval_inputs)\n",
      "            np.save(out_fn_to_fp('eval_tts.npy'), eval_tts)\n",
      "            with open(out_fn_to_fp('eval_num_docs.pkl'), 'wb') as f:\n",
      "                pickle.dump(eval_num_docs, f)\n",
      "    eval_tt_to_count = {TargetType(k):v for k, v in zip(*np.unique(eval_tts, return_counts=True))}\n",
      "    print(eval_tt_to_count)\n",
      "    num_unmasked = eval_tt_to_count.get(TargetType.CONTEXT, 0)\n",
      "    num_masked = eval_tt_to_count.get(TargetType.INFILL, 0)\n",
      "    print('Mask rate (tokens): {:.4f}'.format(num_masked / (num_unmasked + num_masked)))\n",
      "    print('{} documents, {} examples'.format(eval_num_docs, eval_inputs.shape[0]))\n",
      "    print(eval_inputs.shape, eval_inputs.dtype, eval_tts.shape, eval_tts.dtype)\n",
      "    eval_data = TensorDataset(torch.from_numpy(eval_inputs.astype(np.int64)), torch.from_numpy(eval_tts))\n",
      "    del eval_inputs\n",
      "    del eval_tts\n",
      "    if args.train_num_epochs is not None:\n",
      "        train_num_batches = int(float(train_num_docs * args.train_num_epochs) / args.train_batch_size)\n",
      "        if train_num_batches == 0:\n",
      "            return\n",
      "        print('Maximum number of training steps: {}'.format(train_num_batches / args.train_batch_accumulation))\n",
      "    print('Creating datasets')\n",
      "    if not args.eval_only:\n",
      "        train_sampler = RandomSampler(train_data)\n",
      "        train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n",
      "    eval_sampler = SequentialSampler(eval_data)\n",
      "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size, drop_last=True)\n",
      "    print('Initializing model...')\n",
      "    set_random_seed(args.seed)\n",
      "    if args.model_name in ilm.constants.GPT2_MODEL_NAMES:\n",
      "        model_type = GPT2LMHeadModel\n",
      "        cfg_type = GPT2Config\n",
      "    if resuming:\n",
      "        print('from saved checkpoint (resuming)')\n",
      "        model = model_type.from_pretrained(args.train_dir)\n",
      "    else:\n",
      "        if args.train_from_scratch:\n",
      "            print('from scratch')\n",
      "            cfg = cfg_type.from_pretrained(args.model_name)\n",
      "            model = model_type(cfg)\n",
      "        else:\n",
      "            print('from pretrained checkpoint')\n",
      "            model = model_type.from_pretrained(args.model_name)\n",
      "    model.resize_token_embeddings(vocab_size)\n",
      "    model.to(device)\n",
      "    model.train()\n",
      "    if not args.eval_only:\n",
      "        params = list(model.named_parameters())\n",
      "        no_decay = ['bias', 'ln']\n",
      "        optimizer_grouped_parameters = [\n",
      "            {'params': [p for n, p in params if not any(nd in n for nd in no_decay)],\n",
      "             'weight_decay': args.train_weight_decay},\n",
      "            {'params': [p for n, p in params if any(nd in n for nd in no_decay)],\n",
      "             'weight_decay': 0.0}\n",
      "        ]\n",
      "        optimizer = AdamW(optimizer_grouped_parameters, lr=args.train_learning_rate, eps=args.train_adam_epsilon)\n",
      "        if resuming:\n",
      "            optimizer.load_state_dict(torch.load(out_fn_to_fp('optimizer.pt')))\n",
      "    if resuming:\n",
      "        try:\n",
      "            with open(out_fn_to_fp('step.pkl'), 'rb') as f:\n",
      "                step = pickle.load(f)\n",
      "        except Exception as e:\n",
      "            if args.eval_only:\n",
      "                step = None\n",
      "            else:\n",
      "                raise e\n",
      "    else:\n",
      "        step = 0\n",
      "    if args.eval_only:\n",
      "        print('Evaluating')\n",
      "        model.eval()\n",
      "        eval_start = time.time()\n",
      "        eval_token_counts = defaultdict(int)\n",
      "        eval_token_loss_sums = defaultdict(float)\n",
      "        for i, eval_batch in enumerate(eval_dataloader):\n",
      "            with torch.no_grad():\n",
      "                eval_inputs, eval_tts = tuple(t.to(device) for t in eval_batch)\n",
      "                eval_logits, _ = model(eval_inputs)\n",
      "                eval_logits_relevant = eval_logits[:, :-1].contiguous().view(-1, eval_logits.shape[-1])\n",
      "                for tag, tts in [\n",
      "                    ('context', [TargetType.CONTEXT]),\n",
      "                    ('infill', [TargetType.INFILL, TargetType.INFILL_SPECIAL]),\n",
      "                    ('infill_textonly', [TargetType.INFILL])]:\n",
      "                    eval_labels = tts_to_labels(eval_inputs, eval_tts, tts)\n",
      "                    eval_labels_relevant = eval_labels[:, 1:]\n",
      "                    eval_labels_relevant_count = (eval_labels_relevant != -1).long().sum().item()\n",
      "                    eval_labels_loss = F.cross_entropy(\n",
      "                        eval_logits_relevant,\n",
      "                        eval_labels_relevant.contiguous().view(-1),\n",
      "                        ignore_index=-1).item()\n",
      "                    eval_token_counts[tag] += eval_labels_relevant_count\n",
      "                    eval_token_loss_sums[tag] += eval_labels_loss * eval_labels_relevant_count\n",
      "        eval_dict = {}\n",
      "        for tag, count in eval_token_counts.items():\n",
      "            loss = eval_token_loss_sums[tag]\n",
      "            if count > 0:\n",
      "                loss /= count\n",
      "            eval_dict['eval_{}_count'.format(tag)] = count\n",
      "            eval_dict['eval_{}_loss'.format(tag)] = loss\n",
      "            eval_dict['eval_{}_ppl'.format(tag)] = np.exp(loss)\n",
      "        eval_dict['eval_time'] = time.time() - eval_start\n",
      "        print('-' * 80)\n",
      "        if step is not None:\n",
      "            print('(Step {}) Eval'.format(step))\n",
      "        for k, v in eval_dict.items():\n",
      "            print('{}: {}'.format(k, v))\n",
      "        if args.wandb:\n",
      "            wandb.log(eval_dict, step=step)\n",
      "    else:\n",
      "        print('Training')\n",
      "        set_random_seed(args.seed)\n",
      "        best_eval_loss = None\n",
      "        num_save = -1\n",
      "        num_summary = -1\n",
      "        num_batches_complete = step * args.train_batch_accumulation\n",
      "        start = time.time()\n",
      "        while True:\n",
      "            if args.train_num_epochs is not None and num_batches_complete >= train_num_batches:\n",
      "                break\n",
      "            for batch in train_dataloader:\n",
      "                if args.train_num_epochs is not None and num_batches_complete >= train_num_batches:\n",
      "                    break\n",
      "                elapsed = time.time() - start\n",
      "                if int(elapsed / args.train_eval_secs) > num_save:\n",
      "                    num_save = int(elapsed / args.train_eval_secs)\n",
      "                    model.eval()\n",
      "                    eval_start = time.time()\n",
      "                    eval_token_counts = defaultdict(int)\n",
      "                    eval_token_loss_sums = defaultdict(float)\n",
      "                    for i, eval_batch in enumerate(eval_dataloader):\n",
      "                        with torch.no_grad():\n",
      "                            eval_inputs, eval_tts = tuple(t.to(device) for t in eval_batch)\n",
      "                            eval_logits, _ = model(eval_inputs)\n",
      "                            eval_logits_relevant = eval_logits[:, :-1].contiguous().view(-1, eval_logits.shape[-1])\n",
      "                            for tag, tts in [\n",
      "                                ('context', [TargetType.CONTEXT]),\n",
      "                                ('infill', [TargetType.INFILL, TargetType.INFILL_SPECIAL]),\n",
      "                                ('infill_textonly', [TargetType.INFILL])]:\n",
      "                                eval_labels = tts_to_labels(eval_inputs, eval_tts, tts)\n",
      "                                eval_labels_relevant = eval_labels[:, 1:]\n",
      "                                eval_labels_relevant_count = (eval_labels_relevant != -1).long().sum().item()\n",
      "                                eval_labels_loss = F.cross_entropy(\n",
      "                                    eval_logits_relevant,\n",
      "                                    eval_labels_relevant.contiguous().view(-1),\n",
      "                                    ignore_index=-1).item()\n",
      "                                eval_token_counts[tag] += eval_labels_relevant_count\n",
      "                                eval_token_loss_sums[tag] += eval_labels_loss * eval_labels_relevant_count\n",
      "                    eval_dict = {}\n",
      "                    for tag, count in eval_token_counts.items():\n",
      "                        loss = eval_token_loss_sums[tag]\n",
      "                        if count > 0:\n",
      "                            loss /= count\n",
      "                        eval_dict['eval_{}_count'.format(tag)] = count\n",
      "                        eval_dict['eval_{}_loss'.format(tag)] = loss\n",
      "                    eval_dict['eval_time'] = time.time() - eval_start\n",
      "                    print('-' * 80)\n",
      "                    print('(Step {}) Eval'.format(step))\n",
      "                    for k, v in eval_dict.items():\n",
      "                        print('{}: {}'.format(k, v))\n",
      "                    if args.wandb:\n",
      "                        wandb.log(eval_dict, step=step)\n",
      "                    if best_eval_loss is None or eval_dict['eval_infill_loss'] < best_eval_loss:\n",
      "                        print('Saving')\n",
      "                        model_to_save = model.module if hasattr(model, 'module') else model\n",
      "                        model_to_save.config.to_json_file(out_fn_to_fp(CONFIG_NAME))\n",
      "                        torch.save(model_to_save.state_dict(), out_fn_to_fp(WEIGHTS_NAME))\n",
      "                        torch.save(optimizer.state_dict(), out_fn_to_fp('optimizer.pt'))\n",
      "                        with open(out_fn_to_fp('step.pkl'), 'wb') as f:\n",
      "                            pickle.dump(step, f)\n",
      "                        best_eval_loss = eval_dict['eval_infill_loss']\n",
      "                    model.train()\n",
      "                inputs, tts = tuple(t.to(device) for t in batch)\n",
      "                labels_context = tts_to_labels(inputs, tts, [TargetType.CONTEXT])\n",
      "                labels_infill = tts_to_labels(inputs, tts, [TargetType.INFILL, TargetType.INFILL_SPECIAL, TargetType.INFILL_REDUNDANT])\n",
      "                logits, _ = model(inputs)\n",
      "                logits_relevant = logits[:, :-1].contiguous().view(-1, logits.shape[-1])\n",
      "                loss_context = F.cross_entropy(\n",
      "                    logits_relevant,\n",
      "                    labels_context[:, 1:].contiguous().view(-1),\n",
      "                    ignore_index=-1)\n",
      "                loss_infill = F.cross_entropy(\n",
      "                    logits_relevant,\n",
      "                    labels_infill[:, 1:].contiguous().view(-1),\n",
      "                    ignore_index=-1)\n",
      "                loss_context_item = loss_context.item()\n",
      "                loss_infill_item = loss_infill.item()\n",
      "                loss = loss_infill\n",
      "                if args.train_context:\n",
      "                    loss += loss_context\n",
      "                if args.train_batch_accumulation != 1:\n",
      "                    loss /= float(args.train_batch_accumulation)\n",
      "                loss.backward()\n",
      "                if int((num_batches_complete + 1) % args.train_batch_accumulation) == 0:\n",
      "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.train_max_grad_norm)\n",
      "                    optimizer.step()\n",
      "                    optimizer.zero_grad()\n",
      "                    step += 1\n",
      "                num_batches_complete += 1\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    from argparse import ArgumentParser\n",
      "\n",
      "    parser = ArgumentParser()\n",
      "\n",
      "    parser.add_argument('experiment_name', type=str)\n",
      "    parser.add_argument('train_dir', type=str)\n",
      "    parser.add_argument('examples_dir', type=str)\n",
      "    parser.add_argument('--seed', type=int)\n",
      "    parser.add_argument('--wandb', action='store_true', dest='wandb')\n",
      "    parser.add_argument('--wandb_project_name', type=str)\n",
      "\n",
      "    mask_args = parser.add_argument_group('Mask')\n",
      "    mask_args.add_argument('--mask_cls', type=str)\n",
      "\n",
      "    tokenizer_args = parser.add_argument_group('Tokenizer')\n",
      "    tokenizer_args.add_argument('--tokenizer_name', type=str, choices=[t.name.lower() for t in ilm.tokenize_util.Tokenizer])\n",
      "    tokenizer_args.add_argument('--tokenizer_custom_vocab_fp', type=str)\n",
      "\n",
      "    task_args = parser.add_argument_group('Task')\n",
      "    task_args.add_argument('--task', type=str, choices=[t.name.lower() for t in Task])\n",
      "\n",
      "    data_args = parser.add_argument_group('Data')\n",
      "    data_args.add_argument('--data_no_cache', action='store_false', dest='data_cache')\n",
      "    data_args.add_argument('--data_loader_num_workers', type=int)\n",
      "\n",
      "    model_args = parser.add_argument_group('Model')\n",
      "    model_args.add_argument('--model_name', type=str, choices=ilm.constants.GPT2_MODEL_NAMES)\n",
      "\n",
      "    train_args = parser.add_argument_group('Train')\n",
      "    train_args.add_argument('--train_examples_tag', type=str)\n",
      "    train_args.add_argument('--train_max_num_examples', type=int)\n",
      "    train_args.add_argument('--train_num_epochs', type=int)\n",
      "    train_args.add_argument('--train_from_scratch', action='store_true', dest='train_from_scratch')\n",
      "    train_args.add_argument('--train_batch_size', type=int)\n",
      "    train_args.add_argument('--train_batch_accumulation', type=int)\n",
      "    train_args.add_argument('--train_sequence_length', type=int)\n",
      "    train_args.add_argument('--train_skip_naive_incomplete', action='store_true', dest='train_skip_naive_incomplete')\n",
      "    train_args.add_argument('--train_eval_secs', type=float)\n",
      "    train_args.add_argument('--train_summary_secs', type=float)\n",
      "    train_args.add_argument('--train_minimal_supervision', action='store_false', dest='train_context')\n",
      "    train_args.add_argument('--train_learning_rate', type=float)\n",
      "    train_args.add_argument('--train_weight_decay', type=float)\n",
      "    train_args.add_argument('--train_adam_epsilon', type=float)\n",
      "    train_args.add_argument('--train_max_grad_norm', type=float)\n",
      "\n",
      "    eval_args = parser.add_argument_group('Eval')\n",
      "    eval_args.add_argument('--eval_only', action='store_true', dest='eval_only')\n",
      "    eval_args.add_argument('--eval_examples_tag', type=str)\n",
      "    eval_args.add_argument('--eval_max_num_examples', type=int)\n",
      "    eval_args.add_argument('--eval_batch_size', type=int)\n",
      "    eval_args.add_argument('--eval_sequence_length', type=int)\n",
      "    eval_args.add_argument('--eval_skip_naive_incomplete', action='store_true', dest='eval_skip_naive_incomplete')\n",
      "\n",
      "    parser.set_defaults(\n",
      "        seed=None,\n",
      "        wandb=False,\n",
      "        wandb_project_name='ilm',\n",
      "        mask_cls='ilm.mask.hierarchical.MaskHierarchical',\n",
      "        tokenizer_name='gpt2_french',  # CHANGE THIS TO \"gpt2_french\" when using French\n",
      "        tokenizer_custom_vocab_fp=None,\n",
      "        task='ilm',\n",
      "        data_cache=True,\n",
      "        data_loader_num_workers=4,\n",
      "        model_name='dbddv01/gpt2-french-small',  # CHANGE THIS TO the French model name, e.g. \"dbddv01/gpt2-french-small\"\n",
      "        train_examples_tag='train',\n",
      "        train_max_num_examples=None,\n",
      "        train_num_epochs=None,\n",
      "        train_from_scratch=False,\n",
      "        train_batch_size=8,\n",
      "        train_batch_accumulation=3,\n",
      "        train_sequence_length=256,\n",
      "        train_skip_naive_incomplete=False,\n",
      "        train_eval_secs=360,\n",
      "        train_summary_secs=360,\n",
      "        train_context=True,\n",
      "        train_learning_rate=5e-5,\n",
      "        train_weight_decay=0.,\n",
      "        train_adam_epsilon=1e-8,\n",
      "        train_max_grad_norm=1.,\n",
      "        eval_only=False,\n",
      "        eval_examples_tag='valid',\n",
      "        eval_max_num_examples=None,\n",
      "        eval_batch_size=8,\n",
      "        eval_sequence_length=256,\n",
      "        eval_skip_naive_incomplete=False)\n",
      "  \n",
      "    args = parser.parse_args()\n",
      "\n",
      "    if args.wandb:\n",
      "        wandb.init(project=args.wandb_project_name, name=args.experiment_name)\n",
      "        wandb.config.update(args)\n",
      "\n",
      "    if args.seed is None:\n",
      "        args.seed = random.randint(0, int(1e6))\n",
      "    print('Random seed {}'.format(args.seed))\n",
      "\n",
      "    train(args)\n"
     ]
    }
   ],
   "source": [
    "!cat train_ilm.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T12:37:55.522778Z",
     "iopub.status.busy": "2025-03-12T12:37:55.522415Z",
     "iopub.status.idle": "2025-03-12T12:37:55.532713Z",
     "shell.execute_reply": "2025-03-12T12:37:55.531990Z",
     "shell.execute_reply.started": "2025-03-12T12:37:55.522754Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_content = \"\"\"from enum import Enum\n",
    "from collections import defaultdict\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, AdamW, CONFIG_NAME, WEIGHTS_NAME\n",
    "try:\n",
    "  import wandb\n",
    "except:\n",
    "  pass\n",
    "\n",
    "import ilm.constants\n",
    "import ilm.mask\n",
    "import ilm.mask.util\n",
    "import ilm.tokenize_util\n",
    "\n",
    "class Task(Enum):\n",
    "  # Example: She ate <?> for <?><S>cereal<E>breakfast<E>\n",
    "  ILM = 0\n",
    "  # Example: <S>cereal<E>breakfast<E>\n",
    "  NO_CONTEXT_ILM = 1\n",
    "  # Example: She ate <?> for <?><S>She ate cereal for breakfast<E>\n",
    "  NAIVE = 2\n",
    "  # Example: <S>She ate cereal for breakfast<E>\n",
    "  LM = 3\n",
    "  # Example: <S>breakfast for cereal ate She<E>\n",
    "  REVERSE_LM = 4\n",
    "  # TODO: NAIVE with no stopwords?\n",
    "\n",
    "\n",
    "class TargetType(Enum):\n",
    "  PAD = 0\n",
    "  CONTEXT = 1\n",
    "  CONTEXT_SPECIAL = 2\n",
    "  CONTEXT_INFILL_SEP = 3\n",
    "  INFILL = 4\n",
    "  INFILL_SPECIAL = 5\n",
    "  INFILL_REDUNDANT = 6\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "# NOTE: Multiprocessing pickle/closure issue workaround\n",
    "_GLOBAL_WORKER_TARGET = None\n",
    "def _worker_target(doc):\n",
    "  return _GLOBAL_WORKER_TARGET(doc)\n",
    "def worker_target_factory(\n",
    "    tokenizer,\n",
    "    start_infill_id,\n",
    "    end_infill_id,\n",
    "    mask_type_to_id,\n",
    "    sequence_length,\n",
    "    task,\n",
    "    skip_naive_incomplete):\n",
    "  def fn(doc_and_char_masks):\n",
    "    doc, char_masks = doc_and_char_masks\n",
    "    try:\n",
    "      return doc_and_char_masks_to_input_and_tt(\n",
    "          doc,\n",
    "          char_masks,\n",
    "          tokenizer,\n",
    "          start_infill_id,\n",
    "          end_infill_id,\n",
    "          mask_type_to_id,\n",
    "          task,\n",
    "          sequence_length,\n",
    "          skip_naive_incomplete)\n",
    "    except Exception as e:\n",
    "      print(e)\n",
    "      return None\n",
    "  return fn\n",
    "\n",
    "\n",
    "def doc_and_char_masks_to_input_and_tt(\n",
    "    doc,\n",
    "    char_masks,\n",
    "    tokenizer,\n",
    "    start_infill_id,\n",
    "    end_infill_id,\n",
    "    mask_type_to_id,\n",
    "    task,\n",
    "    sequence_length,\n",
    "    skip_naive_incomplete):\n",
    "  # Tokenize document\n",
    "  try:\n",
    "    doc_tokens = ilm.tokenize_util.tokenize(doc, tokenizer=tokenizer)\n",
    "    doc_tokens_ids = ilm.tokenize_util.tokens_to_ids(doc_tokens, tokenizer=tokenizer)\n",
    "  except:\n",
    "    doc_tokens = None\n",
    "    #error_to_count['Failed to tokenize document'] += len(char_masks)\n",
    "\n",
    "  # Align character masks to tokens\n",
    "  tok_masks = []\n",
    "  if doc_tokens is not None:\n",
    "    for char_mask in char_masks:\n",
    "      try:\n",
    "        tok_mask = ilm.mask.util.align_char_mask_to_tokens(doc, doc_tokens, char_mask)\n",
    "      except:\n",
    "        #error_to_count['Failed to align character-level mask to tokens'] += 1\n",
    "        continue\n",
    "      tok_masks.append(tok_mask)\n",
    "\n",
    "  # Apply masks\n",
    "  contexts_and_answers = []\n",
    "  for tok_mask in tok_masks:\n",
    "    try:\n",
    "      ca = ilm.mask.util.apply_masked_spans(\n",
    "          doc_tokens_ids,\n",
    "          tok_mask,\n",
    "          mask_type_to_id)\n",
    "    except:\n",
    "      #error_to_count['Failed to apply mask'] += 1\n",
    "      continue\n",
    "    contexts_and_answers.append((tok_mask, ca))\n",
    "\n",
    "  # Skip examples that would be incomplete for Task.NAIVE (typically the longest task)\n",
    "  if skip_naive_incomplete:\n",
    "    contexts_and_answers = [(m, (c, a)) for m, (c, a) in contexts_and_answers if (len(c) + 1 + len(doc_tokens_ids) + 1) <= sequence_length]\n",
    "\n",
    "  special_ids = set([start_infill_id, end_infill_id] + list(mask_type_to_id.values()))\n",
    "\n",
    "  inputs = np.zeros((len(contexts_and_answers), sequence_length), dtype=np.uint16)\n",
    "  tts = np.full((len(contexts_and_answers), sequence_length), TargetType.PAD.value, dtype=np.uint8)\n",
    "  for i, (mask, (context, answers)) in enumerate(contexts_and_answers):\n",
    "    # Create example\n",
    "    example = []\n",
    "\n",
    "    # (Masked) Context\n",
    "    if task in [Task.ILM, Task.NAIVE]:\n",
    "      # Example: She ate <?> for <?>\n",
    "      example += context\n",
    "\n",
    "    # Context / answer separator\n",
    "    context_len = len(example)\n",
    "    # Example: <S>\n",
    "    example += [start_infill_id]\n",
    "\n",
    "    # Answers\n",
    "    if task in [Task.ILM, Task.NO_CONTEXT_ILM]:\n",
    "      # Example: cereal<E>breakfast<E>\n",
    "      for mask_type, answer in answers:\n",
    "        example += answer\n",
    "        example += [end_infill_id]\n",
    "    elif task in [Task.NAIVE, Task.LM]:\n",
    "      # Example: She ate cereal for breakfast<E>\n",
    "      example += doc_tokens_ids\n",
    "      example += [end_infill_id]\n",
    "    elif task == Task.REVERSE_LM:\n",
    "      # Example: breakfast for cereal ate She<E>\n",
    "      example += doc_tokens_ids[::-1]\n",
    "      example += [end_infill_id]\n",
    "    else:\n",
    "      assert False\n",
    "\n",
    "    if len(example) > sequence_length:\n",
    "      example = example[:sequence_length]\n",
    "      #warning_to_count['Example longer than sequence length'] += 1\n",
    "\n",
    "    # Find special tokens\n",
    "    context_special_idxs = [l for l, t in enumerate(example) if l < context_len and t in special_ids]\n",
    "    infill_special_idxs = [l for l, t in enumerate(example) if l > context_len and t in special_ids]\n",
    "\n",
    "    # Store example in output array\n",
    "    if len(example) > 0 and (min(example) < np.iinfo(inputs.dtype).min or max(example) > np.iinfo(inputs.dtype).max):\n",
    "      raise ValueError('Example cannot be stored in numpy array')\n",
    "    inputs[i, :len(example)] = example\n",
    "\n",
    "    # Store target types in output array\n",
    "    tts[i, :context_len] = TargetType.CONTEXT.value\n",
    "    for l in context_special_idxs:\n",
    "      tts[i, l] = TargetType.CONTEXT_SPECIAL.value\n",
    "    tts[i, context_len:context_len+1] = TargetType.CONTEXT_INFILL_SEP.value\n",
    "    if task in [Task.NAIVE, Task.LM, Task.REVERSE_LM]:\n",
    "      tts[i, context_len+1:len(example)] = TargetType.INFILL_REDUNDANT.value\n",
    "      if task == Task.REVERSE_LM:\n",
    "        mask = mask[::-1]\n",
    "      for (_, tok_off, tok_len) in mask:\n",
    "        if task == Task.REVERSE_LM:\n",
    "          tok_off = (len(doc_tokens_ids) - 1) - (tok_off + tok_len - 1)\n",
    "        tts[i, context_len+1+tok_off:context_len+1+tok_off+tok_len] = TargetType.INFILL.value\n",
    "        tts[i, context_len+1+tok_off+tok_len:context_len+1+tok_off+tok_len+1] = TargetType.INFILL_SPECIAL.value\n",
    "    else:\n",
    "      tts[i, context_len+1:len(example)] = TargetType.INFILL.value\n",
    "      for l in infill_special_idxs:\n",
    "        tts[i, l] = TargetType.INFILL_SPECIAL.value\n",
    "\n",
    "  return inputs, tts\n",
    "\n",
    "\n",
    "def masked_dataset_to_inputs_and_tts(\n",
    "    split,\n",
    "    tokenizer,\n",
    "    start_infill_id,\n",
    "    end_infill_id,\n",
    "    mask_type_to_id,\n",
    "    args):\n",
    "  assert split in ['train', 'eval']\n",
    "  if split == 'train':\n",
    "    examples_tag = args.train_examples_tag\n",
    "    sequence_length = args.train_sequence_length\n",
    "    max_num_examples = args.train_max_num_examples\n",
    "    skip_naive_incomplete = args.train_skip_naive_incomplete\n",
    "  else:\n",
    "    examples_tag = args.eval_examples_tag\n",
    "    sequence_length = args.eval_sequence_length\n",
    "    max_num_examples = args.eval_max_num_examples\n",
    "    skip_naive_incomplete = args.eval_skip_naive_incomplete\n",
    "\n",
    "  with open(os.path.join(args.examples_dir, '{}.pkl'.format(examples_tag)), 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "  num_docs = len(dataset)\n",
    "\n",
    "  # Mask and tokenize documents\n",
    "  global _GLOBAL_WORKER_TARGET\n",
    "  _GLOBAL_WORKER_TARGET = worker_target_factory(\n",
    "      tokenizer,\n",
    "      start_infill_id,\n",
    "      end_infill_id,\n",
    "      mask_type_to_id,\n",
    "      sequence_length,\n",
    "      Task[args.task.upper()],\n",
    "      skip_naive_incomplete)\n",
    "  with multiprocessing.Pool(args.data_loader_num_workers) as p:\n",
    "    docs_inputs_and_tts = list(tqdm(\n",
    "      p.imap(_worker_target, dataset),\n",
    "      total=len(dataset)))\n",
    "\n",
    "  inputs = np.concatenate([i for i, _ in docs_inputs_and_tts], axis=0)\n",
    "  tts = np.concatenate([t for _, t in docs_inputs_and_tts], axis=0)\n",
    "\n",
    "  # TODO: Don't bother doing all the work if we're not going to use it\n",
    "  if max_num_examples is not None:\n",
    "    set_random_seed(args.seed)\n",
    "    example_ids = random.sample(list(range(inputs.shape[0])), max_num_examples)\n",
    "    inputs = np.take(inputs, example_ids, axis=0)\n",
    "    tts = np.take(tts, example_ids, axis=0)\n",
    "\n",
    "  return inputs, tts, num_docs\n",
    "\n",
    "\n",
    "def tts_to_labels(inputs, tts, label_tts):\n",
    "  selector = torch.zeros_like(inputs, dtype=torch.bool)\n",
    "  for tt in label_tts:\n",
    "    selector |= tts == tt.value\n",
    "  return torch.where(\n",
    "      selector,\n",
    "      inputs,\n",
    "      torch.full_like(inputs, -1))\n",
    "\n",
    "\n",
    "def train(args):\n",
    "  # Init device\n",
    "  n_gpu = torch.cuda.device_count()\n",
    "  if n_gpu == 0:\n",
    "    warnings.warn('No GPU detected. Training on CPU will be very slow')\n",
    "  elif n_gpu > 1:\n",
    "    warnings.warn('This codebase is not optimized for multi GPU usage')\n",
    "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "  # Lambda for filenames\n",
    "  example_tag_to_fp = lambda tag: os.path.join(args.examples_dir, '{}.pkl'.format(tag))\n",
    "  out_fn_to_fp = lambda fn: os.path.join(args.train_dir, fn)\n",
    "\n",
    "  # Create training dir\n",
    "  os.makedirs(args.train_dir, exist_ok=True)\n",
    "  resuming = os.path.exists(out_fn_to_fp('step.pkl'))\n",
    "\n",
    "  # Create tokenizer\n",
    "  tokenizer = ilm.tokenize_util.Tokenizer[args.tokenizer_name.upper()]\n",
    "  if tokenizer == ilm.tokenize_util.Tokenizer.CUSTOM:\n",
    "    ilm.tokenize_util.set_custom_vocab_fp(args.tokenizer_custom_vocab_fp)\n",
    "\n",
    "  # Update tokenizer\n",
    "  base_vocab_size = ilm.tokenize_util.vocab_size(tokenizer)\n",
    "  start_infill_id = base_vocab_size + 0\n",
    "  end_infill_id = base_vocab_size + 1\n",
    "  additional_ids_to_tokens = {\n",
    "      start_infill_id: '<|startofinfill|>',\n",
    "      end_infill_id: '<|endofinfill|>'\n",
    "  }\n",
    "  mask_cls = ilm.mask.util.mask_cls_str_to_type(args.mask_cls)\n",
    "  mask_types = mask_cls.mask_types()\n",
    "  mask_type_to_id = {}\n",
    "  for i, t in enumerate(mask_types):\n",
    "    t_id = base_vocab_size + 2 + i\n",
    "    t_tok = '<|infill_{}|>'.format(mask_cls.mask_type_serialize(t))\n",
    "    additional_ids_to_tokens[t_id] = t_tok\n",
    "    mask_type_to_id[t] = t_id\n",
    "  print(additional_ids_to_tokens)\n",
    "  vocab_size = ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "  with open(out_fn_to_fp('additional_ids_to_tokens.pkl'), 'wb') as f:\n",
    "    pickle.dump(additional_ids_to_tokens, f)\n",
    "\n",
    "  # Load training data\n",
    "  if not args.eval_only:\n",
    "    print('Loading training data')\n",
    "    loaded_from_cache = False\n",
    "    if args.data_cache:\n",
    "      try:\n",
    "        train_inputs = np.load(out_fn_to_fp('train_inp.npy'))\n",
    "        train_tts = np.load(out_fn_to_fp('train_tts.npy'))\n",
    "        with open(out_fn_to_fp('train_num_docs.pkl'), 'rb') as f:\n",
    "          train_num_docs = pickle.load(f)\n",
    "        loaded_from_cache = True\n",
    "      except:\n",
    "        pass\n",
    "    if not loaded_from_cache:\n",
    "      train_inputs, train_tts, train_num_docs = masked_dataset_to_inputs_and_tts(\n",
    "          'train',\n",
    "          tokenizer,\n",
    "          start_infill_id,\n",
    "          end_infill_id,\n",
    "          mask_type_to_id,\n",
    "          args)\n",
    "      if args.data_cache:\n",
    "        np.save(out_fn_to_fp('train_inp.npy'), train_inputs)\n",
    "        np.save(out_fn_to_fp('train_tts.npy'), train_tts)\n",
    "        with open(out_fn_to_fp('train_num_docs.pkl'), 'wb') as f:\n",
    "          pickle.dump(train_num_docs, f)\n",
    "    train_tt_to_count = {TargetType(k):v for k, v in zip(*np.unique(train_tts, return_counts=True))}\n",
    "    print(train_tt_to_count)\n",
    "    num_unmasked = train_tt_to_count.get(TargetType.CONTEXT, 0)\n",
    "    num_masked = train_tt_to_count.get(TargetType.INFILL, 0)\n",
    "    print('Mask rate (tokens): {:.4f}'.format(num_masked / (num_unmasked + num_masked)))\n",
    "    print('{} documents, {} examples'.format(train_num_docs, train_inputs.shape[0]))\n",
    "    print(train_inputs.shape, train_inputs.dtype, train_tts.shape, train_tts.dtype)\n",
    "    train_data = TensorDataset(\n",
    "        torch.from_numpy(train_inputs.astype(np.int64)),\n",
    "        torch.from_numpy(train_tts))\n",
    "    del train_inputs\n",
    "    del train_tts\n",
    "\n",
    "  # Load eval data\n",
    "  print('Loading eval data')\n",
    "  loaded_from_cache = False\n",
    "  if args.data_cache:\n",
    "    try:\n",
    "      eval_inputs = np.load(out_fn_to_fp('eval_inp.npy'))\n",
    "      eval_tts = np.load(out_fn_to_fp('eval_tts.npy'))\n",
    "      with open(out_fn_to_fp('eval_num_docs.pkl'), 'rb') as f:\n",
    "        eval_num_docs = pickle.load(f)\n",
    "      loaded_from_cache = True\n",
    "    except:\n",
    "      pass\n",
    "  if not loaded_from_cache:\n",
    "    eval_inputs, eval_tts, eval_num_docs = masked_dataset_to_inputs_and_tts(\n",
    "        'eval',\n",
    "        tokenizer,\n",
    "        start_infill_id,\n",
    "        end_infill_id,\n",
    "        mask_type_to_id,\n",
    "        args)\n",
    "    if args.data_cache:\n",
    "      np.save(out_fn_to_fp('eval_inp.npy'), eval_inputs)\n",
    "      np.save(out_fn_to_fp('eval_tts.npy'), eval_tts)\n",
    "      with open(out_fn_to_fp('eval_num_docs.pkl'), 'wb') as f:\n",
    "        pickle.dump(eval_num_docs, f)\n",
    "  eval_tt_to_count = {TargetType(k):v for k, v in zip(*np.unique(eval_tts, return_counts=True))}\n",
    "  print(eval_tt_to_count)\n",
    "  num_unmasked = eval_tt_to_count.get(TargetType.CONTEXT, 0)\n",
    "  num_masked = eval_tt_to_count.get(TargetType.INFILL, 0)\n",
    "  print('Mask rate (tokens): {:.4f}'.format(num_masked / (num_unmasked + num_masked)))\n",
    "  print('{} documents, {} examples'.format(eval_num_docs, eval_inputs.shape[0]))\n",
    "  print(eval_inputs.shape, eval_inputs.dtype, eval_tts.shape, eval_tts.dtype)\n",
    "  eval_data = TensorDataset(\n",
    "      torch.from_numpy(eval_inputs.astype(np.int64)),\n",
    "      torch.from_numpy(eval_tts))\n",
    "  del eval_inputs\n",
    "  del eval_tts\n",
    "\n",
    "  # Calculate number of steps to train for (return if we're just pre-cacheing data)\n",
    "  if args.train_num_epochs is not None:\n",
    "    train_num_batches = int(float(train_num_docs * args.train_num_epochs) / args.train_batch_size)\n",
    "    if train_num_batches == 0:\n",
    "      return\n",
    "    print('Maximum number of training steps: {}'.format(train_num_batches / args.train_batch_accumulation))\n",
    "\n",
    "  # Create data iterators\n",
    "  print('Creating datasets')\n",
    "  if not args.eval_only:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args.train_batch_size, drop_last=True)\n",
    "  eval_sampler = SequentialSampler(eval_data)\n",
    "  eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size, drop_last=True)\n",
    "\n",
    "  # Load model\n",
    "  print('Initializing model...')\n",
    "  set_random_seed(args.seed)\n",
    "  if args.model_name in ilm.constants.GPT2_MODEL_NAMES:\n",
    "    model_type = GPT2LMHeadModel\n",
    "    cfg_type = GPT2Config\n",
    "  if resuming:\n",
    "    print('from saved checkpoint (resuming)')\n",
    "    model = model_type.from_pretrained(args.train_dir)\n",
    "  else:\n",
    "    if args.train_from_scratch:\n",
    "      print('from scratch')\n",
    "      cfg = cfg_type.from_pretrained(args.model_name)\n",
    "      model = model_type(cfg)\n",
    "    else:\n",
    "      print('from pretrained checkpoint')\n",
    "      model = model_type.from_pretrained(args.model_name)\n",
    "  model.resize_token_embeddings(vocab_size)\n",
    "  model.to(device)\n",
    "  model.train()\n",
    "\n",
    "  # Reset random seed in case model init triggered RNG\n",
    "\n",
    "  # Initialize optimizers\n",
    "  if not args.eval_only:\n",
    "    params = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'ln']\n",
    "    optimizer_grouped_parameters = [\n",
    "      {\n",
    "        'params': [p for n, p in params if not any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': args.train_weight_decay\n",
    "      },\n",
    "      {\n",
    "        'params': [p for n, p in params if any(nd in n for nd in no_decay)],\n",
    "        'weight_decay': 0.0\n",
    "      }\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters,\n",
    "        lr=args.train_learning_rate,\n",
    "        eps=args.train_adam_epsilon)\n",
    "    if resuming:\n",
    "      optimizer.load_state_dict(torch.load(out_fn_to_fp('optimizer.pt')))\n",
    "\n",
    "  # Create global step\n",
    "  if resuming:\n",
    "    try:\n",
    "      with open(out_fn_to_fp('step.pkl'), 'rb') as f:\n",
    "        step = pickle.load(f)\n",
    "    except Exception as e:\n",
    "      if args.eval_only:\n",
    "        step = None\n",
    "      else:\n",
    "        raise e\n",
    "  else:\n",
    "    step = 0\n",
    "\n",
    "  if args.eval_only:\n",
    "    print('Evaluating')\n",
    "    model.eval()\n",
    "\n",
    "    eval_start = time.time()\n",
    "    eval_token_counts = defaultdict(int)\n",
    "    eval_token_loss_sums = defaultdict(float)\n",
    "    for i, eval_batch in enumerate(eval_dataloader):\n",
    "      with torch.no_grad():\n",
    "        eval_inputs, eval_tts = tuple(t.to(device) for t in eval_batch)\n",
    "        eval_logits= model(eval_inputs)\n",
    "        eval_logits = eval_logits[0]\n",
    "        # print(\"eval_logits\", eval_logits)\n",
    "        eval_logits_relevant = eval_logits[:, :-1].contiguous().view(-1, eval_logits.shape[-1])\n",
    "\n",
    "        for tag, tts in [\n",
    "            ('context', [TargetType.CONTEXT]),\n",
    "            ('infill', [TargetType.INFILL, TargetType.INFILL_SPECIAL]),\n",
    "            ('infill_textonly', [TargetType.INFILL])]:\n",
    "          eval_labels = tts_to_labels(eval_inputs, eval_tts, tts)\n",
    "          eval_labels_relevant = eval_labels[:, 1:]\n",
    "          eval_labels_relevant_count = (eval_labels_relevant != -1).long().sum().item()\n",
    "          eval_labels_loss = F.cross_entropy(\n",
    "              eval_logits_relevant,\n",
    "              eval_labels_relevant.contiguous().view(-1),\n",
    "              ignore_index=-1).item()\n",
    "          eval_token_counts[tag] += eval_labels_relevant_count\n",
    "          eval_token_loss_sums[tag] += eval_labels_loss * eval_labels_relevant_count\n",
    "\n",
    "    eval_dict = {}\n",
    "    for tag, count in eval_token_counts.items():\n",
    "      loss = eval_token_loss_sums[tag]\n",
    "      if count > 0:\n",
    "        loss /= count\n",
    "      eval_dict['eval_{}_count'.format(tag)] = count\n",
    "      eval_dict['eval_{}_loss'.format(tag)] = loss\n",
    "      eval_dict['eval_{}_ppl'.format(tag)] = np.exp(loss)\n",
    "    eval_dict['eval_time'] = time.time() - eval_start\n",
    "\n",
    "    print('-' * 80)\n",
    "    if step is not None:\n",
    "      print('(Step {}) Eval'.format(step))\n",
    "    for k, v in eval_dict.items():\n",
    "      print('{}: {}'.format(k, v))\n",
    "    if args.wandb:\n",
    "      wandb.log(eval_dict, step=step)\n",
    "\n",
    "  else:\n",
    "    print('Training')\n",
    "    set_random_seed(args.seed)\n",
    "    best_eval_loss = None\n",
    "    num_save = -1\n",
    "    num_summary = -1\n",
    "    num_batches_complete = step * args.train_batch_accumulation\n",
    "    start = time.time()\n",
    "    while True:\n",
    "      if args.train_num_epochs is not None and num_batches_complete >= train_num_batches:\n",
    "        break\n",
    "\n",
    "      for batch in train_dataloader:\n",
    "        if args.train_num_epochs is not None and num_batches_complete >= train_num_batches:\n",
    "          break\n",
    "\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        # Evaluate\n",
    "        if int(elapsed / args.train_eval_secs) > num_save:\n",
    "          num_save = int(elapsed / args.train_eval_secs)\n",
    "\n",
    "          model.eval()\n",
    "\n",
    "          eval_start = time.time()\n",
    "          eval_token_counts = defaultdict(int)\n",
    "          eval_token_loss_sums = defaultdict(float)\n",
    "          for i, eval_batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "              eval_inputs, eval_tts = tuple(t.to(device) for t in eval_batch)\n",
    "              eval_logits = model(eval_inputs)\n",
    "              eval_logits = eval_logits[0]\n",
    "              # print('eval_logits', eval_logits)\n",
    "              eval_logits_relevant = eval_logits[:, :-1].contiguous().view(-1, eval_logits.shape[-1])\n",
    "\n",
    "              for tag, tts in [\n",
    "                  ('context', [TargetType.CONTEXT]),\n",
    "                  ('infill', [TargetType.INFILL, TargetType.INFILL_SPECIAL]),\n",
    "                  ('infill_textonly', [TargetType.INFILL])]:\n",
    "                eval_labels = tts_to_labels(eval_inputs, eval_tts, tts)\n",
    "                eval_labels_relevant = eval_labels[:, 1:]\n",
    "                eval_labels_relevant_count = (eval_labels_relevant != -1).long().sum().item()\n",
    "                eval_labels_loss = F.cross_entropy(\n",
    "                    eval_logits_relevant,\n",
    "                    eval_labels_relevant.contiguous().view(-1),\n",
    "                    ignore_index=-1).item()\n",
    "                eval_token_counts[tag] += eval_labels_relevant_count\n",
    "                eval_token_loss_sums[tag] += eval_labels_loss * eval_labels_relevant_count\n",
    "\n",
    "          eval_dict = {}\n",
    "          for tag, count in eval_token_counts.items():\n",
    "            loss = eval_token_loss_sums[tag]\n",
    "            if count > 0:\n",
    "              loss /= count\n",
    "            eval_dict['eval_{}_count'.format(tag)] = count\n",
    "            eval_dict['eval_{}_loss'.format(tag)] = loss\n",
    "          eval_dict['eval_time'] = time.time() - eval_start\n",
    "\n",
    "          print('-' * 80)\n",
    "          print('(Step {}) Eval'.format(step))\n",
    "          for k, v in eval_dict.items():\n",
    "            print('{}: {}'.format(k, v))\n",
    "          if args.wandb:\n",
    "            wandb.log(eval_dict, step=step)\n",
    "\n",
    "          if best_eval_loss is None or eval_dict['eval_infill_loss'] < best_eval_loss:\n",
    "            print('Saving')\n",
    "            model_to_save = model.module if hasattr(model, 'module') else model\n",
    "            model_to_save.config.to_json_file(out_fn_to_fp(CONFIG_NAME))\n",
    "            torch.save(model_to_save.state_dict(), out_fn_to_fp(WEIGHTS_NAME))\n",
    "            torch.save(optimizer.state_dict(), out_fn_to_fp('optimizer.pt'))\n",
    "            with open(out_fn_to_fp('step.pkl'), 'wb') as f:\n",
    "              pickle.dump(step, f)\n",
    "            best_eval_loss = eval_dict['eval_infill_loss']\n",
    "\n",
    "          model.train()\n",
    "\n",
    "        # Train\n",
    "        inputs, tts = tuple(t.to(device) for t in batch)\n",
    "        # TODO: Option to train on CONTEXT_SPECIAL?\n",
    "        labels_context = tts_to_labels(inputs, tts, [TargetType.CONTEXT])\n",
    "        # TODO: Option to skip training on INFILL_REDUNDANT?\n",
    "        # NOTE: This would give Task.NAIVE/Task.LM less supervision overall but put them more in line with the supervision that Task.ILM and Task.NO_CONTEXT_ILM receive\n",
    "        labels_infill = tts_to_labels(inputs, tts, [TargetType.INFILL, TargetType.INFILL_SPECIAL, TargetType.INFILL_REDUNDANT])\n",
    "        logits= model(inputs)\n",
    "        logits = logits[0]\n",
    "        logits_relevant = logits[:, :-1].contiguous().view(-1, logits.shape[-1])\n",
    "        loss_context = F.cross_entropy(\n",
    "            logits_relevant,\n",
    "            labels_context[:, 1:].contiguous().view(-1),\n",
    "            ignore_index=-1)\n",
    "        loss_infill = F.cross_entropy(\n",
    "            logits_relevant,\n",
    "            labels_infill[:, 1:].contiguous().view(-1),\n",
    "            ignore_index=-1)\n",
    "\n",
    "        loss_context_item = loss_context.item()\n",
    "        loss_infill_item = loss_infill.item()\n",
    "\n",
    "        loss = loss_infill\n",
    "        if args.train_context:\n",
    "          loss += loss_context\n",
    "\n",
    "        if args.train_batch_accumulation != 1:\n",
    "          loss /= float(args.train_batch_accumulation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Summarize\n",
    "        if int(elapsed / args.train_summary_secs) > num_summary:\n",
    "          num_summary = int(elapsed / args.train_summary_secs)\n",
    "\n",
    "          print('-' * 80)\n",
    "          print('(Step {}) Summary'.format(step))\n",
    "          print(loss_context_item)\n",
    "          print(loss_infill_item)\n",
    "          with torch.no_grad():\n",
    "            for t in inputs, labels_context, labels_infill:\n",
    "              t0 = list(t[0].cpu().numpy())\n",
    "              # print('-' * 40)\n",
    "              # print(t0)\n",
    "            for t in inputs, labels_context, labels_infill:\n",
    "              t0 = list(t[0].cpu().numpy())\n",
    "              # print('-' * 40)\n",
    "              # print(ilm.tokenize_util.decode([0 if t == -1 else t for t in t0], tokenizer))\n",
    "\n",
    "          if args.wandb:\n",
    "            wandb.log({\n",
    "              'loss_context': loss_context_item,\n",
    "              'loss_infill': loss_infill_item,\n",
    "            }, step=step)\n",
    "\n",
    "        if ((num_batches_complete + 1) % args.train_batch_accumulation) == 0:\n",
    "          torch.nn.utils.clip_grad_norm_(model.parameters(), args.train_max_grad_norm)\n",
    "          optimizer.step()\n",
    "          optimizer.zero_grad()\n",
    "          step += 1\n",
    "\n",
    "        num_batches_complete += 1\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  from argparse import ArgumentParser\n",
    "\n",
    "  parser = ArgumentParser()\n",
    "\n",
    "  parser.add_argument('experiment_name', type=str)\n",
    "  parser.add_argument('train_dir', type=str)\n",
    "  parser.add_argument('examples_dir', type=str)\n",
    "  parser.add_argument('--seed', type=int)\n",
    "  parser.add_argument('--wandb', action='store_true', dest='wandb')\n",
    "  parser.add_argument('--wandb_project_name', type=str)\n",
    "\n",
    "  mask_args = parser.add_argument_group('Mask')\n",
    "  mask_args.add_argument('--mask_cls', type=str)\n",
    "\n",
    "  tokenizer_args = parser.add_argument_group('Tokenizer')\n",
    "  tokenizer_args.add_argument('--tokenizer_name', type=str, choices=[t.name.lower() for t in ilm.tokenize_util.Tokenizer])\n",
    "  tokenizer_args.add_argument('--tokenizer_custom_vocab_fp', type=str)\n",
    "\n",
    "  task_args = parser.add_argument_group('Task')\n",
    "  task_args.add_argument('--task', type=str, choices=[t.name.lower() for t in Task])\n",
    "\n",
    "  data_args = parser.add_argument_group('Data')\n",
    "  data_args.add_argument('--data_no_cache', action='store_false', dest='data_cache')\n",
    "  data_args.add_argument('--data_loader_num_workers', type=int)\n",
    "\n",
    "  model_args = parser.add_argument_group('Model')\n",
    "  model_args.add_argument('--model_name', type=str, choices=ilm.constants.GPT2_MODEL_NAMES)\n",
    "\n",
    "  train_args = parser.add_argument_group('Train')\n",
    "  train_args.add_argument('--train_examples_tag', type=str)\n",
    "  train_args.add_argument('--train_max_num_examples', type=int)\n",
    "  train_args.add_argument('--train_num_epochs', type=int)\n",
    "  train_args.add_argument('--train_from_scratch', action='store_true', dest='train_from_scratch')\n",
    "  train_args.add_argument('--train_batch_size', type=int)\n",
    "  train_args.add_argument('--train_batch_accumulation', type=int)\n",
    "  train_args.add_argument('--train_sequence_length', type=int)\n",
    "  train_args.add_argument('--train_skip_naive_incomplete', action='store_true', dest='train_skip_naive_incomplete')\n",
    "  train_args.add_argument('--train_eval_secs', type=float)\n",
    "  train_args.add_argument('--train_summary_secs', type=float)\n",
    "  train_args.add_argument('--train_minimal_supervision', action='store_false', dest='train_context')\n",
    "  train_args.add_argument('--train_learning_rate', type=float)\n",
    "  train_args.add_argument('--train_weight_decay', type=float)\n",
    "  train_args.add_argument('--train_adam_epsilon', type=float)\n",
    "  train_args.add_argument('--train_max_grad_norm', type=float)\n",
    "\n",
    "  eval_args = parser.add_argument_group('Eval')\n",
    "  eval_args.add_argument('--eval_only', action='store_true', dest='eval_only')\n",
    "  eval_args.add_argument('--eval_examples_tag', type=str)\n",
    "  eval_args.add_argument('--eval_max_num_examples', type=int)\n",
    "  eval_args.add_argument('--eval_batch_size', type=int)\n",
    "  eval_args.add_argument('--eval_sequence_length', type=int)\n",
    "  eval_args.add_argument('--eval_skip_naive_incomplete', action='store_true', dest='eval_skip_naive_incomplete')\n",
    "\n",
    "  parser.set_defaults(\n",
    "      seed=None,\n",
    "      wandb=False,\n",
    "      wandb_project_name='ilm',\n",
    "      mask_cls='ilm.mask.hierarchical.MaskHierarchical',\n",
    "      tokenizer_name='gpt2',\n",
    "      tokenizer_custom_vocab_fp=None,\n",
    "      task='ilm',\n",
    "      data_cache=True,\n",
    "      data_loader_num_workers=4,\n",
    "      model_name='gpt2',\n",
    "      train_examples_tag='train',\n",
    "      train_max_num_examples=None,\n",
    "      train_num_epochs=None,\n",
    "      train_from_scratch=False,\n",
    "      train_batch_size=8,\n",
    "      train_batch_accumulation=3,\n",
    "      train_sequence_length=256,\n",
    "      train_skip_naive_incomplete=False,\n",
    "      train_eval_secs=360,\n",
    "      train_summary_secs=360,\n",
    "      train_context=True,\n",
    "      train_learning_rate=5e-5,\n",
    "      train_weight_decay=0.,\n",
    "      train_adam_epsilon=1e-8,\n",
    "      train_max_grad_norm=1.,\n",
    "      eval_only=False,\n",
    "      eval_examples_tag='valid',\n",
    "      eval_max_num_examples=None,\n",
    "      eval_batch_size=8,\n",
    "      eval_sequence_length=256,\n",
    "      eval_skip_naive_incomplete=False)\n",
    "  \n",
    "  args = parser.parse_args()\n",
    "\n",
    "  if args.wandb:\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        name=args.experiment_name)\n",
    "    wandb.config.update(args)\n",
    "\n",
    "  if args.seed is None:\n",
    "    args.seed = random.randint(0, int(1e6))\n",
    "  print('Random seed {}'.format(args.seed))\n",
    "\n",
    "  train(args)\n",
    "\n",
    "\"\"\"\n",
    "with open(\"train_ilm.py\", \"w\") as f:\n",
    "    f.write(new_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T12:38:02.116635Z",
     "iopub.status.busy": "2025-03-12T12:38:02.116355Z",
     "iopub.status.idle": "2025-03-12T14:29:29.950067Z",
     "shell.execute_reply": "2025-03-12T14:29:29.948864Z",
     "shell.execute_reply.started": "2025-03-12T12:38:02.116613Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 12:38:06.185407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-12 12:38:06.207800: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-12 12:38:06.214358: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Random seed 765483\n",
      "/kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/train_ilm.py:271: UserWarning: This codebase is not optimized for multi GPU usage\n",
      "  warnings.warn('This codebase is not optimized for multi GPU usage')\n",
      "{50257: '<|startofinfill|>', 50258: '<|endofinfill|>', 50259: '<|infill_document|>', 50260: '<|infill_paragraph|>', 50261: '<|infill_sentence|>', 50262: '<|infill_ngram|>', 50263: '<|infill_word|>'}\n",
      "Loading training data\n",
      "{<TargetType.PAD: 0>: 33922272, <TargetType.CONTEXT: 1>: 4503857, <TargetType.CONTEXT_SPECIAL: 2>: 476206, <TargetType.CONTEXT_INFILL_SEP: 3>: 156856, <TargetType.INFILL: 4>: 1081679, <TargetType.INFILL_SPECIAL: 5>: 429242}\n",
      "Mask rate (tokens): 0.1937\n",
      "10519 documents, 158477 examples\n",
      "(158477, 256) uint16 (158477, 256) uint8\n",
      "Loading eval data\n",
      "{<TargetType.PAD: 0>: 108502, <TargetType.CONTEXT: 1>: 15113, <TargetType.CONTEXT_SPECIAL: 2>: 1671, <TargetType.CONTEXT_INFILL_SEP: 3>: 508, <TargetType.INFILL: 4>: 3760, <TargetType.INFILL_SPECIAL: 5>: 1518}\n",
      "Mask rate (tokens): 0.1992\n",
      "554 documents, 512 examples\n",
      "(512, 256) uint16 (512, 256) uint8\n",
      "Maximum number of training steps: 3506.3333333333335\n",
      "Creating datasets\n",
      "Initializing model...\n",
      "from saved checkpoint (resuming)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/train_ilm.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(out_fn_to_fp('optimizer.pt')))\n",
      "Training\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 0) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.79981306692438\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 8.870780347919501\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 6.28047354855436\n",
      "eval_time: 13.067899942398071\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 0) Summary\n",
      "6.244888782501221\n",
      "8.578877449035645\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 192) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.293923032993227\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 4.317303021778976\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.625726698941373\n",
      "eval_time: 14.579496622085571\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 192) Summary\n",
      "4.690519332885742\n",
      "4.724985122680664\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 381) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.138704140439891\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 4.1986710400959\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.66819859844573\n",
      "eval_time: 15.059385061264038\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 381) Summary\n",
      "4.5469889640808105\n",
      "4.04802942276001\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 570) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.101588402913959\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 4.05431047132766\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.442485110430007\n",
      "eval_time: 14.606146097183228\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 570) Summary\n",
      "3.7542002201080322\n",
      "3.390310287475586\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 759) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.062761201060437\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.9577739739698097\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.288374753708535\n",
      "eval_time: 14.75139307975769\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 759) Summary\n",
      "3.908486843109131\n",
      "3.5105583667755127\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 947) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.042107525547827\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.9051547807109497\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.1851374289456835\n",
      "eval_time: 14.980766773223877\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 947) Summary\n",
      "3.7800519466400146\n",
      "4.458726406097412\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1136) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.044638098463219\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.855767486717901\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.132191960545296\n",
      "eval_time: 14.983236074447632\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1136) Summary\n",
      "3.8234102725982666\n",
      "4.31428861618042\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1325) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.035003912476482\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.821906092854421\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.109559678333871\n",
      "eval_time: 14.886316537857056\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1325) Summary\n",
      "4.331749439239502\n",
      "3.711585521697998\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1514) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.029431324847368\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.774551087367169\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 5.050509546728844\n",
      "eval_time: 14.971893548965454\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1514) Summary\n",
      "3.162519931793213\n",
      "3.3333945274353027\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1703) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.016668783649183\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.7079287315237113\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.919378763944545\n",
      "eval_time: 14.748197555541992\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1703) Summary\n",
      "4.147838592529297\n",
      "3.511960029602051\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1892) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.0278295426282344\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.676127914408474\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.950992288614842\n",
      "eval_time: 14.718429565429688\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 1892) Summary\n",
      "4.224733352661133\n",
      "3.088031768798828\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2080) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.026335127571534\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.6545922530334347\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.8931506004105225\n",
      "eval_time: 14.918832063674927\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2080) Summary\n",
      "3.7584872245788574\n",
      "3.2029008865356445\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2270) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.028038276064937\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.6312285398795505\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.834148289548589\n",
      "eval_time: 14.71483325958252\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2270) Summary\n",
      "3.687326192855835\n",
      "3.247570276260376\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2458) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.031916606739563\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.576674202302497\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.796369212231737\n",
      "eval_time: 15.074804544448853\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2458) Summary\n",
      "4.0712151527404785\n",
      "3.0550458431243896\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2647) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.025279853594948\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.5606197400181614\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.761785101383291\n",
      "eval_time: 15.042556047439575\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2647) Summary\n",
      "3.8182156085968018\n",
      "3.379640817642212\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2836) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.028227773703656\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.56616701904505\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.770718183479411\n",
      "eval_time: 14.986095428466797\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 2836) Summary\n",
      "3.9711408615112305\n",
      "2.2008659839630127\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3027) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.036485416434952\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.5224016605791335\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.67678841574395\n",
      "eval_time: 15.088888883590698\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3027) Summary\n",
      "3.8287034034729004\n",
      "3.3728036880493164\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3216) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.0492724624882115\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.591542660535037\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.799150810469972\n",
      "eval_time: 15.126247882843018\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3216) Summary\n",
      "3.7067813873291016\n",
      "3.114478349685669\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3407) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.042979372143502\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.493228409813407\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.677716520175021\n",
      "eval_time: 14.680437088012695\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3407) Summary\n",
      "3.8380861282348633\n",
      "2.5803439617156982\n"
     ]
    }
   ],
   "source": [
    "# # $DATASET=\"data/french_dataset\"; $TRAIN_DIR=\"../Weights/ILM\"; $EXAMPLES_DIR=\"data/masks/french_dataset\"; \n",
    "!python train_ilm.py compound_dataset ../Weights/ILM  data/masks/compound_dataset --train_examples_tag train --eval_examples_tag valid --train_num_epochs 8 --eval_max_num_examples 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:49:13.422168Z",
     "iopub.status.busy": "2025-03-12T14:49:13.421819Z",
     "iopub.status.idle": "2025-03-12T15:47:43.818947Z",
     "shell.execute_reply": "2025-03-12T15:47:43.817830Z",
     "shell.execute_reply.started": "2025-03-12T14:49:13.422134Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-12 14:49:17.514745: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-12 14:49:17.535602: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-12 14:49:17.541943: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "Random seed 88985\n",
      "/kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/train_ilm.py:271: UserWarning: This codebase is not optimized for multi GPU usage\n",
      "  warnings.warn('This codebase is not optimized for multi GPU usage')\n",
      "{50257: '<|startofinfill|>', 50258: '<|endofinfill|>', 50259: '<|infill_document|>', 50260: '<|infill_paragraph|>', 50261: '<|infill_sentence|>', 50262: '<|infill_ngram|>', 50263: '<|infill_word|>'}\n",
      "Loading training data\n",
      "{<TargetType.PAD: 0>: 33922272, <TargetType.CONTEXT: 1>: 4503857, <TargetType.CONTEXT_SPECIAL: 2>: 476206, <TargetType.CONTEXT_INFILL_SEP: 3>: 156856, <TargetType.INFILL: 4>: 1081679, <TargetType.INFILL_SPECIAL: 5>: 429242}\n",
      "Mask rate (tokens): 0.1937\n",
      "10519 documents, 158477 examples\n",
      "(158477, 256) uint16 (158477, 256) uint8\n",
      "Loading eval data\n",
      "{<TargetType.PAD: 0>: 108502, <TargetType.CONTEXT: 1>: 15113, <TargetType.CONTEXT_SPECIAL: 2>: 1671, <TargetType.CONTEXT_INFILL_SEP: 3>: 508, <TargetType.INFILL: 4>: 3760, <TargetType.INFILL_SPECIAL: 5>: 1518}\n",
      "Mask rate (tokens): 0.1992\n",
      "554 documents, 512 examples\n",
      "(512, 256) uint16 (512, 256) uint8\n",
      "Maximum number of training steps: 5259.333333333333\n",
      "Creating datasets\n",
      "Initializing model...\n",
      "from saved checkpoint (resuming)\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/train_ilm.py:442: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(out_fn_to_fp('optimizer.pt')))\n",
      "Training\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3407) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.042979372143502\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.493228409813407\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.677716520175021\n",
      "eval_time: 11.629318475723267\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3407) Summary\n",
      "3.37768292427063\n",
      "3.3528597354888916\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3603) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.058200757347334\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.490935490726747\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.684654643878024\n",
      "eval_time: 14.587703943252563\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3603) Summary\n",
      "3.300962448120117\n",
      "2.8371899127960205\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3791) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.073804703571159\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.514631563942285\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.71650604738834\n",
      "eval_time: 14.956198930740356\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3791) Summary\n",
      "3.3424856662750244\n",
      "2.777019739151001\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3982) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.089931706795815\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.492258537398364\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.6988926408138685\n",
      "eval_time: 15.021628379821777\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 3982) Summary\n",
      "3.8103179931640625\n",
      "2.869934558868408\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4173) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.078929455293617\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.453749325700943\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.632780565733605\n",
      "eval_time: 15.020508527755737\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4173) Summary\n",
      "3.1191065311431885\n",
      "2.6044349670410156\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4362) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.096819943763507\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.4566684806830237\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.622891044870336\n",
      "eval_time: 15.049084186553955\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4362) Summary\n",
      "2.9907379150390625\n",
      "3.067307472229004\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4553) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.123280917203639\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.4971321954018872\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.707734773957983\n",
      "eval_time: 15.004793405532837\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4553) Summary\n",
      "3.3716728687286377\n",
      "3.2013461589813232\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4744) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.1188798818961\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.4619510773683686\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.623006784852515\n",
      "eval_time: 14.72393798828125\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4744) Summary\n",
      "3.4235591888427734\n",
      "3.1703107357025146\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4935) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.13082512558505\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.4505083724015044\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.591247295952858\n",
      "eval_time: 15.186668157577515\n",
      "Saving\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 4935) Summary\n",
      "3.4316394329071045\n",
      "2.714914083480835\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 5124) Eval\n",
      "eval_context_count: 14686\n",
      "eval_context_loss: 4.131681054501804\n",
      "eval_infill_count: 5278\n",
      "eval_infill_loss: 3.4704606610294904\n",
      "eval_infill_textonly_count: 3760\n",
      "eval_infill_textonly_loss: 4.697718110426943\n",
      "eval_time: 14.97283124923706\n",
      "--------------------------------------------------------------------------------\n",
      "(Step 5124) Summary\n",
      "2.80949068069458\n",
      "2.070303201675415\n"
     ]
    }
   ],
   "source": [
    "# # $DATASET=\"data/french_dataset\"; $TRAIN_DIR=\"../Weights/ILM\"; $EXAMPLES_DIR=\"data/masks/french_dataset\"; \n",
    "!python train_ilm.py compound_dataset ../Weights/ILM  data/masks/compound_dataset --train_examples_tag train --eval_examples_tag valid --train_num_epochs 12 --eval_max_num_examples 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:50:28.057875Z",
     "iopub.status.busy": "2025-03-12T15:50:28.057526Z",
     "iopub.status.idle": "2025-03-12T15:50:28.061779Z",
     "shell.execute_reply": "2025-03-12T15:50:28.060884Z",
     "shell.execute_reply.started": "2025-03-12T15:50:28.057846Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"dbddv01/gpt2-french-small\")\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(\"dbddv01/gpt2-french-small\")\n",
    "# inputs = tokenizer(\"Bonjour, comment ça va ?\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# print(outputs.logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T14:10:25.206602Z",
     "iopub.status.busy": "2025-03-07T14:10:25.206245Z",
     "iopub.status.idle": "2025-03-07T14:10:25.213214Z",
     "shell.execute_reply": "2025-03-07T14:10:25.211852Z",
     "shell.execute_reply.started": "2025-03-07T14:10:25.206557Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-a6861435e9d3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-a6861435e9d3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    $DATASET=\"data/french_dataset\"; $TRAIN_DIR=\"../Weights/ILM\"; $EXAMPLES_DIR=\"data/masks/french_dataset\"; python train_ilm.py experiment_compound_dataset ${TRAIN_DIR} ${EXAMPLES_DIR} --train_examples_tag train --eval_examples_tag valid --train_num_epochs 4 --eval_max_num_examples 512\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# $DATASET=\"data/french_dataset\"; $TRAIN_DIR=\"../Weights/ILM\"; $EXAMPLES_DIR=\"data/masks/french_dataset\"; python train_ilm.py experiment_compound_dataset ${TRAIN_DIR} ${EXAMPLES_DIR} --train_examples_tag train --eval_examples_tag valid --train_num_epochs 4 --eval_max_num_examples 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate Hate check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T15:09:57.218888Z",
     "iopub.status.busy": "2025-03-08T15:09:57.218566Z",
     "iopub.status.idle": "2025-03-08T15:09:57.224745Z",
     "shell.execute_reply": "2025-03-08T15:09:57.223864Z",
     "shell.execute_reply.started": "2025-03-08T15:09:57.218864Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/French-Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:04:17.192347Z",
     "iopub.status.busy": "2025-03-12T17:04:17.192129Z",
     "iopub.status.idle": "2025-03-12T17:04:17.198688Z",
     "shell.execute_reply": "2025-03-12T17:04:17.197609Z",
     "shell.execute_reply.started": "2025-03-12T17:04:17.192326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "%cd Hate-Check-Necessity-Suffisciency/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-08T14:46:45.587027Z",
     "iopub.status.busy": "2025-03-08T14:46:45.586654Z",
     "iopub.status.idle": "2025-03-08T14:46:45.592767Z",
     "shell.execute_reply": "2025-03-08T14:46:45.591821Z",
     "shell.execute_reply.started": "2025-03-08T14:46:45.586993Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working\n"
     ]
    }
   ],
   "source": [
    "# %cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:05:11.053176Z",
     "iopub.status.busy": "2025-03-12T17:05:11.052867Z",
     "iopub.status.idle": "2025-03-12T17:05:30.854447Z",
     "shell.execute_reply": "2025-03-12T17:05:30.853770Z",
     "shell.execute_reply.started": "2025-03-12T17:05:11.053146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "import ilm.ilm.tokenize_util\n",
    "from ilm.ilm.infer import infill_with_ilm\n",
    "from perturbation_functions import calculate_necc_and_suff, gen_num_samples_table, gen_probs_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:05:30.856059Z",
     "iopub.status.busy": "2025-03-12T17:05:30.855532Z",
     "iopub.status.idle": "2025-03-12T17:05:30.861683Z",
     "shell.execute_reply": "2025-03-12T17:05:30.860835Z",
     "shell.execute_reply.started": "2025-03-12T17:05:30.856035Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "new_content = \"\"\"import numpy as np\n",
    "import torch\n",
    "import ilm\n",
    "from ilm.ilm.tokenize_util import encode, decode\n",
    "from ilm.ilm.infer import infill_with_ilm\n",
    "from math import ceil\n",
    "\n",
    "\n",
    "def gen_probs(text_len):\n",
    "    probs = np.zeros(text_len)\n",
    "    for nn in range(1, text_len):\n",
    "        probs[nn] = 1 / nn\n",
    "    probs = probs[1:]\n",
    "    return probs / np.sum(probs)\n",
    "\n",
    "def gen_probs_table(max_n):\n",
    "    probs_table = np.zeros((max_n+1, max_n-1))\n",
    "    for nn in range(2, max_n+1):\n",
    "        probs_table[nn, :nn-1] = gen_probs(nn)\n",
    "    return probs_table\n",
    "\n",
    "\n",
    "def gen_masks(ll, kk, num_samples, reverse=False):\n",
    "    rng = np.random.default_rng()\n",
    "    if reverse:\n",
    "        masks = np.ones((num_samples, ll), dtype=bool)\n",
    "    else:\n",
    "        masks = np.zeros((num_samples, ll), dtype=bool)\n",
    "    for ii in range(num_samples):\n",
    "        mask_ids = rng.choice(ll, kk[ii], replace=False)\n",
    "        if reverse:\n",
    "            masks[ii, mask_ids] = False\n",
    "        else:\n",
    "            masks[ii, mask_ids] = True\n",
    "    return masks\n",
    "\n",
    "\n",
    "def calculate_num_samples(text_len, exp_samples_per_tokn):\n",
    "    probs = [1/kk for kk in range(1, text_len)]\n",
    "    probs = [pp/sum(probs) for pp in probs] # probability of each subset size\n",
    "    probs_a = [(kk/text_len)*pp for pp, kk in zip(probs, range(1, text_len))] # probability of perturbing a token given the subset size\n",
    "    return ceil(exp_samples_per_tokn / sum(probs_a))\n",
    "\n",
    "\n",
    "def gen_num_samples_table(max_len, exp_samples_per_tokn):\n",
    "    num_samples_table = np.zeros((max_len+1), dtype=int)\n",
    "    # this is not well defined for less than 3 tokens.\n",
    "    for ii in range(3, max_len+1):\n",
    "        num_samples_table[ii] = calculate_num_samples(ii, exp_samples_per_tokn)\n",
    "\n",
    "    return num_samples_table\n",
    "\n",
    "\n",
    "def generate_masked_text_mask_tokn(orig_text, mask_tokn, num_samples,\n",
    "                                   reverse=False, probs_table=None):\n",
    "    split_txt = orig_text.strip().split()\n",
    "    # first, calculate the probabilities for masking k tokens for k=1..n\n",
    "    if probs_table is None:\n",
    "        probs = gen_probs(len(split_txt))\n",
    "    else:\n",
    "        probs = probs_table[len(split_txt), :len(split_txt) - 1]\n",
    "\n",
    "    # probs = np.zeros((max_masked))\n",
    "    # for ii in range(max_masked):\n",
    "    #     probs[ii] = 0.5 ** (ii + 1)  # probability of masking k tokens decrease exponentially\n",
    "    # if reverse:\n",
    "    #     probs = np.flip(probs)  # Because if reverse, we're going to mask all but k tokens\n",
    "    # probs = probs / np.sum(probs)\n",
    "    rng = np.random.default_rng()\n",
    "    kk = rng.choice(np.arange(1, len(split_txt)), num_samples, p=probs)\n",
    "\n",
    "    masks = gen_masks(len(split_txt), kk, num_samples, reverse)\n",
    "    masked = []\n",
    "    for mm in masks.tolist():\n",
    "        tokens = \" \".join([mask_tokn if msk else tt for msk, tt in zip(mm, split_txt)])\n",
    "        masked.append(tokens)\n",
    "\n",
    "    return masked, masks\n",
    "\n",
    "\n",
    "# reverse option masks all but k tokens.\n",
    "def generate_masked_text(orig_text, num_samples, mask_tokn, tokenizer, reverse=False, #max_masked=3\n",
    "                         merge_conseq_infills=True, probs_table=None):\n",
    "    split_txt = orig_text.strip().split()\n",
    "    # first, calculate the probabilities for masking k tokens for k=1..n\n",
    "    if probs_table is None:\n",
    "        probs = gen_probs(len(split_txt))\n",
    "    else:\n",
    "        probs = probs_table[len(split_txt), :len(split_txt)-1]\n",
    "\n",
    "    # probs = np.zeros((max_masked))\n",
    "    # for ii in range(max_masked):\n",
    "    #     probs[ii] = 0.5 ** (ii + 1)  # probability of masking k tokens decrease exponentially\n",
    "    # if reverse:\n",
    "    #     probs = np.flip(probs)  # Because if reverse, we're going to mask all but k tokens\n",
    "    # probs = probs / np.sum(probs)\n",
    "    rng = np.random.default_rng()\n",
    "    kk = rng.choice(np.arange(1, len(split_txt)), num_samples, p=probs)\n",
    "\n",
    "    masks = gen_masks(len(split_txt), kk, num_samples, reverse)\n",
    "    masked = []\n",
    "    for mm in masks.tolist():\n",
    "        token_ids = []\n",
    "        subtext = \"\"\n",
    "        for ii, qq in enumerate(mm):\n",
    "            if qq and subtext:\n",
    "                token_ids = token_ids + encode(subtext.rstrip(), tokenizer) + [mask_tokn]\n",
    "                subtext = \" \"\n",
    "            elif qq:\n",
    "                token_ids = token_ids + [mask_tokn]\n",
    "                subtext = \" \"\n",
    "            elif subtext:\n",
    "                subtext = subtext + split_txt[ii] + \" \"\n",
    "            else:\n",
    "                subtext = split_txt[ii] + \" \"\n",
    "        if subtext:\n",
    "            token_ids = token_ids + encode(subtext.rstrip(), tokenizer)\n",
    "\n",
    "        if merge_conseq_infills:\n",
    "            merged_token_ids = [token_ids[0]]\n",
    "            for mm in token_ids[1:]:\n",
    "                if (merged_token_ids[-1] == mask_tokn) & (mm == mask_tokn):\n",
    "                    continue\n",
    "                else:\n",
    "                    merged_token_ids.append(mm)\n",
    "            token_ids = merged_token_ids\n",
    "        masked.append(token_ids)\n",
    "\n",
    "    return masked, masks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# infill each with ilm, decode and return as a list\n",
    "def infill_masked(masked, model, additional_tokens_to_ids, tokenizer, num_infills=1):\n",
    "    generated = []\n",
    "    for mm in masked:\n",
    "        gg = infill_with_ilm(model, additional_tokens_to_ids, mm, num_infills=num_infills)[0]\n",
    "        generated.append(decode(gg, tokenizer).strip(\":\"))\n",
    "    return generated\n",
    "\n",
    "\n",
    "\n",
    "def get_preds(orig, perturbed, cl_model, cl_tokenizer, binary=True):\n",
    "    inputs = [orig]\n",
    "    inputs.extend(perturbed)\n",
    "    toknd = cl_tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    logits = cl_model(**toknd).logits\n",
    "    results = torch.softmax(logits, dim=1).cpu().detach().numpy()[:, 1]\n",
    "    if binary:\n",
    "        results = np.rint(results)\n",
    "\n",
    "    orig_pred = results[0]\n",
    "    preds = results[1:]\n",
    "    return orig_pred, preds\n",
    "\n",
    "def get_preds_and_scores(texts, tokenizer, model, pbar=None, batchsize=64):\n",
    "    preds = []\n",
    "    scores = []\n",
    "    nn = 0\n",
    "    while nn<len(texts):\n",
    "        batch = texts[nn:nn+batchsize]\n",
    "        nn += batchsize\n",
    "        encodings = tokenizer(batch, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        outputs = model(**encodings)\n",
    "        ss = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pp = torch.argmax(outputs.logits, dim=-1)\n",
    "        scores.extend(ss[:,1].tolist())\n",
    "        preds.extend(pp.tolist())\n",
    "        if pbar:\n",
    "            pbar.update(len(batch))\n",
    "    return preds, scores\n",
    "\n",
    "def calc_suff(base_pred, preds, masks):\n",
    "    ll = masks.shape[1]\n",
    "    suff = []\n",
    "    for ii in range(ll):\n",
    "        mm = masks[:, ii]\n",
    "        suff_ii = np.mean(preds[np.logical_not(mm)] - base_pred) # mean difference of the predictions where token is not perturbed\n",
    "        suff.append(suff_ii)\n",
    "    return suff\n",
    "\n",
    "\n",
    "def calc_necc(orig_pred, preds, masks):\n",
    "    ll = masks.shape[1]\n",
    "    necc = []\n",
    "    for ii in range(ll):\n",
    "        mm = masks[:, ii]\n",
    "        necc_ii = np.mean(orig_pred - preds[mm]) # mean difference of predictions that are different from the original prediction\n",
    "        necc.append(necc_ii)\n",
    "    return necc\n",
    "\n",
    "\n",
    "def calculate_necc_and_suff(text, ilm_tokenizer, ilm_model, cl_tokenizer, cl_model, num_samples,\n",
    "                            mask_tokn, additional_tokens_to_ids, binary=True, use_masks_only=False,\n",
    "                            probs_table=None, base_pred=0.0, return_pert_only=False):\n",
    "    split_txt = text.strip().split()\n",
    "    if type(num_samples) is np.ndarray:\n",
    "        num_samples = num_samples[len(split_txt)]\n",
    "\n",
    "    if use_masks_only:\n",
    "        necc_perturbed, necc_masks = generate_masked_text_mask_tokn(text, mask_tokn, num_samples,\n",
    "                                                                    reverse=False, probs_table=probs_table)\n",
    "        suff_perturbed, suff_masks = generate_masked_text_mask_tokn(text, mask_tokn, num_samples,\n",
    "                                                                    reverse=True, probs_table=probs_table)\n",
    "    else:\n",
    "        necc_masked, necc_masks = generate_masked_text(text, num_samples, mask_tokn, ilm_tokenizer,\n",
    "                                                       reverse=False, merge_conseq_infills=True, probs_table=probs_table)\n",
    "        suff_masked, suff_masks = generate_masked_text(text, num_samples, mask_tokn, ilm_tokenizer,\n",
    "                                                       reverse=True, merge_conseq_infills=True,\n",
    "                                                       probs_table=probs_table)\n",
    "        necc_perturbed = infill_masked(necc_masked, ilm_model, additional_tokens_to_ids, ilm_tokenizer)\n",
    "        suff_perturbed = infill_masked(suff_masked, ilm_model, additional_tokens_to_ids, ilm_tokenizer)\n",
    "\n",
    "    if return_pert_only:\n",
    "        return necc_perturbed, suff_perturbed, necc_masks, suff_masks\n",
    "\n",
    "    orig_pred, necc_preds = get_preds(text, necc_perturbed, cl_model, cl_tokenizer, binary=binary)\n",
    "    _, suff_preds = get_preds(text, suff_perturbed, cl_model, cl_tokenizer, binary=binary)\n",
    "    necc = calc_necc(orig_pred, necc_preds, necc_masks)\n",
    "    suff = calc_suff(base_pred, suff_preds, suff_masks)\n",
    "\n",
    "    return necc, suff, necc_perturbed, suff_perturbed, necc_masks, suff_masks, orig_pred, necc_preds, suff_preds\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "with open(\"perturbation_functions.py\", \"w\") as f:\n",
    "    f.write(new_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:51:07.628906Z",
     "iopub.status.busy": "2025-03-12T16:51:07.628229Z",
     "iopub.status.idle": "2025-03-12T16:51:07.779179Z",
     "shell.execute_reply": "2025-03-12T16:51:07.777930Z",
     "shell.execute_reply.started": "2025-03-12T16:51:07.628852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.6G\n",
      "-rw-r--r-- 1 root root  179 Mar 12 16:49 additional_ids_to_tokens.pkl\n",
      "-rw-r--r-- 1 root root  929 Mar 12 16:49 config.json\n",
      "-rw-r--r-- 1 root root 257K Mar 12 16:49 eval_inp.npy\n",
      "-rw-r--r-- 1 root root   15 Mar 12 16:49 eval_num_docs.pkl\n",
      "-rw-r--r-- 1 root root 129K Mar 12 16:49 eval_tts.npy\n",
      "-rw-r--r-- 1 root root 950M Mar 12 16:49 optimizer.pt\n",
      "-rw-r--r-- 1 root root 475M Mar 12 16:49 pytorch_model.bin\n",
      "-rw-r--r-- 1 root root   15 Mar 12 16:49 step.pkl\n",
      "-rw-r--r-- 1 root root  78M Mar 12 16:49 train_inp.npy\n",
      "-rw-r--r-- 1 root root   15 Mar 12 16:49 train_num_docs.pkl\n",
      "-rw-r--r-- 1 root root  39M Mar 12 16:49 train_tts.npy\n"
     ]
    }
   ],
   "source": [
    "!ls -lh Weights/ILM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:33:01.765283Z",
     "iopub.status.busy": "2025-03-12T16:33:01.764954Z",
     "iopub.status.idle": "2025-03-12T16:33:01.769469Z",
     "shell.execute_reply": "2025-03-12T16:33:01.768184Z",
     "shell.execute_reply.started": "2025-03-12T16:33:01.765257Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# model_path = \"Weights/ILM/pytorch_model.bin\"\n",
    "# try:\n",
    "#     state_dict = torch.load(model_path, map_location=\"cuda\")\n",
    "#     print(\"Checkpoint loaded successfully!\")\n",
    "# except Exception as e:\n",
    "#     print(\"Error loading checkpoint:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T17:05:49.442185Z",
     "iopub.status.busy": "2025-03-12T17:05:49.441888Z",
     "iopub.status.idle": "2025-03-12T17:05:50.100595Z",
     "shell.execute_reply": "2025-03-12T17:05:50.099922Z",
     "shell.execute_reply.started": "2025-03-12T17:05:49.442162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<|startofinfill|>': 50257, '<|endofinfill|>': 50258, '<|infill_document|>': 50259, '<|infill_paragraph|>': 50260, '<|infill_sentence|>': 50261, '<|infill_ngram|>': 50262, '<|infill_word|>': 50263}\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = 'Weights/ILM/'\n",
    "MASK_CLS = 'ilm.mask.hierarchical.MaskHierarchical'\n",
    "\n",
    "tokenizer = ilm.ilm.tokenize_util.Tokenizer.GPT2\n",
    "with open(os.path.join(MODEL_DIR, 'additional_ids_to_tokens.pkl'), 'rb') as f:\n",
    "    additional_ids_to_tokens = pickle.load(f)\n",
    "additional_tokens_to_ids = {v:k for k, v in additional_ids_to_tokens.items()}\n",
    "try:\n",
    "    ilm.ilm.tokenize_util.update_tokenizer(additional_ids_to_tokens, tokenizer)\n",
    "except ValueError:\n",
    "    print('Already updated')\n",
    "print(additional_tokens_to_ids)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_DIR)\n",
    "model.eval()\n",
    "_ = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:48:55.107299Z",
     "iopub.status.busy": "2025-03-12T16:48:55.106973Z",
     "iopub.status.idle": "2025-03-12T16:48:55.129475Z",
     "shell.execute_reply": "2025-03-12T16:48:55.128595Z",
     "shell.execute_reply.started": "2025-03-12T16:48:55.107263Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_suite_cases = pd.read_csv(\"hatecheck-data/test_suite_cases.csv\", index_col=\"case_id\") #.drop(columns=['Unnamed:0'])\n",
    "test_suite_cases.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "target_ds = ['disabeled people', 'black people']\n",
    "funcs = ['derog_neg_emote_h', 'derog_neg_attrib_h', 'derog_dehum_h']\n",
    "\n",
    "test_suite_cases = test_suite_cases[test_suite_cases.target_ident.isin(target_ds) & \n",
    "                                    test_suite_cases.functionality.isin(funcs)]\n",
    "tts = [text for _, text in test_suite_cases.test_case.items()]\n",
    "\n",
    "with open(\"Data/HateCheck_test_suite_cases.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(tts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T18:18:45.342061Z",
     "iopub.status.busy": "2025-03-12T18:18:45.341697Z",
     "iopub.status.idle": "2025-03-12T19:13:28.516769Z",
     "shell.execute_reply": "2025-03-12T19:13:28.515798Z",
     "shell.execute_reply.started": "2025-03-12T18:18:45.342025Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 60/120 [54:43<54:43, 54.72s/it]  \n"
     ]
    }
   ],
   "source": [
    "# generate approximately 100 perturbations for each token. \n",
    "num_samples = gen_num_samples_table(20, 100)\n",
    "probs_table = gen_probs_table(20)\n",
    "mask_tokn = additional_tokens_to_ids['<|infill_ngram|>']\n",
    "\n",
    "orig_texts = []\n",
    "necc_perturbed = []\n",
    "suff_perturbed = []\n",
    "necc_masks = []\n",
    "suff_masks = []\n",
    "\n",
    "with open(\"Data/HateCheck_test_suite_cases.txt\", \"r\") as ff:\n",
    "    with tqdm(total=120) as pbar:\n",
    "        for text in ff:\n",
    "            necc_pp, suff_pp, necc_mm, suff_mm = calculate_necc_and_suff(text, ilm_tokenizer=tokenizer, ilm_model=model, cl_tokenizer=None, cl_model=None, num_samples=num_samples,\n",
    "                               mask_tokn=mask_tokn, additional_tokens_to_ids=additional_tokens_to_ids, probs_table=probs_table, \n",
    "                               return_pert_only=True)\n",
    "\n",
    "            orig_texts.append(text)\n",
    "            necc_perturbed.append(necc_pp)\n",
    "            suff_perturbed.append(suff_pp)\n",
    "            necc_masks.append(necc_mm)\n",
    "            suff_masks.append(suff_mm)\n",
    "            pbar.update(1)\n",
    "    \n",
    "necc_suff_perturbations = {'orig_texts': orig_texts, \n",
    "                           'necc_perturbed': necc_perturbed, \n",
    "                           'suff_perturbed': suff_perturbed,\n",
    "                           'necc_masks': necc_masks,\n",
    "                           'suff_masks': suff_masks}\n",
    "\n",
    "pickle.dump(necc_suff_perturbations, open('Data/HateCheck_necc_suff_perturbations.pickle', 'wb'))\n",
    "# pickle.dump(necc_suff_perturbations, open('Data/HateCheck_necc_suff_perturbations_2.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-12T18:17:36.788930Z",
     "iopub.status.busy": "2025-03-12T18:17:36.788587Z",
     "iopub.status.idle": "2025-03-12T18:17:36.962953Z",
     "shell.execute_reply": "2025-03-12T18:17:36.962132Z",
     "shell.execute_reply.started": "2025-03-12T18:17:36.788901Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import torch\n",
      "import ilm\n",
      "from ilm.ilm.tokenize_util import encode, decode\n",
      "from ilm.ilm.infer import infill_with_ilm\n",
      "from math import ceil\n",
      "\n",
      "\n",
      "def gen_probs(text_len):\n",
      "    probs = np.zeros(text_len)\n",
      "    for nn in range(1, text_len):\n",
      "        probs[nn] = 1 / nn\n",
      "    probs = probs[1:]\n",
      "    return probs / np.sum(probs)\n",
      "\n",
      "def gen_probs_table(max_n):\n",
      "    probs_table = np.zeros((max_n+1, max_n-1))\n",
      "    for nn in range(2, max_n+1):\n",
      "        probs_table[nn, :nn-1] = gen_probs(nn)\n",
      "    return probs_table\n",
      "\n",
      "\n",
      "def gen_masks(ll, kk, num_samples, reverse=False):\n",
      "    rng = np.random.default_rng()\n",
      "    if reverse:\n",
      "        masks = np.ones((num_samples, ll), dtype=bool)\n",
      "    else:\n",
      "        masks = np.zeros((num_samples, ll), dtype=bool)\n",
      "    for ii in range(num_samples):\n",
      "        mask_ids = rng.choice(ll, kk[ii], replace=False)\n",
      "        if reverse:\n",
      "            masks[ii, mask_ids] = False\n",
      "        else:\n",
      "            masks[ii, mask_ids] = True\n",
      "    return masks\n",
      "\n",
      "\n",
      "def calculate_num_samples(text_len, exp_samples_per_tokn):\n",
      "    probs = [1/kk for kk in range(1, text_len)]\n",
      "    probs = [pp/sum(probs) for pp in probs] # probability of each subset size\n",
      "    probs_a = [(kk/text_len)*pp for pp, kk in zip(probs, range(1, text_len))] # probability of perturbing a token given the subset size\n",
      "    return ceil(exp_samples_per_tokn / sum(probs_a))\n",
      "\n",
      "\n",
      "def gen_num_samples_table(max_len, exp_samples_per_tokn):\n",
      "    num_samples_table = np.zeros((max_len+1), dtype=int)\n",
      "    # this is not well defined for less than 3 tokens.\n",
      "    for ii in range(3, max_len+1):\n",
      "        num_samples_table[ii] = calculate_num_samples(ii, exp_samples_per_tokn)\n",
      "\n",
      "    return num_samples_table\n",
      "\n",
      "\n",
      "def generate_masked_text_mask_tokn(orig_text, mask_tokn, num_samples,\n",
      "                                   reverse=False, probs_table=None):\n",
      "    split_txt = orig_text.strip().split()\n",
      "    # first, calculate the probabilities for masking k tokens for k=1..n\n",
      "    if probs_table is None:\n",
      "        probs = gen_probs(len(split_txt))\n",
      "    else:\n",
      "        probs = probs_table[len(split_txt), :len(split_txt) - 1]\n",
      "\n",
      "    # probs = np.zeros((max_masked))\n",
      "    # for ii in range(max_masked):\n",
      "    #     probs[ii] = 0.5 ** (ii + 1)  # probability of masking k tokens decrease exponentially\n",
      "    # if reverse:\n",
      "    #     probs = np.flip(probs)  # Because if reverse, we're going to mask all but k tokens\n",
      "    # probs = probs / np.sum(probs)\n",
      "    rng = np.random.default_rng()\n",
      "    kk = rng.choice(np.arange(1, len(split_txt)), num_samples, p=probs)\n",
      "\n",
      "    masks = gen_masks(len(split_txt), kk, num_samples, reverse)\n",
      "    masked = []\n",
      "    for mm in masks.tolist():\n",
      "        tokens = \" \".join([mask_tokn if msk else tt for msk, tt in zip(mm, split_txt)])\n",
      "        masked.append(tokens)\n",
      "\n",
      "    return masked, masks\n",
      "\n",
      "\n",
      "# reverse option masks all but k tokens.\n",
      "def generate_masked_text(orig_text, num_samples, mask_tokn, tokenizer, reverse=False, #max_masked=3\n",
      "                         merge_conseq_infills=True, probs_table=None):\n",
      "    split_txt = orig_text.strip().split()\n",
      "    # first, calculate the probabilities for masking k tokens for k=1..n\n",
      "    if probs_table is None:\n",
      "        probs = gen_probs(len(split_txt))\n",
      "    else:\n",
      "        probs = probs_table[len(split_txt), :len(split_txt)-1]\n",
      "\n",
      "    # probs = np.zeros((max_masked))\n",
      "    # for ii in range(max_masked):\n",
      "    #     probs[ii] = 0.5 ** (ii + 1)  # probability of masking k tokens decrease exponentially\n",
      "    # if reverse:\n",
      "    #     probs = np.flip(probs)  # Because if reverse, we're going to mask all but k tokens\n",
      "    # probs = probs / np.sum(probs)\n",
      "    rng = np.random.default_rng()\n",
      "    kk = rng.choice(np.arange(1, len(split_txt)), num_samples, p=probs)\n",
      "\n",
      "    masks = gen_masks(len(split_txt), kk, num_samples, reverse)\n",
      "    masked = []\n",
      "    for mm in masks.tolist():\n",
      "        token_ids = []\n",
      "        subtext = \"\"\n",
      "        for ii, qq in enumerate(mm):\n",
      "            if qq and subtext:\n",
      "                token_ids = token_ids + encode(subtext.rstrip(), tokenizer) + [mask_tokn]\n",
      "                subtext = \" \"\n",
      "            elif qq:\n",
      "                token_ids = token_ids + [mask_tokn]\n",
      "                subtext = \" \"\n",
      "            elif subtext:\n",
      "                subtext = subtext + split_txt[ii] + \" \"\n",
      "            else:\n",
      "                subtext = split_txt[ii] + \" \"\n",
      "        if subtext:\n",
      "            token_ids = token_ids + encode(subtext.rstrip(), tokenizer)\n",
      "\n",
      "        if merge_conseq_infills:\n",
      "            merged_token_ids = [token_ids[0]]\n",
      "            for mm in token_ids[1:]:\n",
      "                if (merged_token_ids[-1] == mask_tokn) & (mm == mask_tokn):\n",
      "                    continue\n",
      "                else:\n",
      "                    merged_token_ids.append(mm)\n",
      "            token_ids = merged_token_ids\n",
      "        masked.append(token_ids)\n",
      "\n",
      "    return masked, masks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# infill each with ilm, decode and return as a list\n",
      "def infill_masked(masked, model, additional_tokens_to_ids, tokenizer, num_infills=1):\n",
      "    generated = []\n",
      "    for mm in masked:\n",
      "        gg = infill_with_ilm(model, additional_tokens_to_ids, mm, num_infills=num_infills)[0]\n",
      "        generated.append(decode(gg, tokenizer).strip(\":\"))\n",
      "    return generated\n",
      "\n",
      "\n",
      "\n",
      "def get_preds(orig, perturbed, cl_model, cl_tokenizer, binary=True):\n",
      "    inputs = [orig]\n",
      "    inputs.extend(perturbed)\n",
      "    toknd = cl_tokenizer(inputs, return_tensors=\"pt\", padding=True, truncation=True)\n",
      "    logits = cl_model(**toknd).logits\n",
      "    results = torch.softmax(logits, dim=1).cpu().detach().numpy()[:, 1]\n",
      "    if binary:\n",
      "        results = np.rint(results)\n",
      "\n",
      "    orig_pred = results[0]\n",
      "    preds = results[1:]\n",
      "    return orig_pred, preds\n",
      "\n",
      "def get_preds_and_scores(texts, tokenizer, model, pbar=None, batchsize=64):\n",
      "    preds = []\n",
      "    scores = []\n",
      "    nn = 0\n",
      "    while nn<len(texts):\n",
      "        batch = texts[nn:nn+batchsize]\n",
      "        nn += batchsize\n",
      "        encodings = tokenizer(batch, truncation=True, padding=True, return_tensors=\"pt\")\n",
      "        outputs = model(**encodings)\n",
      "        ss = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
      "        pp = torch.argmax(outputs.logits, dim=-1)\n",
      "        scores.extend(ss[:,1].tolist())\n",
      "        preds.extend(pp.tolist())\n",
      "        if pbar:\n",
      "            pbar.update(len(batch))\n",
      "    return preds, scores\n",
      "\n",
      "def calc_suff(base_pred, preds, masks):\n",
      "    ll = masks.shape[1]\n",
      "    suff = []\n",
      "    for ii in range(ll):\n",
      "        mm = masks[:, ii]\n",
      "        suff_ii = np.mean(preds[np.logical_not(mm)] - base_pred) # mean difference of the predictions where token is not perturbed\n",
      "        suff.append(suff_ii)\n",
      "    return suff\n",
      "\n",
      "\n",
      "def calc_necc(orig_pred, preds, masks):\n",
      "    ll = masks.shape[1]\n",
      "    necc = []\n",
      "    for ii in range(ll):\n",
      "        mm = masks[:, ii]\n",
      "        necc_ii = np.mean(orig_pred - preds[mm]) # mean difference of predictions that are different from the original prediction\n",
      "        necc.append(necc_ii)\n",
      "    return necc\n",
      "\n",
      "\n",
      "def calculate_necc_and_suff(text, ilm_tokenizer, ilm_model, cl_tokenizer, cl_model, num_samples,\n",
      "                            mask_tokn, additional_tokens_to_ids, binary=True, use_masks_only=False,\n",
      "                            probs_table=None, base_pred=0.0, return_pert_only=False):\n",
      "    split_txt = text.strip().split()\n",
      "    if type(num_samples) is np.ndarray:\n",
      "        num_samples = num_samples[len(split_txt)]\n",
      "\n",
      "    if use_masks_only:\n",
      "        necc_perturbed, necc_masks = generate_masked_text_mask_tokn(text, mask_tokn, num_samples,\n",
      "                                                                    reverse=False, probs_table=probs_table)\n",
      "        suff_perturbed, suff_masks = generate_masked_text_mask_tokn(text, mask_tokn, num_samples,\n",
      "                                                                    reverse=True, probs_table=probs_table)\n",
      "    else:\n",
      "        necc_masked, necc_masks = generate_masked_text(text, num_samples, mask_tokn, ilm_tokenizer,\n",
      "                                                       reverse=False, merge_conseq_infills=True, probs_table=probs_table)\n",
      "        suff_masked, suff_masks = generate_masked_text(text, num_samples, mask_tokn, ilm_tokenizer,\n",
      "                                                       reverse=True, merge_conseq_infills=True,\n",
      "                                                       probs_table=probs_table)\n",
      "        necc_perturbed = infill_masked(necc_masked, ilm_model, additional_tokens_to_ids, ilm_tokenizer)\n",
      "        suff_perturbed = infill_masked(suff_masked, ilm_model, additional_tokens_to_ids, ilm_tokenizer)\n",
      "\n",
      "    if return_pert_only:\n",
      "        return necc_perturbed, suff_perturbed, necc_masks, suff_masks\n",
      "\n",
      "    orig_pred, necc_preds = get_preds(text, necc_perturbed, cl_model, cl_tokenizer, binary=binary)\n",
      "    _, suff_preds = get_preds(text, suff_perturbed, cl_model, cl_tokenizer, binary=binary)\n",
      "    necc = calc_necc(orig_pred, necc_preds, necc_masks)\n",
      "    suff = calc_suff(base_pred, suff_preds, suff_masks)\n",
      "\n",
      "    return necc, suff, necc_perturbed, suff_perturbed, necc_masks, suff_masks, orig_pred, necc_preds, suff_preds\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat perturbation_functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:11:31.449339Z",
     "iopub.status.busy": "2025-03-13T10:11:31.448833Z",
     "iopub.status.idle": "2025-03-13T10:11:38.018294Z",
     "shell.execute_reply": "2025-03-13T10:11:38.016904Z",
     "shell.execute_reply.started": "2025-03-13T10:11:31.449281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordsegment\n",
      "  Downloading wordsegment-1.3.1-py2.py3-none-any.whl.metadata (7.7 kB)\n",
      "Downloading wordsegment-1.3.1-py2.py3-none-any.whl (4.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wordsegment\n",
      "Successfully installed wordsegment-1.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wordsegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:11:38.020560Z",
     "iopub.status.busy": "2025-03-13T10:11:38.020208Z",
     "iopub.status.idle": "2025-03-13T10:11:38.641455Z",
     "shell.execute_reply": "2025-03-13T10:11:38.640289Z",
     "shell.execute_reply.started": "2025-03-13T10:11:38.020531Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "#import preprocessor\n",
    "from html import unescape\n",
    "import re\n",
    "import string\n",
    "import wordsegment as ws\n",
    "#import emoji\n",
    "ws.load() # load vocab for word segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:11:42.151703Z",
     "iopub.status.busy": "2025-03-13T10:11:42.151186Z",
     "iopub.status.idle": "2025-03-13T10:11:42.164492Z",
     "shell.execute_reply": "2025-03-13T10:11:42.163132Z",
     "shell.execute_reply.started": "2025-03-13T10:11:42.151651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def regex_match_segmentation(match):\n",
    "    return ' '.join(ws.segment(match.group(0)))\n",
    "# Define function for cleaning text\n",
    "def clean_text(text):\n",
    "    \n",
    "    # convert HTML codes\n",
    "    text = unescape(text)\n",
    "    \n",
    "    # lowercase text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # replace mentions, URLs and emojis with special token\n",
    "    text = re.sub(r\"@[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"u/[A-Za-z0-9_-]+\",'[USER]',text)\n",
    "    text = re.sub(r\"http\\S+\",'[URL]',text)\n",
    "    \n",
    "    # find and split hashtags into words\n",
    "    text = re.sub(r\"#[A-Za-z0-9]+\", regex_match_segmentation, text)\n",
    "\n",
    "    # remove punctuation at beginning of string (quirk in Davidson data)\n",
    "    text = text.lstrip(\"!\")\n",
    "    text = text.lstrip(\":\")\n",
    "    \n",
    "    # remove newline and tab characters\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('[linebreak]', ' ')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-12T23:48:31.737216Z",
     "iopub.status.busy": "2025-03-12T23:48:31.736813Z",
     "iopub.status.idle": "2025-03-12T23:48:31.867473Z",
     "shell.execute_reply": "2025-03-12T23:48:31.866011Z",
     "shell.execute_reply.started": "2025-03-12T23:48:31.737182Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1_ILM_dataset_prep.ipynb\n",
      " 2_train_ILM.ipynb\n",
      " 3_generate_HateCheck_perturbations.ipynb\n",
      " 4_generate_datasets_for_classification.ipynb\n",
      " 5_train_and_eval_classifiers.ipynb\n",
      " 6_calculate_necc_and_suff.ipynb\n",
      " 7_analysis_of_results.ipynb\n",
      " 8_masking_vs_perturbing.ipynb\n",
      " 9_comparison_with_SHAP_and_LIME.ipynb\n",
      " \u001b[0m\u001b[01;34mData\u001b[0m/\n",
      "'Dynamically Generated Hate Dataset v0.2.3.csv'\n",
      " \u001b[01;34mhatecheck-data\u001b[0m/\n",
      " \u001b[01;34milm\u001b[0m/\n",
      " perturbation_functions.py\n",
      " \u001b[01;34m__pycache__\u001b[0m/\n",
      " README.md\n",
      " state.db\n",
      " train.py\n",
      " \u001b[01;34mWeights\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T00:56:31.179712Z",
     "iopub.status.busy": "2025-03-13T00:56:31.179404Z",
     "iopub.status.idle": "2025-03-13T00:56:31.818260Z",
     "shell.execute_reply": "2025-03-13T00:56:31.817393Z",
     "shell.execute_reply.started": "2025-03-13T00:56:31.179686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-13 00:56:31--  https://raw.githubusercontent.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset/refs/heads/main/Dynamically%20Generated%20Hate%20Dataset%20v0.2.3.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 9755776 (9.3M) [text/plain]\n",
      "Saving to: ‘Dynamically Generated Hate Dataset v0.2.3.csv.2’\n",
      "\n",
      "Dynamically Generat 100%[===================>]   9.30M  --.-KB/s    in 0.09s   \n",
      "\n",
      "2025-03-13 00:56:31 (106 MB/s) - ‘Dynamically Generated Hate Dataset v0.2.3.csv.2’ saved [9755776/9755776]\n",
      "\n",
      "--2025-03-13 00:56:31--  http://data/Dynamically-Generated-Hate-Speech-Dataset.csv\n",
      "Resolving data (data)... failed: No address associated with hostname.\n",
      "wget: unable to resolve host address ‘data’\n",
      "FINISHED --2025-03-13 00:56:31--\n",
      "Total wall clock time: 0.4s\n",
      "Downloaded: 1 files, 9.3M in 0.09s (106 MB/s)\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/bvidgen/Dynamically-Generated-Hate-Speech-Dataset/refs/heads/main/Dynamically%20Generated%20Hate%20Dataset%20v0.2.3.csv Data/Dynamically-Generated-Hate-Speech-Dataset.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:24:55.447853Z",
     "iopub.status.busy": "2025-03-13T10:24:55.447435Z",
     "iopub.status.idle": "2025-03-13T10:24:56.373242Z",
     "shell.execute_reply": "2025-03-13T10:24:56.372339Z",
     "shell.execute_reply.started": "2025-03-13T10:24:55.447809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique label values before mapping: ['hate' 'nothate']\n",
      "Unique label values after mapping: [1 0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from html import unescape\n",
    "import os\n",
    "\n",
    "# --- Step 1. Load the Dataset ---\n",
    "# Adjust the file path and separator (e.g. comma or tab) as needed.\n",
    "df = pd.read_csv(\"../Dynamically Generated Hate Dataset v0.2.3.csv\", sep=\",\")\n",
    "\n",
    "df.text = df.text.astype(str).apply(lambda tt: clean_text(tt))\n",
    "\n",
    "# # --- Step 3. Process the 'Target' Column (Optional) ---\n",
    "# # If the 'Target' column uses shorthand labels, define a mapping dictionary.\n",
    "# # (Adjust the mapping according to your dataset documentation.)\n",
    "# target_mapping = {\n",
    "#     'w': 'women',\n",
    "#     'm': 'muslims',\n",
    "#     'b': 'black',\n",
    "#     'r': 'racism',  \n",
    "#     'h': 'hispanic',\n",
    "#     # add other mappings as needed...\n",
    "# }\n",
    "\n",
    "# def map_target(target):\n",
    "#     \"\"\"\n",
    "#     Convert shorthand target labels to full names.\n",
    "#     If multiple targets are given (separated by commas), process each.\n",
    "#     If the value is 'notgiven' or 'none', keep it unchanged.\n",
    "#     \"\"\"\n",
    "#     if pd.isna(target) or target in ['notgiven', 'none']:\n",
    "#         return target\n",
    "#     # Split on commas if multiple targets are present\n",
    "#     targets = [t.strip() for t in target.split(',')]\n",
    "#     mapped = [target_mapping.get(t, t) for t in targets]\n",
    "#     return \", \".join(mapped)\n",
    "\n",
    "# df['Target'] = df['Target'].astype(str).apply(map_target)\n",
    "\n",
    "# # --- Step 4. (Optional) Inspect Distributions ---\n",
    "# print(\"Overall Label distribution:\")\n",
    "# print(df['label'].value_counts())\n",
    "\n",
    "# print(\"\\nDistribution of 'Type' for hateful entries:\")\n",
    "# print(df[df['label'] == 'hate']['type'].value_counts())\n",
    "\n",
    "# print(\"\\nDistribution of 'Target' for hateful entries:\")\n",
    "# print(df[df['label'] == 'hate']['target'].value_counts())\n",
    "\n",
    "# Clean the label column (strip spaces and convert to lowercase)\n",
    "df['label'] = df['label'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# Print unique values before mapping\n",
    "print(\"Unique label values before mapping:\", df['label'].unique())\n",
    "\n",
    "\n",
    "df['label'] = df['label'].map({'hate': 1, 'nothate': 0})\n",
    "# df = df.dropna(subset=['label'])\n",
    "df['label'] = df['label'].fillna(0)\n",
    "print(\"Unique label values after mapping:\", df['label'].unique())\n",
    "\n",
    "# --- Step 5. Split into Train/Dev/Test ---\n",
    "\n",
    "df_train = df[df['split'] == 'train'].copy()\n",
    "df_valid   = df[df['split'] == 'dev'].copy()\n",
    "df_test  = df[df['split'] == 'test'].copy()\n",
    "\n",
    "# This dataset already has a 'Split' column with values 'train', 'dev', and 'test'\n",
    "df_train = df_train[['text', 'label']]\n",
    "df_valid = df_valid[['text', 'label']]\n",
    "df_test  = df_test[['text', 'label']]\n",
    "\n",
    "# --- Step 6. Save the Processed Splits into a Folder ---\n",
    "output_folder = \"Data/Dynamic_hate\"\n",
    "# os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "df_train.to_csv(output_folder+\"train.csv\")\n",
    "df_valid.to_csv(output_folder+\"valid.csv\")\n",
    "df_test.to_csv(output_folder+ \"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T01:24:41.612520Z",
     "iopub.status.busy": "2025-03-13T01:24:41.612203Z",
     "iopub.status.idle": "2025-03-13T01:24:41.890464Z",
     "shell.execute_reply": "2025-03-13T01:24:41.889493Z",
     "shell.execute_reply.started": "2025-03-13T01:24:41.612492Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm Data/Dynamic_hate/valid.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:13:06.178857Z",
     "iopub.status.busy": "2025-03-13T10:13:06.178298Z",
     "iopub.status.idle": "2025-03-13T10:13:06.188553Z",
     "shell.execute_reply": "2025-03-13T10:13:06.187238Z",
     "shell.execute_reply.started": "2025-03-13T10:13:06.178807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32924, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:23:22.120704Z",
     "iopub.status.busy": "2025-03-13T10:23:22.120299Z",
     "iopub.status.idle": "2025-03-13T10:23:22.124988Z",
     "shell.execute_reply": "2025-03-13T10:23:22.123755Z",
     "shell.execute_reply.started": "2025-03-13T10:23:22.120677Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df_train.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:23:18.074678Z",
     "iopub.status.busy": "2025-03-13T10:23:18.074255Z",
     "iopub.status.idle": "2025-03-13T10:23:18.078737Z",
     "shell.execute_reply": "2025-03-13T10:23:18.077470Z",
     "shell.execute_reply.started": "2025-03-13T10:23:18.074649Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# print(df.head(300))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:13:16.588666Z",
     "iopub.status.busy": "2025-03-13T10:13:16.588192Z",
     "iopub.status.idle": "2025-03-13T10:13:16.596249Z",
     "shell.execute_reply": "2025-03-13T10:13:16.594928Z",
     "shell.execute_reply.started": "2025-03-13T10:13:16.588634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# train_labels = df['label'].to_numpy()  # if the column is named \"Label\"\n",
    "# print(np.unique(train_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:23:01.660613Z",
     "iopub.status.busy": "2025-03-13T10:23:01.660209Z",
     "iopub.status.idle": "2025-03-13T10:23:01.664653Z",
     "shell.execute_reply": "2025-03-13T10:23:01.663536Z",
     "shell.execute_reply.started": "2025-03-13T10:23:01.660582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # df = pd.read_csv(\"Data/Dynamic_hate/train.csv\")\n",
    "# df = pd.read_csv(\"Data/Dynamic_hate/train.csv\", engine=\"python\", encoding=\"utf-8\")\n",
    "\n",
    "# nan_rows = df[df['label'].isna()]\n",
    "# print(nan_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:23:08.852434Z",
     "iopub.status.busy": "2025-03-13T10:23:08.852051Z",
     "iopub.status.idle": "2025-03-13T10:23:08.856499Z",
     "shell.execute_reply": "2025-03-13T10:23:08.855537Z",
     "shell.execute_reply.started": "2025-03-13T10:23:08.852403Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# df.to_csv(\"Data/Dynamic_hate/train.csv\", index=False, quoting=csv.QUOTE_MINIMAL, encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T00:59:39.041539Z",
     "iopub.status.busy": "2025-03-13T00:59:39.041231Z",
     "iopub.status.idle": "2025-03-13T00:59:39.238596Z",
     "shell.execute_reply": "2025-03-13T00:59:39.237885Z",
     "shell.execute_reply.started": "2025-03-13T00:59:39.041513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Measuring_dat_hate/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T10:23:13.514890Z",
     "iopub.status.busy": "2025-03-13T10:23:13.514505Z",
     "iopub.status.idle": "2025-03-13T10:23:13.519469Z",
     "shell.execute_reply": "2025-03-13T10:23:13.518233Z",
     "shell.execute_reply.started": "2025-03-13T10:23:13.514863Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### measuring-hate-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T00:21:59.454287Z",
     "iopub.status.busy": "2025-03-13T00:21:59.453865Z",
     "iopub.status.idle": "2025-03-13T00:22:20.949624Z",
     "shell.execute_reply": "2025-03-13T00:22:20.948543Z",
     "shell.execute_reply.started": "2025-03-13T00:21:59.454256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hate speech score statistics:\n",
      "count    135556.000000\n",
      "mean         -0.567428\n",
      "std           2.380003\n",
      "min          -8.340000\n",
      "25%          -2.330000\n",
      "50%          -0.340000\n",
      "75%           1.410000\n",
      "max           6.300000\n",
      "Name: hate_speech_score, dtype: float64\n",
      "Hate label distribution:\n",
      "hate_label\n",
      "0    86508\n",
      "1    49048\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Abuse label distribution:\n",
      "abuse_label\n",
      "0    74954\n",
      "1    60602\n",
      "Name: count, dtype: int64\n",
      "Processed splits saved:\n",
      "Hate folder -> Data/Measuring_dat_hate\n",
      "Abuse folder -> Data/Measuring_dat_abuse\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import datasets  # Hugging Face datasets library\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "\n",
    "# --- Load the Measuring Hate Speech dataset using Hugging Face ---\n",
    "dataset = datasets.load_dataset('ucberkeley-dlab/measuring-hate-speech')\n",
    "df = dataset['train'].to_pandas()\n",
    "\n",
    "# Clean the text column\n",
    "df['text'] = df['text'].astype(str).apply(clean_text)\n",
    "\n",
    "# --- Inspect the hate speech score distribution ---\n",
    "print(\"Hate speech score statistics:\")\n",
    "print(df['hate_speech_score'].describe())\n",
    "\n",
    "# --- Define final labels ---\n",
    "# For hate speech, use a stricter threshold (> 0.5)\n",
    "df['hate_label'] = (df['hate_speech_score'] > 0.5).astype(int)\n",
    "# For abuse (a broader notion), use a lower threshold (> 0)\n",
    "df['abuse_label'] = (df['hate_speech_score'] > 0).astype(int)\n",
    "\n",
    "print(\"Hate label distribution:\")\n",
    "print(df['hate_label'].value_counts())\n",
    "print(\"\\nAbuse label distribution:\")\n",
    "print(df['abuse_label'].value_counts())\n",
    "\n",
    "# --- Split the dataset (80/10/10) ---\n",
    "# We stratify by hate_label here; you could also stratify by abuse_label if desired.\n",
    "train, val_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['hate_label'])\n",
    "val, test = train_test_split(val_test, test_size=0.5, random_state=42, stratify=val_test['hate_label'])\n",
    "\n",
    "# --- For hate version, keep only \"text\" and \"hate_label\", then rename hate_label -> label ---\n",
    "train_hate = train[['text', 'hate_label']].copy().rename(columns={'hate_label': 'label'})\n",
    "val_hate   = val[['text', 'hate_label']].copy().rename(columns={'hate_label': 'label'})\n",
    "test_hate  = test[['text', 'hate_label']].copy().rename(columns={'hate_label': 'label'})\n",
    "\n",
    "# --- For abuse version, keep only \"text\" and \"abuse_label\", then rename abuse_label -> label ---\n",
    "train_abuse = train[['text', 'abuse_label']].copy().rename(columns={'abuse_label': 'label'})\n",
    "val_abuse   = val[['text', 'abuse_label']].copy().rename(columns={'abuse_label': 'label'})\n",
    "test_abuse  = test[['text', 'abuse_label']].copy().rename(columns={'abuse_label': 'label'})\n",
    "\n",
    "# --- Create output folders and save ---\n",
    "hate_output_folder = \"Data/Measuring_dat_hate\"\n",
    "abuse_output_folder = \"Data/Measuring_dat_abuse\"\n",
    "\n",
    "os.makedirs(hate_output_folder, exist_ok=True)\n",
    "os.makedirs(abuse_output_folder, exist_ok=True)\n",
    "\n",
    "train_hate.to_csv(os.path.join(hate_output_folder, \"train.csv\"), index=False)\n",
    "val_hate.to_csv(os.path.join(hate_output_folder, \"valid.csv\"), index=False)\n",
    "test_hate.to_csv(os.path.join(hate_output_folder, \"test.csv\"), index=False)\n",
    "\n",
    "train_abuse.to_csv(os.path.join(abuse_output_folder, \"train.csv\"), index=False)\n",
    "val_abuse.to_csv(os.path.join(abuse_output_folder, \"valid.csv\"), index=False)\n",
    "test_abuse.to_csv(os.path.join(abuse_output_folder, \"test.csv\"), index=False)\n",
    "\n",
    "print(\"Processed splits saved:\")\n",
    "print(\"Hate folder ->\", hate_output_folder)\n",
    "print(\"Abuse folder ->\", abuse_output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T00:19:31.318934Z",
     "iopub.status.busy": "2025-03-13T00:19:31.318540Z",
     "iopub.status.idle": "2025-03-13T00:19:31.327925Z",
     "shell.execute_reply": "2025-03-13T00:19:31.326851Z",
     "shell.execute_reply.started": "2025-03-13T00:19:31.318899Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>abuse_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78120</th>\n",
       "      <td>did you put glue on your face and then suck your own dick?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97542</th>\n",
       "      <td>trump is the only jews true friend. i hope trump will be the next us president and help israel to kill all palestinian fucking muslim. we will kill all muslim in the world.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132306</th>\n",
       "      <td>i, a catholic and a jesuit, am grateful for this muslim holy season for challenging me to be a more dedicated child of god. url</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114534</th>\n",
       "      <td>geez..... i think #northkorea may be right. #barackobama is a monkey! surely acts like one.   url\"</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102697</th>\n",
       "      <td>at 7:19 is why i hate people, women especially look how mad she got just because the dog splashed her.. f*** you you stupid b**** either have fun or go kill yourself</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                text  \\\n",
       "78120                                                                                                                     did you put glue on your face and then suck your own dick?   \n",
       "97542   trump is the only jews true friend. i hope trump will be the next us president and help israel to kill all palestinian fucking muslim. we will kill all muslim in the world.   \n",
       "132306                                               i, a catholic and a jesuit, am grateful for this muslim holy season for challenging me to be a more dedicated child of god. url   \n",
       "114534                                                                            geez..... i think #northkorea may be right. #barackobama is a monkey! surely acts like one.   url\"   \n",
       "102697         at 7:19 is why i hate people, women especially look how mad she got just because the dog splashed her.. f*** you you stupid b**** either have fun or go kill yourself   \n",
       "\n",
       "        abuse_label  \n",
       "78120             0  \n",
       "97542             1  \n",
       "132306            0  \n",
       "114534            1  \n",
       "102697            1  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_abuse.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hate speech classifier train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:22:13.202499Z",
     "iopub.status.busy": "2025-03-18T12:22:13.202310Z",
     "iopub.status.idle": "2025-03-18T12:22:25.557740Z",
     "shell.execute_reply": "2025-03-18T12:22:25.556764Z",
     "shell.execute_reply.started": "2025-03-18T12:22:13.202482Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.47.0\n",
      "    Uninstalling transformers-4.47.0:\n",
      "      Successfully uninstalled transformers-4.47.0\n",
      "Successfully installed transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:32:34.395197Z",
     "iopub.status.busy": "2025-03-18T12:32:34.394812Z",
     "iopub.status.idle": "2025-03-18T12:32:34.405698Z",
     "shell.execute_reply": "2025-03-18T12:32:34.404928Z",
     "shell.execute_reply.started": "2025-03-18T12:32:34.395174Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import argparse\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "\n",
    "import os\n",
    "import random\n",
    "#os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:23:30.965166Z",
     "iopub.status.busy": "2025-03-18T12:23:30.964886Z",
     "iopub.status.idle": "2025-03-18T12:23:30.970048Z",
     "shell.execute_reply": "2025-03-18T12:23:30.969097Z",
     "shell.execute_reply.started": "2025-03-18T12:23:30.965146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HateDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:26:10.233241Z",
     "iopub.status.busy": "2025-03-18T12:26:10.232831Z",
     "iopub.status.idle": "2025-03-18T12:26:10.238674Z",
     "shell.execute_reply": "2025-03-18T12:26:10.237904Z",
     "shell.execute_reply.started": "2025-03-18T12:26:10.233211Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WeightedTrainer(Trainer):\n",
    "    def __init__(self, class_weights, **kwargs):\n",
    "       # self.class_weights = torch.FloatTensor(class_weights)\n",
    "        self.weighted_loss = torch.nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights)).to(DEVICE)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        loss = self.weighted_loss(logits, labels)\n",
    "        if return_outputs:\n",
    "            return loss, outputs\n",
    "        else:\n",
    "            return loss\n",
    "    # def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "    #     labels = inputs.pop(\"labels\")\n",
    "    #     outputs = model(**inputs)\n",
    "    #     logits = outputs[0]\n",
    "    #     loss = self.weighted_loss(logits, labels)\n",
    "    #     if return_outputs:\n",
    "    #         return loss, outputs\n",
    "    #     return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:33:26.048985Z",
     "iopub.status.busy": "2025-03-18T12:33:26.048619Z",
     "iopub.status.idle": "2025-03-18T12:33:26.056554Z",
     "shell.execute_reply": "2025-03-18T12:33:26.055571Z",
     "shell.execute_reply.started": "2025-03-18T12:33:26.048952Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_datasets(data_dir):\n",
    "    \n",
    "    train_df = pd.read_csv(data_dir + \"/train.csv\",engine=\"python\")\n",
    "    train_df = train_df.dropna()\n",
    "    train_df = train_df.dropna(subset=['label'])\n",
    "    valid_df = pd.read_csv(data_dir + \"/valid.csv\",engine=\"python\")\n",
    "    valid_df = valid_df.dropna()\n",
    "    valid_df = valid_df.dropna(subset=['label'])\n",
    "    test_df = pd.read_csv(data_dir + \"/test.csv\",engine=\"python\")\n",
    "    test_df = test_df.dropna()\n",
    "    test_df = test_df.dropna(subset=['label'])\n",
    "    \n",
    "    \n",
    "    train_texts = train_df['text'].astype(\"string\").tolist()\n",
    "    valid_texts = valid_df['text'].astype(\"string\").tolist()\n",
    "    test_texts = test_df['text'].astype(\"string\").tolist()\n",
    "\n",
    "    train_labels = train_df['label'].astype(\"int\").tolist()\n",
    "    valid_labels = valid_df['label'].astype(\"int\").tolist()\n",
    "    test_labels = test_df['label'].astype(\"int\").tolist()\n",
    "\n",
    "    # tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    # add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "    special_tokens_dict = {'additional_special_tokens': ['[USER]', '[EMOJI]', '[URL]']}\n",
    "    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "    train_encodings = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    valid_encodings = tokenizer(valid_texts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
    "    test_encodings = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "    train_dataset = HateDataset(train_encodings, train_labels)\n",
    "    valid_dataset = HateDataset(valid_encodings, valid_labels)\n",
    "    test_dataset = HateDataset(test_encodings, test_labels)\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset, len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:33:29.792253Z",
     "iopub.status.busy": "2025-03-18T12:33:29.791925Z",
     "iopub.status.idle": "2025-03-18T12:33:29.798983Z",
     "shell.execute_reply": "2025-03-18T12:33:29.797971Z",
     "shell.execute_reply.started": "2025-03-18T12:33:29.792226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def calculate_class_weights(data_dir):\n",
    "    dataset = pd.read_csv(data_dir + \"/train.csv\",engine=\"python\")\n",
    "    dataset = dataset.dropna(subset=['label'])\n",
    "    train_labels = dataset.label.to_numpy()\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "    print(\"class weights are {}\".format(class_weights))\n",
    "    return class_weights\n",
    "\n",
    "\n",
    "def train_model(train_dataset, valid_dataset, tok_len,  class_weights, output_dir, learning_rate, num_epochs, batch_size):\n",
    "    training_args = TrainingArguments(\n",
    "        save_steps=2500,\n",
    "        output_dir=output_dir,  # output directory\n",
    "        num_train_epochs=num_epochs,  # total number of training epochs\n",
    "        per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "        per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "        warmup_steps=500,  # number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # strength of weight decay\n",
    "        learning_rate=learning_rate,\n",
    "        seed=123,\n",
    "        logging_dir=os.path.join(output_dir, \"logs\"),\n",
    "        logging_steps=50,          # log every 50 steps\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=250,\n",
    "    )\n",
    "\n",
    "    # model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\").to(DEVICE)\n",
    "    model = model.to(DEVICE)\n",
    "    model.resize_token_embeddings(tok_len)\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        class_weights=class_weights,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=valid_dataset\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "        print(\"resuming from checkpoint...\")\n",
    "    except ValueError:\n",
    "        print(\"No checkpoints found. training from scratch...\")\n",
    "        trainer.train()\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T11:13:28.677713Z",
     "iopub.status.busy": "2025-03-13T11:13:28.677396Z",
     "iopub.status.idle": "2025-03-13T11:13:28.681279Z",
     "shell.execute_reply": "2025-03-13T11:13:28.680252Z",
     "shell.execute_reply.started": "2025-03-13T11:13:28.677686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"Data/Dynamic_hate/train.csv\", engine=\"python\", encoding=\"utf-8\")\n",
    "# # df['label'].dropna()\n",
    "# df = df.dropna(subset=['label'])\n",
    "# # dataset = pd.read_csv(data_dir + \"/train.csv\",engine=\"python\")\n",
    "# train_labels = df.label.to_numpy()\n",
    "# # class_weights = compute_class_weight('balanced', classes=np.unique(train_labels), y=train_labels)\n",
    "# print(np.unique(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T11:13:29.786528Z",
     "iopub.status.busy": "2025-03-13T11:13:29.786250Z",
     "iopub.status.idle": "2025-03-13T11:13:29.790139Z",
     "shell.execute_reply": "2025-03-13T11:13:29.789243Z",
     "shell.execute_reply.started": "2025-03-13T11:13:29.786507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train_labels[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-18T12:33:33.674499Z",
     "iopub.status.busy": "2025-03-18T12:33:33.674155Z",
     "iopub.status.idle": "2025-03-18T12:34:09.753157Z",
     "shell.execute_reply": "2025-03-18T12:34:09.751866Z",
     "shell.execute_reply.started": "2025-03-18T12:33:33.674464Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f94c477c9347db816aa5b132032997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e20787a18b9440eea9a25ca883c85587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8de9dc1fe824963bdeb87c2d10156ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69fa96d4086a401e9d1ab461d672d65d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class weights are [1.08416754 0.92795941]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a0f7e0d04540c0b8d2f4706b46ce64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No checkpoints found. training from scratch...\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1684498d7d91>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataset, valid_dataset, tok_len, class_weights, output_dir, learning_rate, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"resuming from checkpoint...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2208\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresume_from_checkpoint\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2209\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No valid checkpoint found in output directory ({args.output_dir})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No valid checkpoint found in output directory (Weights/eng_classif/Dynamic_hate)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7a6a2a3614e0>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_class_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdd_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     trainer = train_model(train_dataset,\n\u001b[0m\u001b[1;32m     20\u001b[0m                           \u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                           \u001b[0mtok_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-1684498d7d91>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataset, valid_dataset, tok_len, class_weights, output_dir, learning_rate, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No checkpoints found. training from scratch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2239\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2242\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2243\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2453\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2455\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2457\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    558\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    914\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    844\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1288\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1289\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1290\u001b[0;31m         wi.setup(\n\u001b[0m\u001b[1;32m   1291\u001b[0m             \u001b[0minit_settings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, init_settings, config, config_exclude_keys, config_include_keys, allow_val_change, monitor_gym)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offline\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_noop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             wandb_login._login(\n\u001b[0m\u001b[1;32m    290\u001b[0m                 \u001b[0manonymous\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manonymous\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m                 \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforce\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_login\u001b[0;34m(anonymous, key, relogin, host, force, timeout, _backend, _silent, _disable_warning, _entity)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwlogin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_key\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;34m\"\"\"Updates the global API key by prompting the user.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prompt_api_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mApiKeyStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNOTTY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             directive = (\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_login.py\u001b[0m in \u001b[0;36m_prompt_api_key\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m                 key = apikey.prompt_api_key(\n\u001b[0m\u001b[1;32m    244\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m                     \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/apikey.py\u001b[0m in \u001b[0;36mprompt_api_key\u001b[0;34m(settings, api, input_callback, browser_callback, no_offline, no_create, local)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mjupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"google.colab\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mlog_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOG_STRING_NOCOLOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattempt_colab_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapp_url\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mwrite_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mattempt_colab_login\u001b[0;34m(app_url)\u001b[0m\n\u001b[1;32m    334\u001b[0m     )\n\u001b[1;32m    335\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_wandbApiKey\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_dir = \"Data/\"\n",
    "# datasets = [\"CAD_hate\", \"CAD_abuse\", \"Founta_hate\", \"Founta_abuse\", \"Davidson_hate\", \"Davidson_abuse\"]\n",
    "# datasets = [\"CAD_hate\", \"CAD_abuse\"]\n",
    "# datasets = [\"CAD_hate\", \"CAD_abuse\", \"Davidson_hate\", \"Davidson_abuse\", \"Dynamic_hate\", \"Measuring_dat_hate\", \"Measuring_dat_abuse\"]\n",
    "datasets = [ \"Dynamic_hate\", \"Measuring_dat_hate\", \"Measuring_dat_abuse\"]\n",
    "\n",
    "output_dir = \"Weights/eng_classif/\"\n",
    "num_epochs = 5\n",
    "batch_size = 16\n",
    "learning_rate = 5e-5\n",
    "\n",
    "for dataset in datasets:\n",
    "    dd_dir = dataset_dir + dataset\n",
    "    oo_dir = output_dir + dataset\n",
    "\n",
    "    train_dataset, valid_dataset, test_dataset, tok_len = create_datasets(dd_dir)\n",
    "    class_weights = calculate_class_weights(dd_dir)\n",
    "\n",
    "    trainer = train_model(train_dataset,\n",
    "                          valid_dataset,\n",
    "                          tok_len,\n",
    "                          class_weights,\n",
    "                          oo_dir,\n",
    "                          learning_rate,\n",
    "                          num_epochs,\n",
    "                          batch_size)\n",
    "    \n",
    "    trainer.save_model(oo_dir)\n",
    "    \n",
    "    print(\"Training done, evaluating...\")\n",
    "    valid_preds = np.argmax(trainer.predict(valid_dataset)[0], axis=1) #should be numpy ndarray\n",
    "    valid_labels = np.array(valid_dataset.labels)\n",
    "\n",
    "    cls_report_valid = classification_report(valid_labels, valid_preds, output_dict=True)\n",
    "    pickle.dump(cls_report_valid, open(oo_dir + \"/cls_report_valid.pickle\", \"wb\"))\n",
    "\n",
    "    test_preds = np.argmax(trainer.predict(test_dataset)[0], axis=1)\n",
    "    test_labels = np.array(test_dataset.labels)\n",
    "    \n",
    "    cls_report_test = classification_report(test_labels, test_preds, output_dict=True)\n",
    "    pickle.dump(cls_report_test, open(oo_dir + \"/cls_report_test.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-13T11:10:36.196321Z",
     "iopub.status.busy": "2025-03-13T11:10:36.196043Z",
     "iopub.status.idle": "2025-03-13T11:10:36.695497Z",
     "shell.execute_reply": "2025-03-13T11:10:36.694464Z",
     "shell.execute_reply.started": "2025-03-13T11:10:36.196301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 13 11:10:36 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   71C    P0             34W /   70W |    1951MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  Tesla T4                       Off |   00000000:00:05.0 Off |                    0 |\n",
      "| N/A   41C    P8             11W /   70W |       3MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate_necc_and_suff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T16:04:48.926791Z",
     "iopub.status.busy": "2025-03-23T16:04:48.926485Z",
     "iopub.status.idle": "2025-03-23T16:04:48.932464Z",
     "shell.execute_reply": "2025-03-23T16:04:48.931547Z",
     "shell.execute_reply.started": "2025-03-23T16:04:48.926770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'Hate-Check-Necessity-Suffisciency'\n",
      "/kaggle/working/Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "%cd Hate-Check-Necessity-Suffisciency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:44:28.245248Z",
     "iopub.status.busy": "2025-03-23T11:44:28.244807Z",
     "iopub.status.idle": "2025-03-23T11:44:28.252651Z",
     "shell.execute_reply": "2025-03-23T11:44:28.251565Z",
     "shell.execute_reply.started": "2025-03-23T11:44:28.245211Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta\n"
     ]
    }
   ],
   "source": [
    "# %cd Weights/eng_classif/\n",
    "# !mkdir Weights/eng_classif_roberta/\n",
    "# %cd Weights/eng_classif_roberta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:30:23.671485Z",
     "iopub.status.busy": "2025-03-23T11:30:23.671127Z",
     "iopub.status.idle": "2025-03-23T11:31:12.575407Z",
     "shell.execute_reply": "2025-03-23T11:31:12.573980Z",
     "shell.execute_reply.started": "2025-03-23T11:30:23.671455Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1GO3GkRkbH-yUTNSEsFlEU6c_v9bQcjbA\n",
      "From (redirected): https://drive.google.com/uc?id=1GO3GkRkbH-yUTNSEsFlEU6c_v9bQcjbA&confirm=t&uuid=e1d4c3e1-739f-43dd-a976-56a73628b13c\n",
      "To: /kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/eng_classif_RoBerta.zip\n",
      "100%|███████████████████████████████████████| 6.08G/6.08G [00:44<00:00, 138MB/s]\n"
     ]
    }
   ],
   "source": [
    "# !gdown --id 1YUtTOkUwF5reY9wZTUD0Lc-i-KDz69AD\n",
    "# !gdown --id 1GO3GkRkbH-yUTNSEsFlEU6c_v9bQcjbA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:31:12.578008Z",
     "iopub.status.busy": "2025-03-23T11:31:12.577565Z",
     "iopub.status.idle": "2025-03-23T11:32:30.723569Z",
     "shell.execute_reply": "2025-03-23T11:32:30.722577Z",
     "shell.execute_reply.started": "2025-03-23T11:31:12.577961Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  eng_classif_RoBerta.zip\n",
      "   creating: Measuring_dat_hate/\n",
      "   creating: Measuring_dat_hate/checkpoint-33890/\n",
      "  inflating: Measuring_dat_hate/checkpoint-33890/config.json  \n",
      "  inflating: Measuring_dat_hate/checkpoint-33890/model.safetensors  \n",
      "  inflating: Measuring_dat_hate/checkpoint-33890/optimizer.pt  \n",
      "  inflating: Measuring_dat_hate/checkpoint-33890/rng_state.pth  \n",
      "  inflating: Measuring_dat_hate/checkpoint-33890/scheduler.pt  \n",
      "  inflating: Measuring_dat_hate/checkpoint-33890/trainer_state.json  \n",
      "  inflating: Measuring_dat_hate/checkpoint-33890/training_args.bin  \n",
      "  inflating: Measuring_dat_hate/cls_report_test.pickle  \n",
      "  inflating: Measuring_dat_hate/cls_report_valid.pickle  \n",
      "  inflating: Measuring_dat_hate/config.json  \n",
      "   creating: Measuring_dat_hate/logs/\n",
      "  inflating: Measuring_dat_hate/logs/events.out.tfevents.1742434786.05c94691d167.188.3  \n",
      "  inflating: Measuring_dat_hate/model.safetensors  \n",
      "  inflating: Measuring_dat_hate/training_args.bin  \n",
      "   creating: Dynamic_hate/\n",
      "   creating: Dynamic_hate/checkpoint-5000/\n",
      "  inflating: Dynamic_hate/checkpoint-5000/config.json  \n",
      "  inflating: Dynamic_hate/checkpoint-5000/model.safetensors  \n",
      "  inflating: Dynamic_hate/checkpoint-5000/optimizer.pt  \n",
      "  inflating: Dynamic_hate/checkpoint-5000/rng_state.pth  \n",
      "  inflating: Dynamic_hate/checkpoint-5000/scheduler.pt  \n",
      "  inflating: Dynamic_hate/checkpoint-5000/trainer_state.json  \n",
      "  inflating: Dynamic_hate/checkpoint-5000/training_args.bin  \n",
      "  inflating: Dynamic_hate/cls_report_test.pickle  \n",
      "  inflating: Dynamic_hate/cls_report_valid.pickle  \n",
      "  inflating: Dynamic_hate/config.json  \n",
      "   creating: Dynamic_hate/logs/\n",
      "  inflating: Dynamic_hate/logs/events.out.tfevents.1742414492.05c94691d167.188.0  \n",
      "  inflating: Dynamic_hate/logs/events.out.tfevents.1742414792.05c94691d167.188.1  \n",
      "  inflating: Dynamic_hate/logs/events.out.tfevents.1742416294.05c94691d167.188.2  \n",
      "  inflating: Dynamic_hate/model.safetensors  \n",
      "  inflating: Dynamic_hate/training_args.bin  \n",
      "   creating: Measuring_dat_abuse/\n",
      "   creating: Measuring_dat_abuse/checkpoint-25000/\n",
      "  inflating: Measuring_dat_abuse/checkpoint-25000/config.json  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-25000/model.safetensors  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-25000/optimizer.pt  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-25000/rng_state.pth  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-25000/scheduler.pt  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-25000/trainer_state.json  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-25000/training_args.bin  \n",
      "   creating: Measuring_dat_abuse/checkpoint-33890/\n",
      "  inflating: Measuring_dat_abuse/checkpoint-33890/config.json  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-33890/model.safetensors  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-33890/optimizer.pt  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-33890/rng_state.pth  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-33890/scheduler.pt  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-33890/trainer_state.json  \n",
      "  inflating: Measuring_dat_abuse/checkpoint-33890/training_args.bin  \n",
      "  inflating: Measuring_dat_abuse/cls_report_test.pickle  \n",
      "  inflating: Measuring_dat_abuse/cls_report_valid.pickle  \n",
      "  inflating: Measuring_dat_abuse/config.json  \n",
      "   creating: Measuring_dat_abuse/logs/\n",
      "  inflating: Measuring_dat_abuse/logs/events.out.tfevents.1742466656.05c94691d167.188.4  \n",
      "  inflating: Measuring_dat_abuse/logs/events.out.tfevents.1742489637.1675135f4262.177.0  \n",
      "  inflating: Measuring_dat_abuse/model.safetensors  \n",
      "  inflating: Measuring_dat_abuse/training_args.bin  \n"
     ]
    }
   ],
   "source": [
    "# !unzip eng_classif.zip\n",
    "# !unzip eng_classif_RoBerta.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:44:43.387564Z",
     "iopub.status.busy": "2025-03-23T11:44:43.387188Z",
     "iopub.status.idle": "2025-03-23T11:44:43.657452Z",
     "shell.execute_reply": "2025-03-23T11:44:43.656219Z",
     "shell.execute_reply.started": "2025-03-23T11:44:43.387535Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm eng_classif.zip\n",
    "# !rm -rf CAD_hate\n",
    "\n",
    "# !rm eng_classif_RoBerta.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:44:51.188843Z",
     "iopub.status.busy": "2025-03-23T11:44:51.188459Z",
     "iopub.status.idle": "2025-03-23T11:44:51.195106Z",
     "shell.execute_reply": "2025-03-23T11:44:51.193704Z",
     "shell.execute_reply.started": "2025-03-23T11:44:51.188812Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "# cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T16:04:55.853392Z",
     "iopub.status.busy": "2025-03-23T16:04:55.853102Z",
     "iopub.status.idle": "2025-03-23T16:05:20.963017Z",
     "shell.execute_reply": "2025-03-23T16:05:20.962275Z",
     "shell.execute_reply.started": "2025-03-23T16:04:55.853370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, RobertaTokenizerFast\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast, RobertaForSequenceClassification\n",
    "\n",
    "from random import shuffle\n",
    "from perturbation_functions import get_preds_and_scores, calc_suff, calc_necc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T16:05:20.964400Z",
     "iopub.status.busy": "2025-03-23T16:05:20.963901Z",
     "iopub.status.idle": "2025-03-23T16:05:20.982155Z",
     "shell.execute_reply": "2025-03-23T16:05:20.981119Z",
     "shell.execute_reply.started": "2025-03-23T16:05:20.964376Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perts = pickle.load(open(\"Data/HateCheck_necc_suff_perturbations.pickle\",\"rb\"))\n",
    "perts['orig_texts'] = [tt.strip(' \\n') for tt in perts['orig_texts']]\n",
    "perts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- distillbert/roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T16:05:20.984101Z",
     "iopub.status.busy": "2025-03-23T16:05:20.983777Z",
     "iopub.status.idle": "2025-03-23T16:05:23.453815Z",
     "shell.execute_reply": "2025-03-23T16:05:23.453045Z",
     "shell.execute_reply.started": "2025-03-23T16:05:20.984078Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12563f3d8d4145f88dfbf4665c7f0ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DELL\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\DELL\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63845afb027e4a73a2a8570477351d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72c4a3c39a0478e8b00ae5c76e712d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "787aba00e29c434f84d01e7609095b63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276c511beadd462780b93ef4e0aa3cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
    "\n",
    "# add special tokens for URLs, emojis and mentions (--> see pre-processing)\n",
    "special_tokens_dict = {'additional_special_tokens': ['[USER]','[EMOJI]','[URL]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# datasets = ['CAD_abuse', \n",
    "#             'Davidson_abuse', \n",
    "#             'Founta_abuse',\n",
    "#             'CAD_hate',\n",
    "#             'Davidson_hate',\n",
    "#             'Founta_hate']\n",
    "\n",
    "datasets = ['Measuring_dat_abuse', \n",
    "            'Measuring_dat_hate', \n",
    "            'Dynamic_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:48:14.870358Z",
     "iopub.status.busy": "2025-03-23T11:48:14.869995Z",
     "iopub.status.idle": "2025-03-23T12:49:31.335065Z",
     "shell.execute_reply": "2025-03-23T12:49:31.332692Z",
     "shell.execute_reply.started": "2025-03-23T11:48:14.870331Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Measuring_dat_abuse.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75c91b57162482f8d514fb9b3652450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Measuring_dat_hate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238cd8a628a54f10932d4d646066e998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying HateCheck perturbations with Dynamic_hate.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b74e38a7321f4597a6b10c0da4f8a5f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "orig_preds = {}\n",
    "orig_scores = {}\n",
    "necc_preds = {}\n",
    "necc_scores = {}\n",
    "suff_preds = {}\n",
    "suff_scores = {}\n",
    "\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Classifying HateCheck perturbations with {}.\".format(dataset))\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"Weights/eng_classif_roberta/{}\".format(dataset))\n",
    "    # model = DistilBertForSequenceClassification.from_pretrained(\"Weights/eng_classif/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    \n",
    "    total_len = len(perts['orig_texts']) + sum(len(nn) for nn in perts['necc_perturbed']) + sum(len(nn) for nn in perts['suff_perturbed'])\n",
    " \n",
    "    with tqdm(total=total_len) as pbar:\n",
    "        orig_preds[dataset], orig_scores[dataset] = get_preds_and_scores(perts['orig_texts'], tokenizer, model, pbar)\n",
    "        \n",
    "        necc_preds[dataset] = []\n",
    "        necc_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['necc_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            necc_preds[dataset].append(pp)\n",
    "            necc_scores[dataset].append(ss)\n",
    "            \n",
    "        suff_preds[dataset] = []\n",
    "        suff_scores[dataset] = []\n",
    "    \n",
    "        for tt in perts['suff_perturbed']:\n",
    "            pp, ss = get_preds_and_scores(tt, tokenizer, model, pbar)\n",
    "            suff_preds[dataset].append(pp)\n",
    "            suff_scores[dataset].append(ss)\n",
    "            \n",
    "        \n",
    "final_results = {\n",
    "                'orig_preds': orig_preds,\n",
    "                'orig_scores': orig_scores,\n",
    "                'necc_preds': necc_preds,\n",
    "                'necc_scores': necc_scores,\n",
    "                'suff_preds': suff_preds,\n",
    "                'suff_scores': suff_scores,\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:50:32.573118Z",
     "iopub.status.busy": "2025-03-23T12:50:32.572706Z",
     "iopub.status.idle": "2025-03-23T12:50:32.577453Z",
     "shell.execute_reply": "2025-03-23T12:50:32.576368Z",
     "shell.execute_reply.started": "2025-03-23T12:50:32.573081Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !mkdir Data/roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:50:35.213480Z",
     "iopub.status.busy": "2025-03-23T12:50:35.213120Z",
     "iopub.status.idle": "2025-03-23T12:50:35.228123Z",
     "shell.execute_reply": "2025-03-23T12:50:35.227054Z",
     "shell.execute_reply.started": "2025-03-23T12:50:35.213454Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pickle.dump(final_results, open(\"Data/roberta/HateCheck_necc_suff_preds.pickle\", \"wb\"))\n",
    "# pickle.dump(final_results, open(\"Data/HateCheck_necc_suff_preds.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T16:06:12.460522Z",
     "iopub.status.busy": "2025-03-23T16:06:12.460217Z",
     "iopub.status.idle": "2025-03-23T16:06:12.967570Z",
     "shell.execute_reply": "2025-03-23T16:06:12.966742Z",
     "shell.execute_reply.started": "2025-03-23T16:06:12.460501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(\"ilm/data/ILM/compound_dataset/train.txt\", \"r\") as ff:\n",
    "    compound_dataset = ff.read().split(\"\\n\\n\\n\")\n",
    "compound_dataset = [tt.strip(\" :`.,\") for tt in compound_dataset]\n",
    "shuffle(compound_dataset)\n",
    "compound_dataset = compound_dataset[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T16:06:14.400766Z",
     "iopub.status.busy": "2025-03-23T16:06:14.400473Z",
     "iopub.status.idle": "2025-03-23T16:06:14.406140Z",
     "shell.execute_reply": "2025-03-23T16:06:14.405351Z",
     "shell.execute_reply.started": "2025-03-23T16:06:14.400746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i will. the book a glossary of the construction, decoration and use of arms and armor, by george cameron stone (isbn 0486407268) shows records of back scabbards in chinese and korean armories, and in some mongol weapons depictions as well. however, there is evidence in guy windsor swordsman companion that european and near east cultures never used a back scabbard. at best, it would be a baldric slung from a shoulder which was probably incorrectly drawn. i have combined the information as best as i could to form a cohesive fit.|'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compound_dataset[4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T16:06:35.433606Z",
     "iopub.status.busy": "2025-03-23T16:06:35.433310Z",
     "iopub.status.idle": "2025-03-23T17:23:15.612145Z",
     "shell.execute_reply": "2025-03-23T17:23:15.611125Z",
     "shell.execute_reply.started": "2025-03-23T16:06:35.433584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "baseline_preds = {}\n",
    "baseline_scores = {}\n",
    "for dataset in datasets: \n",
    "    # model = DistilBertForSequenceClassification.from_pretrained(\"Weights/eng_classif/{}\".format(dataset))\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"Weights/eng_classif_roberta/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    preds, scores = get_preds_and_scores(compound_dataset, tokenizer, model, batchsize=16 )\n",
    "    baseline_preds[dataset] = sum(preds)/len(preds)\n",
    "    baseline_scores[dataset] = sum(scores)/len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:23:55.935085Z",
     "iopub.status.busy": "2025-03-23T17:23:55.934707Z",
     "iopub.status.idle": "2025-03-23T17:23:55.940232Z",
     "shell.execute_reply": "2025-03-23T17:23:55.939348Z",
     "shell.execute_reply.started": "2025-03-23T17:23:55.935056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Measuring_dat_abuse': 0.0348,\n",
       " 'Measuring_dat_hate': 0.1054,\n",
       " 'Dynamic_hate': 0.0938}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:24:07.949836Z",
     "iopub.status.busy": "2025-03-23T17:24:07.949524Z",
     "iopub.status.idle": "2025-03-23T17:24:07.954523Z",
     "shell.execute_reply": "2025-03-23T17:24:07.953612Z",
     "shell.execute_reply.started": "2025-03-23T17:24:07.949793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pickle.dump({'baseline_preds':baseline_preds, 'baseline_scores':baseline_scores}, open(\"Classifier_baselines_Roberta.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:26:24.844240Z",
     "iopub.status.busy": "2025-03-23T17:26:24.843944Z",
     "iopub.status.idle": "2025-03-23T17:26:24.856053Z",
     "shell.execute_reply": "2025-03-23T17:26:24.855177Z",
     "shell.execute_reply.started": "2025-03-23T17:26:24.844218Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_results = pickle.load(open(\"Data/roberta/HateCheck_necc_suff_preds.pickle\", \"rb\"))\n",
    "# print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:26:28.777411Z",
     "iopub.status.busy": "2025-03-23T17:26:28.777098Z",
     "iopub.status.idle": "2025-03-23T17:26:28.870464Z",
     "shell.execute_reply": "2025-03-23T17:26:28.869849Z",
     "shell.execute_reply.started": "2025-03-23T17:26:28.777386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "necc_results = {}\n",
    "necc_results_nb = {}\n",
    "suff_results = {}\n",
    "suff_results_nb = {}\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    ## NECCESSITY CALCULATIONS\n",
    "    neccs = []\n",
    "    for oo, pp, mm in zip(final_results['orig_preds'][dataset], \n",
    "                          final_results['necc_preds'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs.append(calc_necc(oo, pp, mm))\n",
    "    necc_results[dataset] = neccs \n",
    "    \n",
    "    neccs_nb = []\n",
    "    for oo, pp, mm in zip(final_results['orig_scores'][dataset], \n",
    "                          final_results['necc_scores'][dataset], \n",
    "                          perts['necc_masks']):\n",
    "        pp = np.array(pp)\n",
    "        neccs_nb.append(calc_necc(oo, pp, mm))\n",
    "    necc_results_nb[dataset] = neccs_nb\n",
    "    \n",
    "    ## SUFFICIENCY CALCULATIONS\n",
    "    baseline_pred = baseline_preds[dataset]\n",
    "    baseline_score = baseline_scores[dataset]\n",
    "    \n",
    "    suffs = []\n",
    "    for pp, mm in zip(final_results['suff_preds'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs.append(calc_suff(baseline_pred, pp, mm))\n",
    "    suff_results[dataset] = suffs \n",
    "    \n",
    "    suffs_nb = []\n",
    "    for pp, mm in zip(final_results['suff_scores'][dataset], perts['suff_masks']):\n",
    "        pp = np.array(pp)\n",
    "        suffs_nb.append(calc_suff(baseline_score, pp, mm))\n",
    "    suff_results_nb[dataset] = suffs_nb     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T17:26:35.106460Z",
     "iopub.status.busy": "2025-03-23T17:26:35.106137Z",
     "iopub.status.idle": "2025-03-23T17:26:35.124045Z",
     "shell.execute_reply": "2025-03-23T17:26:35.123324Z",
     "shell.execute_reply.started": "2025-03-23T17:26:35.106435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "hatecheck_necc_suff_results = {\n",
    "    'necc_results': necc_results,\n",
    "    'necc_results_nb': necc_results_nb,\n",
    "    'suff_results': suff_results, \n",
    "    'suff_results_nb': suff_results_nb\n",
    "}\n",
    "\n",
    "# pickle.dump(hatecheck_necc_suff_results, open('Data/HateCheck_necc_suff_results_all.pickle', 'wb'))\n",
    "pickle.dump(hatecheck_necc_suff_results, open('Data/roberta/HateCheck_necc_suff_results_all.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:17:57.156690Z",
     "iopub.status.busy": "2025-03-23T15:17:57.156381Z",
     "iopub.status.idle": "2025-03-23T15:17:57.183347Z",
     "shell.execute_reply": "2025-03-23T15:17:57.182590Z",
     "shell.execute_reply.started": "2025-03-23T15:17:57.156665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Now get the predictions for all models for the entire hatecheck suite\n",
    "hc_test_cases_all = pd.read_csv(\"hatecheck-data/test_suite_cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:17:59.810543Z",
     "iopub.status.busy": "2025-03-23T15:17:59.810247Z",
     "iopub.status.idle": "2025-03-23T15:17:59.843368Z",
     "shell.execute_reply": "2025-03-23T15:17:59.842593Z",
     "shell.execute_reply.started": "2025-03-23T15:17:59.810519Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>functionality</th>\n",
       "      <th>case_id</th>\n",
       "      <th>test_case</th>\n",
       "      <th>label_gold</th>\n",
       "      <th>target_ident</th>\n",
       "      <th>direction</th>\n",
       "      <th>focus_words</th>\n",
       "      <th>focus_lemma</th>\n",
       "      <th>ref_case_id</th>\n",
       "      <th>ref_templ_id</th>\n",
       "      <th>templ_id</th>\n",
       "      <th>case_templ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate women.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate trans people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>3</td>\n",
       "      <td>I hate gay people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>4</td>\n",
       "      <td>I hate black people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>5</td>\n",
       "      <td>I hate disabled people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      functionality  case_id                 test_case  \\\n",
       "0           0  derog_neg_emote_h        1            I hate women.    \n",
       "1           1  derog_neg_emote_h        2     I hate trans people.    \n",
       "2           2  derog_neg_emote_h        3       I hate gay people.    \n",
       "3           3  derog_neg_emote_h        4     I hate black people.    \n",
       "4           4  derog_neg_emote_h        5  I hate disabled people.    \n",
       "\n",
       "  label_gold     target_ident direction focus_words focus_lemma  ref_case_id  \\\n",
       "0    hateful            women   general        hate        hate          NaN   \n",
       "1    hateful     trans people   general        hate        hate          NaN   \n",
       "2    hateful       gay people   general        hate        hate          NaN   \n",
       "3    hateful     black people   general        hate        hate          NaN   \n",
       "4    hateful  disabled people   general        hate        hate          NaN   \n",
       "\n",
       "   ref_templ_id  templ_id            case_templ  \n",
       "0           NaN         1  I hate [IDENTITY_P].  \n",
       "1           NaN         1  I hate [IDENTITY_P].  \n",
       "2           NaN         1  I hate [IDENTITY_P].  \n",
       "3           NaN         1  I hate [IDENTITY_P].  \n",
       "4           NaN         1  I hate [IDENTITY_P].  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc_test_cases_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:38:47.265576Z",
     "iopub.status.busy": "2025-03-23T15:38:47.265283Z",
     "iopub.status.idle": "2025-03-23T15:38:47.274216Z",
     "shell.execute_reply": "2025-03-23T15:38:47.273233Z",
     "shell.execute_reply.started": "2025-03-23T15:38:47.265554Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>functionality</th>\n",
       "      <th>case_id</th>\n",
       "      <th>test_case</th>\n",
       "      <th>label_gold</th>\n",
       "      <th>target_ident</th>\n",
       "      <th>direction</th>\n",
       "      <th>focus_words</th>\n",
       "      <th>focus_lemma</th>\n",
       "      <th>ref_case_id</th>\n",
       "      <th>ref_templ_id</th>\n",
       "      <th>templ_id</th>\n",
       "      <th>case_templ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate women.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>2</td>\n",
       "      <td>I hate trans people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>3</td>\n",
       "      <td>I hate gay people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>4</td>\n",
       "      <td>I hate black people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>derog_neg_emote_h</td>\n",
       "      <td>5</td>\n",
       "      <td>I hate disabled people.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>hate</td>\n",
       "      <td>hate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate [IDENTITY_P].</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723</th>\n",
       "      <td>3723</td>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3897</td>\n",
       "      <td>Gay people are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1232.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>3724</td>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3898</td>\n",
       "      <td>Black people are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1233.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>3725</td>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3899</td>\n",
       "      <td>Disabled people are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1234.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>3726</td>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3900</td>\n",
       "      <td>Muslims are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1235.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>3727</td>\n",
       "      <td>spell_leet_h</td>\n",
       "      <td>3901</td>\n",
       "      <td>Immigrants are all complete 4ssholes.</td>\n",
       "      <td>hateful</td>\n",
       "      <td>immigrants</td>\n",
       "      <td>general</td>\n",
       "      <td>4ssholes</td>\n",
       "      <td>asshole</td>\n",
       "      <td>1236.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>866</td>\n",
       "      <td>[IDENTITY_P] are all complete 4ssholes.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3728 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      functionality  case_id  \\\n",
       "0              0  derog_neg_emote_h        1   \n",
       "1              1  derog_neg_emote_h        2   \n",
       "2              2  derog_neg_emote_h        3   \n",
       "3              3  derog_neg_emote_h        4   \n",
       "4              4  derog_neg_emote_h        5   \n",
       "...          ...                ...      ...   \n",
       "3723        3723       spell_leet_h     3897   \n",
       "3724        3724       spell_leet_h     3898   \n",
       "3725        3725       spell_leet_h     3899   \n",
       "3726        3726       spell_leet_h     3900   \n",
       "3727        3727       spell_leet_h     3901   \n",
       "\n",
       "                                        test_case label_gold     target_ident  \\\n",
       "0                                  I hate women.     hateful            women   \n",
       "1                           I hate trans people.     hateful     trans people   \n",
       "2                             I hate gay people.     hateful       gay people   \n",
       "3                           I hate black people.     hateful     black people   \n",
       "4                        I hate disabled people.     hateful  disabled people   \n",
       "...                                           ...        ...              ...   \n",
       "3723       Gay people are all complete 4ssholes.     hateful       gay people   \n",
       "3724     Black people are all complete 4ssholes.     hateful     black people   \n",
       "3725  Disabled people are all complete 4ssholes.     hateful  disabled people   \n",
       "3726          Muslims are all complete 4ssholes.     hateful          Muslims   \n",
       "3727       Immigrants are all complete 4ssholes.     hateful       immigrants   \n",
       "\n",
       "     direction focus_words focus_lemma  ref_case_id  ref_templ_id  templ_id  \\\n",
       "0      general        hate        hate          NaN           NaN         1   \n",
       "1      general        hate        hate          NaN           NaN         1   \n",
       "2      general        hate        hate          NaN           NaN         1   \n",
       "3      general        hate        hate          NaN           NaN         1   \n",
       "4      general        hate        hate          NaN           NaN         1   \n",
       "...        ...         ...         ...          ...           ...       ...   \n",
       "3723   general    4ssholes     asshole       1232.0         256.0       866   \n",
       "3724   general    4ssholes     asshole       1233.0         256.0       866   \n",
       "3725   general    4ssholes     asshole       1234.0         256.0       866   \n",
       "3726   general    4ssholes     asshole       1235.0         256.0       866   \n",
       "3727   general    4ssholes     asshole       1236.0         256.0       866   \n",
       "\n",
       "                                   case_templ  \n",
       "0                        I hate [IDENTITY_P].  \n",
       "1                        I hate [IDENTITY_P].  \n",
       "2                        I hate [IDENTITY_P].  \n",
       "3                        I hate [IDENTITY_P].  \n",
       "4                        I hate [IDENTITY_P].  \n",
       "...                                       ...  \n",
       "3723  [IDENTITY_P] are all complete 4ssholes.  \n",
       "3724  [IDENTITY_P] are all complete 4ssholes.  \n",
       "3725  [IDENTITY_P] are all complete 4ssholes.  \n",
       "3726  [IDENTITY_P] are all complete 4ssholes.  \n",
       "3727  [IDENTITY_P] are all complete 4ssholes.  \n",
       "\n",
       "[3728 rows x 13 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc_test_cases_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_preds_and_scores(texts, tokenizer, model, batchsize=64):\n",
    "    preds = []\n",
    "    scores = []\n",
    "    for i in tqdm(range(0, len(texts), batchsize), desc=\"Evaluating\"):\n",
    "        batch = texts[i:i+batchsize]\n",
    "        encodings = tokenizer(batch, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        outputs = model(**encodings)\n",
    "        ss = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        pp = torch.argmax(outputs.logits, dim=-1)\n",
    "        scores.extend(ss[:, 1].tolist())\n",
    "        preds.extend(pp.tolist())\n",
    "    return preds, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:18:03.541018Z",
     "iopub.status.busy": "2025-03-23T15:18:03.540715Z",
     "iopub.status.idle": "2025-03-23T15:20:49.532938Z",
     "shell.execute_reply": "2025-03-23T15:20:49.532006Z",
     "shell.execute_reply.started": "2025-03-23T15:18:03.540973Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 59/59 [03:06<00:00,  3.16s/it]\n",
      "Evaluating: 100%|██████████| 59/59 [05:23<00:00,  5.49s/it]\n",
      "Evaluating: 100%|██████████| 59/59 [04:24<00:00,  4.48s/it]\n"
     ]
    }
   ],
   "source": [
    "hc_test_cases_all = hc_test_cases_all.test_case.tolist()\n",
    "# hc_test_cases_all = hc_test_cases_all['test_case']\n",
    "\n",
    "hc_preds = {}\n",
    "hc_scores = {}\n",
    "for dataset in datasets: \n",
    "    # model = DistilBertForSequenceClassification.from_pretrained(\"Weights/eng_classif/{}\".format(dataset))\n",
    "    model = RobertaForSequenceClassification.from_pretrained(\"Weights/eng_classif_roberta/{}\".format(dataset))\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.eval()\n",
    "    # preds, scores = get_preds_and_scores(hc_test_cases_all, tokenizer, model)\n",
    "    preds, scores = get_preds_and_scores(hc_test_cases_all, tokenizer, model)\n",
    "    hc_preds[dataset] = preds\n",
    "    hc_scores[dataset] = scores\n",
    "\n",
    "# pickle.dump({'preds': hc_preds, 'scores':hc_scores}, open('Data/HateCheck_results_all_models.pickle', \"wb\"))\n",
    "pickle.dump({'preds': hc_preds, 'scores':hc_scores}, open('Data/roberta/HateCheck_results_all_models.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:38:54.775095Z",
     "iopub.status.busy": "2025-03-23T15:38:54.774765Z",
     "iopub.status.idle": "2025-03-23T15:38:54.783139Z",
     "shell.execute_reply": "2025-03-23T15:38:54.782314Z",
     "shell.execute_reply.started": "2025-03-23T15:38:54.775065Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                               I hate women. \n",
       "1                                                        I hate trans people. \n",
       "2                                                          I hate gay people. \n",
       "3                                                        I hate black people. \n",
       "4                                                     I hate disabled people. \n",
       "                                                   ...                        \n",
       "Measuring_dat_abuse_score    [0.016809698194265366, 0.014449874870479107, 0...\n",
       "Measuring_dat_hate_pred      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "Measuring_dat_hate_score     [0.12978467345237732, 0.12978672981262207, 0.1...\n",
       "Dynamic_hate_pred            [1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, ...\n",
       "Dynamic_hate_score           [0.999765932559967, 0.9997701048851013, 0.9997...\n",
       "Name: test_case, Length: 3734, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc_test_cases_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-23T15:34:18.238251Z",
     "iopub.status.busy": "2025-03-23T15:34:18.237923Z",
     "iopub.status.idle": "2025-03-23T15:34:18.266971Z",
     "shell.execute_reply": "2025-03-23T15:34:18.266272Z",
     "shell.execute_reply.started": "2025-03-23T15:34:18.238226Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Measuring_dat_abuse': [0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'Measuring_dat_hate': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  ...],\n",
       " 'Dynamic_hate': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hc_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:34:29.345027Z",
     "iopub.status.busy": "2025-03-23T15:34:29.344728Z",
     "iopub.status.idle": "2025-03-23T15:34:29.349947Z",
     "shell.execute_reply": "2025-03-23T15:34:29.349067Z",
     "shell.execute_reply.started": "2025-03-23T15:34:29.344981Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dynamic_hate'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:34:31.695197Z",
     "iopub.status.busy": "2025-03-23T15:34:31.694845Z",
     "iopub.status.idle": "2025-03-23T15:34:31.709375Z",
     "shell.execute_reply": "2025-03-23T15:34:31.708592Z",
     "shell.execute_reply.started": "2025-03-23T15:34:31.695167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    hc_test_cases_all['{}_pred'.format(dataset)] = hc_preds[dataset]\n",
    "    hc_test_cases_all['{}_score'.format(dataset)] = hc_scores[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T15:34:34.704643Z",
     "iopub.status.busy": "2025-03-23T15:34:34.704372Z",
     "iopub.status.idle": "2025-03-23T15:34:34.710294Z",
     "shell.execute_reply": "2025-03-23T15:34:34.709431Z",
     "shell.execute_reply.started": "2025-03-23T15:34:34.704620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(hc_test_cases_all, open('Data/HateCheck_templates_and_results.pickle', \"wb\"))\n",
    "pickle.dump(hc_test_cases_all, open('Data/roberta/HateCheck_templates_and_results.pickle', \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-19T20:41:02.209758Z",
     "iopub.status.busy": "2025-03-19T20:41:02.209388Z",
     "iopub.status.idle": "2025-03-19T20:41:02.217843Z",
     "shell.execute_reply": "2025-03-19T20:41:02.216733Z",
     "shell.execute_reply.started": "2025-03-19T20:41:02.209725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "%cd Hate-Check-Necessity-Suffisciency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:40:47.670447Z",
     "iopub.status.busy": "2025-03-20T11:40:47.669924Z",
     "iopub.status.idle": "2025-03-20T11:40:47.675548Z",
     "shell.execute_reply": "2025-03-20T11:40:47.674231Z",
     "shell.execute_reply.started": "2025-03-20T11:40:47.670401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:40:50.612651Z",
     "iopub.status.busy": "2025-03-20T11:40:50.612299Z",
     "iopub.status.idle": "2025-03-20T11:40:50.644326Z",
     "shell.execute_reply": "2025-03-20T11:40:50.643173Z",
     "shell.execute_reply.started": "2025-03-20T11:40:50.612624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "preds = pickle.load(open(\"Data/HateCheck_necc_suff_preds.pickle\", \"rb\"))\n",
    "results = pickle.load(open(\"Data/HateCheck_necc_suff_results_all.pickle\", \"rb\"))\n",
    "perturbations = pickle.load(open(\"Data/HateCheck_necc_suff_perturbations.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:40:53.797788Z",
     "iopub.status.busy": "2025-03-20T11:40:53.797416Z",
     "iopub.status.idle": "2025-03-20T11:40:53.804942Z",
     "shell.execute_reply": "2025-03-20T11:40:53.803718Z",
     "shell.execute_reply.started": "2025-03-20T11:40:53.797761Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_texts', 'necc_perturbed', 'suff_perturbed', 'necc_masks', 'suff_masks'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perturbations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:40:56.167772Z",
     "iopub.status.busy": "2025-03-20T11:40:56.167439Z",
     "iopub.status.idle": "2025-03-20T11:40:56.174263Z",
     "shell.execute_reply": "2025-03-20T11:40:56.173314Z",
     "shell.execute_reply.started": "2025-03-20T11:40:56.167748Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['orig_preds', 'orig_scores', 'necc_preds', 'necc_scores', 'suff_preds', 'suff_scores'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:40:56.995436Z",
     "iopub.status.busy": "2025-03-20T11:40:56.995024Z",
     "iopub.status.idle": "2025-03-20T11:40:57.002008Z",
     "shell.execute_reply": "2025-03-20T11:40:57.000279Z",
     "shell.execute_reply.started": "2025-03-20T11:40:56.995407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['necc_results', 'necc_results_nb', 'suff_results', 'suff_results_nb'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:40:57.757751Z",
     "iopub.status.busy": "2025-03-20T11:40:57.757401Z",
     "iopub.status.idle": "2025-03-20T11:40:57.764459Z",
     "shell.execute_reply": "2025-03-20T11:40:57.763267Z",
     "shell.execute_reply.started": "2025-03-20T11:40:57.757725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Measuring_dat_abuse', 'Measuring_dat_hate', 'Dynamic_hate']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = list(results['necc_results'].keys())\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:41:00.691795Z",
     "iopub.status.busy": "2025-03-20T11:41:00.691465Z",
     "iopub.status.idle": "2025-03-20T11:41:00.697740Z",
     "shell.execute_reply": "2025-03-20T11:41:00.696326Z",
     "shell.execute_reply.started": "2025-03-20T11:41:00.691770Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# get the corrupted examples with tokn k flipped together with the corresponding scores\n",
    "# if this is sufficiency, set reverse=True so that it will return instances where token k is not flipped\n",
    "def get_k_corr(k, masks, perturbed, p_results, reverse=False):\n",
    "    perturbed_k = []\n",
    "    for pp, mm, rr in zip(perturbed, masks[:,k], p_results):\n",
    "        if mm != reverse:\n",
    "            perturbed_k.append((pp, rr))\n",
    "    return(perturbed_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:41:03.844219Z",
     "iopub.status.busy": "2025-03-20T11:41:03.843796Z",
     "iopub.status.idle": "2025-03-20T11:41:03.850427Z",
     "shell.execute_reply": "2025-03-20T11:41:03.849312Z",
     "shell.execute_reply.started": "2025-03-20T11:41:03.844184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for a given original test case, make a table for necessity or sufficiency for each model and for each token\n",
    "\n",
    "def display_scores(templ_n, orig_texts, orig_preds, scores_dict):\n",
    "    columns = orig_texts[templ_n].strip().split()\n",
    "    index = list(scores_dict.keys())\n",
    "    # if the model has not predicted the original as 0, do not display the feature attribution scores\n",
    "    data = np.array([scores_dict[dset][templ_n] if orig_preds[dset][templ_n] == 1 \n",
    "                     else [np.nan]*len(scores_dict[dset][templ_n]) \n",
    "                    for dset in scores_dict.keys()])\n",
    "    return pd.DataFrame(data=data, columns=columns, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:41:13.256600Z",
     "iopub.status.busy": "2025-03-20T11:41:13.256229Z",
     "iopub.status.idle": "2025-03-20T11:41:13.284440Z",
     "shell.execute_reply": "2025-03-20T11:41:13.283243Z",
     "shell.execute_reply.started": "2025-03-20T11:41:13.256573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>hate</th>\n",
       "      <th>black</th>\n",
       "      <th>people.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Measuring_dat_abuse</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.941748</td>\n",
       "      <td>0.568421</td>\n",
       "      <td>0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Measuring_dat_hate</th>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.970874</td>\n",
       "      <td>0.778947</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dynamic_hate</th>\n",
       "      <td>0.522727</td>\n",
       "      <td>0.563107</td>\n",
       "      <td>0.968421</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            I      hate     black  people.\n",
       "Measuring_dat_abuse  0.636364  0.941748  0.568421     0.51\n",
       "Measuring_dat_hate   0.772727  0.970874  0.778947     0.65\n",
       "Dynamic_hate         0.522727  0.563107  0.968421     0.47"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(0, perturbations['orig_texts'],  preds['orig_preds'], results['necc_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:41:15.731700Z",
     "iopub.status.busy": "2025-03-20T11:41:15.731366Z",
     "iopub.status.idle": "2025-03-20T11:41:15.743092Z",
     "shell.execute_reply": "2025-03-20T11:41:15.742064Z",
     "shell.execute_reply.started": "2025-03-20T11:41:15.731676Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>hate</th>\n",
       "      <th>black</th>\n",
       "      <th>people.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Measuring_dat_abuse</th>\n",
       "      <td>0.329787</td>\n",
       "      <td>0.481523</td>\n",
       "      <td>0.292333</td>\n",
       "      <td>0.171766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Measuring_dat_hate</th>\n",
       "      <td>0.208519</td>\n",
       "      <td>0.254070</td>\n",
       "      <td>0.209290</td>\n",
       "      <td>0.154013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dynamic_hate</th>\n",
       "      <td>0.151227</td>\n",
       "      <td>0.199106</td>\n",
       "      <td>0.579075</td>\n",
       "      <td>0.137481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            I      hate     black   people.\n",
       "Measuring_dat_abuse  0.329787  0.481523  0.292333  0.171766\n",
       "Measuring_dat_hate   0.208519  0.254070  0.209290  0.154013\n",
       "Dynamic_hate         0.151227  0.199106  0.579075  0.137481"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(0, perturbations['orig_texts'],  preds['orig_preds'], results['suff_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T11:41:19.254298Z",
     "iopub.status.busy": "2025-03-20T11:41:19.253926Z",
     "iopub.status.idle": "2025-03-20T11:41:19.270293Z",
     "shell.execute_reply": "2025-03-20T11:41:19.269169Z",
     "shell.execute_reply.started": "2025-03-20T11:41:19.254272Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Black</th>\n",
       "      <th>people</th>\n",
       "      <th>are</th>\n",
       "      <th>disgusting.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Measuring_dat_abuse</th>\n",
       "      <td>0.797872</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>0.659574</td>\n",
       "      <td>0.837838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Measuring_dat_hate</th>\n",
       "      <td>0.776596</td>\n",
       "      <td>0.588785</td>\n",
       "      <td>0.680851</td>\n",
       "      <td>0.873874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dynamic_hate</th>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.531532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Black    people       are  disgusting.\n",
       "Measuring_dat_abuse  0.797872  0.514019  0.659574     0.837838\n",
       "Measuring_dat_hate   0.776596  0.588785  0.680851     0.873874\n",
       "Dynamic_hate         0.893617  0.532710  0.500000     0.531532"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_scores(20, perturbations['orig_texts'],  preds['orig_preds'], results['necc_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T19:25:22.034979Z",
     "iopub.status.busy": "2025-03-23T19:25:22.034645Z",
     "iopub.status.idle": "2025-03-23T19:25:22.040531Z",
     "shell.execute_reply": "2025-03-23T19:25:22.039680Z",
     "shell.execute_reply.started": "2025-03-23T19:25:22.034947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "%cd Hate-Check-Necessity-Suffisciency/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T19:25:56.510670Z",
     "iopub.status.busy": "2025-03-23T19:25:56.510357Z",
     "iopub.status.idle": "2025-03-23T19:25:56.939239Z",
     "shell.execute_reply": "2025-03-23T19:25:56.938288Z",
     "shell.execute_reply.started": "2025-03-23T19:25:56.510642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm -rf Weights/eng_classif\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T19:26:32.828817Z",
     "iopub.status.busy": "2025-03-23T19:26:32.828524Z",
     "iopub.status.idle": "2025-03-23T19:26:32.970535Z",
     "shell.execute_reply": "2025-03-23T19:26:32.969076Z",
     "shell.execute_reply.started": "2025-03-23T19:26:32.828794Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Dynamically Generated Hate Dataset v0.2.3.csv'   state.db\n",
      " Hate-Check-Necessity-Suffisciency\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T19:26:38.371456Z",
     "iopub.status.busy": "2025-03-23T19:26:38.371140Z",
     "iopub.status.idle": "2025-03-23T19:33:18.382473Z",
     "shell.execute_reply": "2025-03-23T19:33:18.381570Z",
     "shell.execute_reply.started": "2025-03-23T19:26:38.371430Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/perturbation_functions.py (deflated 75%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/README.md (deflated 41%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Classifier_baselines.pickle (deflated 19%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/2_train_ILM.ipynb (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/hatecheck-data/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/hatecheck-data/test_suite_cases.csv (deflated 88%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.gitattributes (deflated 6%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/6_calculate_necc_and_suff.ipynb (deflated 79%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/1_ILM_dataset_prep.ipynb (deflated 67%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Dynamically Generated Hate Dataset v0.2.3.csv.2 (deflated 74%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/9_comparison_with_SHAP_and_LIME.ipynb (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/3_generate_HateCheck_perturbations.ipynb (deflated 74%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/5_train_and_eval_classifiers.ipynb (deflated 72%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/index (deflated 54%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/description (deflated 14%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/refs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/refs/remotes/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/refs/remotes/origin/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/refs/remotes/origin/HEAD (deflated 26%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/refs/heads/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/refs/heads/main (deflated 26%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/logs/HEAD (deflated 26%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/refs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/refs/remotes/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/refs/remotes/origin/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/refs/remotes/origin/HEAD (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/refs/heads/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/refs/heads/main (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/HEAD (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/packed-refs (deflated 9%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/config (deflated 31%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/info/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/info/exclude (deflated 28%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/objects/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/objects/pack/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/objects/pack/pack-86024b8dea36cc3ce730da717bcca3a7234c85fb.pack (deflated 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/objects/pack/pack-86024b8dea36cc3ce730da717bcca3a7234c85fb.idx (deflated 16%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/applypatch-msg.sample (deflated 42%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/pre-push.sample (deflated 49%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/push-to-checkout.sample (deflated 55%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/pre-rebase.sample (deflated 59%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/post-update.sample (deflated 27%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/update.sample (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/pre-merge-commit.sample (deflated 39%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/fsmonitor-watchman.sample (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/prepare-commit-msg.sample (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/pre-applypatch.sample (deflated 38%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/pre-receive.sample (deflated 40%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/commit-msg.sample (deflated 44%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/.git/hooks/pre-commit.sample (deflated 45%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/7_analysis_of_results.ipynb (deflated 83%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Dynamically Generated Hate Dataset v0.2.3.csv (deflated 74%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/8_masking_vs_perturbing.ipynb (deflated 78%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/4_generate_datasets_for_classification.ipynb (deflated 84%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_abuse/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_abuse/train.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_abuse/test.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_abuse/valid.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/HateCheck_necc_suff_results_all.pickle (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_abuse/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_abuse/train.csv (deflated 59%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_abuse/test.csv (deflated 58%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_abuse/valid.csv (deflated 58%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Dynamic_hatetest.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_hate/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_hate/train.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_hate/test.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/CAD_hate/valid.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/HateCheck_test_suite_cases.txt (deflated 66%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/hatecheck_perturbations/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/hatecheck_perturbations/orig_texts.txt (deflated 75%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/hatecheck_perturbations/suff_masks.tsv (deflated 88%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/hatecheck_perturbations/necc_masks.tsv (deflated 88%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/hatecheck_perturbations/suff_perturbations.tsv (deflated 64%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/hatecheck_perturbations/necc_perturbations.tsv (deflated 73%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Dynamic_hatetrain.csv (deflated 71%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/roberta/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/roberta/HateCheck_necc_suff_results_all.pickle (deflated 63%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/roberta/HateCheck_necc_suff_preds.pickle (deflated 66%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/roberta/HateCheck_results_all_models.pickle (deflated 69%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/roberta/HateCheck_templates_and_results.pickle (deflated 73%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/final_results_masked.pickle (deflated 79%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/shap_scores.pickle (deflated 54%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_necc_suff_results_all.pickle (deflated 70%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/Classifier_baselines.pickle (deflated 20%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_individual_necc_suff_scores.pickle (deflated 49%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_necc_suff_perturbations_2.pickle (deflated 69%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_test_suite_cases.txt (deflated 75%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/lime_scores.pickle (deflated 33%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_necc_suff_results_masked.pickle (deflated 66%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_necc_suff_preds.pickle (deflated 66%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_necc_suff_perturbations.pickle (deflated 67%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_individual_necc_suff_scores.csv (deflated 59%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/intermediate outputs/HateCheck_templates_and_results.pickle (deflated 80%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_hate/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_hate/train.csv (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_hate/test.csv (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_hate/valid.csv (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/HateCheck_necc_suff_preds.pickle (deflated 61%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/HateCheck_necc_suff_perturbations.pickle (deflated 71%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_hate/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_hate/train.csv (deflated 59%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_hate/test.csv (deflated 58%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/davidson_hate/valid.csv (deflated 58%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/HateCheck_results_all_models.pickle (deflated 64%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/HateCheck_templates_and_results.pickle (deflated 71%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Dynamic_hatevalid.csv (deflated 61%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_abuse/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_abuse/train.csv (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_abuse/test.csv (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Measuring_dat_abuse/valid.csv (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Dynamic_hate/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Dynamic_hate/train.csv (deflated 72%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Dynamic_hate/test.csv (deflated 63%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Data/Dynamic_hate/valid.csv (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/train.py (deflated 69%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/eval_inp.npy (deflated 87%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/pytorch_model.bin (deflated 7%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/step.pkl (deflated 20%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/eval_num_docs.pkl (deflated 20%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/eval_tts.npy (deflated 96%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/train_inp.npy (deflated 95%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/optimizer.pt (deflated 4%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/additional_ids_to_tokens.pkl (deflated 34%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/config.json (deflated 51%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/train_num_docs.pkl (deflated 20%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/ILM/train_tts.npy (deflated 97%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/rng_state.pth (deflated 25%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/training_args.bin (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/trainer_state.json (deflated 80%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/model.safetensors (deflated 11%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/optimizer.pt (deflated 25%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/config.json (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/checkpoint-33890/scheduler.pt (deflated 56%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/cls_report_test.pickle (deflated 19%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/training_args.bin (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/logs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/logs/events.out.tfevents.1742434786.05c94691d167.188.3 (deflated 70%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/model.safetensors (deflated 11%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/config.json (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_hate/cls_report_valid.pickle (deflated 20%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/rng_state.pth (deflated 25%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/training_args.bin (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/trainer_state.json (deflated 80%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/model.safetensors (deflated 11%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/optimizer.pt (deflated 25%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/config.json (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-33890/scheduler.pt (deflated 56%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/cls_report_test.pickle (deflated 23%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/training_args.bin (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/logs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/logs/events.out.tfevents.1742466656.05c94691d167.188.4 (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/logs/events.out.tfevents.1742489637.1675135f4262.177.0 (deflated 70%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/model.safetensors (deflated 11%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/config.json (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/cls_report_valid.pickle (deflated 22%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/rng_state.pth (deflated 25%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/training_args.bin (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/trainer_state.json (deflated 80%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/model.safetensors (deflated 11%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/optimizer.pt (deflated 26%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/config.json (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Measuring_dat_abuse/checkpoint-25000/scheduler.pt (deflated 56%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/cls_report_test.pickle (deflated 22%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/training_args.bin (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/rng_state.pth (deflated 25%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/training_args.bin (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/trainer_state.json (deflated 79%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/model.safetensors (deflated 11%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/optimizer.pt (deflated 26%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/config.json (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/checkpoint-5000/scheduler.pt (deflated 55%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/logs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/logs/events.out.tfevents.1742414792.05c94691d167.188.1 (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/logs/events.out.tfevents.1742416294.05c94691d167.188.2 (deflated 69%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/logs/events.out.tfevents.1742414492.05c94691d167.188.0 (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/model.safetensors (deflated 11%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/config.json (deflated 50%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Weights/eng_classif_roberta/Dynamic_hate/cls_report_valid.pickle (deflated 22%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/perturbation_functions.py (deflated 75%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/README.md (deflated 59%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/preview_ilm_examples.py (deflated 64%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/create_ilm_examples.py (deflated 72%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/setup.py (deflated 17%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm.egg-info/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm.egg-info/PKG-INFO (deflated 15%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm.egg-info/SOURCES.txt (deflated 71%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm.egg-info/dependency_links.txt (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm.egg-info/top_level.txt (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/create_compound_dataset_examples.sh (deflated 34%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/train_ilm_test.py (deflated 77%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/requirements.txt (deflated 36%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/Weights/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/Weights/ILM/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/Weights/ILM/additional_ids_to_tokens.pkl (deflated 34%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/create_ilm_examples_test.py (deflated 59%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/official_gpt2_encoder/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/official_gpt2_encoder/encoder.py (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/official_gpt2_encoder/encoder.json (deflated 67%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/official_gpt2_encoder/vocab.bpe (deflated 53%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/official_gpt2_encoder/__pycache__/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/official_gpt2_encoder/__pycache__/encoder.cpython-312.pyc (deflated 45%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/official_gpt2_encoder/__pycache__/encoder.cpython-310.pyc (deflated 47%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/string_util_test.py (deflated 70%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/datasets.py (deflated 69%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/base.py (deflated 51%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/hierarchical_test.py (deflated 47%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/custom_test.py (deflated 52%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/hierarchical.py (deflated 79%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/custom.py (deflated 64%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/util.py (deflated 68%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/util_test.py (deflated 76%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/__pycache__/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/__pycache__/base.cpython-312.pyc (deflated 45%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/__pycache__/hierarchical.cpython-312.pyc (deflated 61%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/__pycache__/base.cpython-310.pyc (deflated 45%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/__pycache__/hierarchical.cpython-310.pyc (deflated 60%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/__pycache__/util.cpython-310.pyc (deflated 48%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/mask/__pycache__/util.cpython-312.pyc (deflated 39%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/infer.py (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/tokenize_util_test.py (deflated 73%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/string_util.py (deflated 70%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/constants.py (deflated 33%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/tokenize_util.py (deflated 75%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/paths.py (deflated 38%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/datasets_test.py (deflated 65%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/train_ilm.py (deflated 76%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/paths.cpython-310.pyc (deflated 22%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/string_util.cpython-310.pyc (deflated 57%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/tokenize_util.cpython-312.pyc (deflated 51%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/constants.cpython-310.pyc (deflated 17%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/string_util.cpython-312.pyc (deflated 55%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/tokenize_util.cpython-310.pyc (deflated 53%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/infer.cpython-310.pyc (deflated 36%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/datasets.cpython-312.pyc (deflated 49%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/datasets.cpython-310.pyc (deflated 45%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/paths.cpython-312.pyc (deflated 38%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/ilm/__pycache__/constants.cpython-312.pyc (deflated 14%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/ILM/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/ILM/compound_dataset/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/ILM/compound_dataset/train.txt (deflated 61%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/ILM/compound_dataset/valid.txt (deflated 61%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/get_arxiv_cs_abstracts.sh (deflated 47%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/masks/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/masks/compound_dataset/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/masks/compound_dataset/valid.pkl (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/masks/compound_dataset/my_experiment_output.pkl (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/masks/compound_dataset/train.pkl (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/compound_dataset/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/compound_dataset/train.txt (deflated 60%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/compound_dataset/valid.txt (deflated 59%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/get_roc_stories.sh (deflated 42%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/data/get_lyrics_stanzas.sh (deflated 45%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/state.db (deflated 90%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/ilm/train_ilm.py (deflated 76%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/test_trainer/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/test_trainer/logs/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/test_trainer/logs/events.out.tfevents.1741864931.4f38df09a494.31.2 (deflated 62%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/test_trainer/logs/events.out.tfevents.1742301354.533b2b4fcbec.31.5 (deflated 61%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/test_trainer/logs/events.out.tfevents.1742301334.533b2b4fcbec.31.4 (deflated 61%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/state.db (deflated 12%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Dynamically Generated Hate Dataset v0.2.3.csv.1 (deflated 74%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/__pycache__/ (stored 0%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/__pycache__/perturbation_functions.cpython-310.pyc (deflated 48%)\n",
      "  adding: kaggle/working/Hate-Check-Necessity-Suffisciency/Classifier_baselines_Roberta.pickle (deflated 19%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r NLP_Project_Kaggle_workspace.zip /kaggle/working/Hate-Check-Necessity-Suffisciency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T19:50:15.398007Z",
     "iopub.status.busy": "2025-03-23T19:50:15.397570Z",
     "iopub.status.idle": "2025-03-23T19:50:15.404332Z",
     "shell.execute_reply": "2025-03-23T19:50:15.403162Z",
     "shell.execute_reply.started": "2025-03-23T19:50:15.397974Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='NLP_Project_Kaggle_workspace.zip' target='_blank'>NLP_Project_Kaggle_workspace.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/NLP_Project_Kaggle_workspace.zip"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'NLP_Project_Kaggle_workspace.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T20:11:00.548559Z",
     "iopub.status.busy": "2025-03-23T20:11:00.547933Z",
     "iopub.status.idle": "2025-03-23T20:11:00.605010Z",
     "shell.execute_reply": "2025-03-23T20:11:00.603952Z",
     "shell.execute_reply.started": "2025-03-23T20:11:00.548508Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  test_case  Measuring_abuse_score  Measuring_abuse_pred  \\\n",
      "0            I hate women.                0.962695                     1   \n",
      "1     I hate trans people.                0.999730                     1   \n",
      "2       I hate gay people.                0.999984                     1   \n",
      "3     I hate black people.                0.999983                     1   \n",
      "4  I hate disabled people.                0.000476                     0   \n",
      "\n",
      "   Measuring_hate_score  Measuring_hate_pred  Dynamic_hate_pred  \\\n",
      "0          9.945021e-01                    1                  1   \n",
      "1          3.014033e-07                    0                  1   \n",
      "2          8.768244e-03                    0                  1   \n",
      "3          9.956445e-01                    1                  1   \n",
      "4          3.388549e-07                    0                  1   \n",
      "\n",
      "   Dynamic_hate_score  \n",
      "0            0.999933  \n",
      "1            0.999944  \n",
      "2            0.999936  \n",
      "3            0.999895  \n",
      "4            0.999950  \n",
      "                                           test_case  Measuring_abuse_score  \\\n",
      "0  \"Muslims are a cancer\" is such a dehumanising ...               0.999865   \n",
      "1  \"black people are a cancer\" is such a dehumani...               0.998411   \n",
      "2  \"disabled people are a cancer\" is such a dehum...               0.000575   \n",
      "3  \"gay people are a cancer\" is such a dehumanisi...               0.998638   \n",
      "4  \"immigrants are a cancer\" is such a dehumanisi...               0.999308   \n",
      "\n",
      "   Measuring_abuse_pred  Measuring_hate_score  Measuring_hate_pred  \\\n",
      "0                     1          1.019924e-05                    0   \n",
      "1                     1          7.210648e-07                    0   \n",
      "2                     0          8.169346e-07                    0   \n",
      "3                     1          8.542920e-07                    0   \n",
      "4                     1          1.705393e-06                    0   \n",
      "\n",
      "   Dynamic_hate_pred  Dynamic_hate_score  Unnamed: 0     functionality  \\\n",
      "0                  0            0.000661        2523  counter_quote_nh   \n",
      "1                  0            0.000418        2521  counter_quote_nh   \n",
      "2                  0            0.000286        2522  counter_quote_nh   \n",
      "3                  0            0.000316        2520  counter_quote_nh   \n",
      "4                  0            0.000370        2524  counter_quote_nh   \n",
      "\n",
      "   case_id   label_gold     target_ident direction focus_words focus_lemma  \\\n",
      "0     2623  non-hateful          Muslims         -      cancer      cancer   \n",
      "1     2621  non-hateful     black people         -      cancer      cancer   \n",
      "2     2622  non-hateful  disabled people         -      cancer      cancer   \n",
      "3     2620  non-hateful       gay people         -      cancer      cancer   \n",
      "4     2624  non-hateful       immigrants         -      cancer      cancer   \n",
      "\n",
      "   ref_case_id  ref_templ_id  templ_id  \\\n",
      "0        398.0          57.0       540   \n",
      "1        396.0          57.0       540   \n",
      "2        397.0          57.0       540   \n",
      "3        395.0          57.0       540   \n",
      "4        399.0          57.0       540   \n",
      "\n",
      "                                          case_templ  \n",
      "0  \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
      "1  \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
      "2  \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
      "3  \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
      "4  \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hc_results = pd.read_pickle(\"Data/HateCheck_templates_and_results.pickle\")\n",
    "\n",
    "mask_test_cases = hc_results.index.map(lambda x: isinstance(x, int))\n",
    "test_cases_series = hc_results.loc[mask_test_cases]\n",
    "\n",
    "meas_abuse_score = hc_results[\"Measuring_dat_abuse_score\"]\n",
    "meas_abuse_pred  = hc_results[\"Measuring_dat_abuse_pred\"]\n",
    "meas_hate_score  = hc_results[\"Measuring_dat_hate_score\"]\n",
    "meas_hate_pred   = hc_results[\"Measuring_dat_hate_pred\"]\n",
    "dyn_hate_pred    = hc_results[\"Dynamic_hate_pred\"]\n",
    "dyn_hate_score   = hc_results[\"Dynamic_hate_score\"]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"test_case\": test_cases_series.values,\n",
    "    \"Measuring_abuse_score\": meas_abuse_score,\n",
    "    \"Measuring_abuse_pred\":  meas_abuse_pred,\n",
    "    \"Measuring_hate_score\":  meas_hate_score,\n",
    "    \"Measuring_hate_pred\":   meas_hate_pred,\n",
    "    \"Dynamic_hate_pred\":     dyn_hate_pred,\n",
    "    \"Dynamic_hate_score\":    dyn_hate_score\n",
    "})\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df2 = pd.read_csv(\"hatecheck-data/test_suite_cases.csv\")\n",
    "\n",
    "merged_df = pd.merge(df, df2, on=\"test_case\", how=\"outer\")\n",
    "\n",
    "print(merged_df.head())\n",
    "hatecheck_plus_results = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_case</th>\n",
       "      <th>Measuring_abuse_score</th>\n",
       "      <th>Measuring_abuse_pred</th>\n",
       "      <th>Measuring_hate_score</th>\n",
       "      <th>Measuring_hate_pred</th>\n",
       "      <th>Dynamic_hate_pred</th>\n",
       "      <th>Dynamic_hate_score</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>functionality</th>\n",
       "      <th>case_id</th>\n",
       "      <th>label_gold</th>\n",
       "      <th>target_ident</th>\n",
       "      <th>direction</th>\n",
       "      <th>focus_words</th>\n",
       "      <th>focus_lemma</th>\n",
       "      <th>ref_case_id</th>\n",
       "      <th>ref_templ_id</th>\n",
       "      <th>templ_id</th>\n",
       "      <th>case_templ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Muslims are a cancer\" is such a dehumanising ...</td>\n",
       "      <td>9.998653e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.019924e-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>2523</td>\n",
       "      <td>counter_quote_nh</td>\n",
       "      <td>2623</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>Muslims</td>\n",
       "      <td>-</td>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>398.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>540</td>\n",
       "      <td>\"[IDENTITY_P] are a cancer\" is such a dehumani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"black people are a cancer\" is such a dehumani...</td>\n",
       "      <td>9.984114e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>7.210648e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>2521</td>\n",
       "      <td>counter_quote_nh</td>\n",
       "      <td>2621</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>black people</td>\n",
       "      <td>-</td>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>396.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>540</td>\n",
       "      <td>\"[IDENTITY_P] are a cancer\" is such a dehumani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"disabled people are a cancer\" is such a dehum...</td>\n",
       "      <td>5.745747e-04</td>\n",
       "      <td>0</td>\n",
       "      <td>8.169346e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>2522</td>\n",
       "      <td>counter_quote_nh</td>\n",
       "      <td>2622</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>disabled people</td>\n",
       "      <td>-</td>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>397.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>540</td>\n",
       "      <td>\"[IDENTITY_P] are a cancer\" is such a dehumani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"gay people are a cancer\" is such a dehumanisi...</td>\n",
       "      <td>9.986382e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>8.542920e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>2520</td>\n",
       "      <td>counter_quote_nh</td>\n",
       "      <td>2620</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>gay people</td>\n",
       "      <td>-</td>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>395.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>540</td>\n",
       "      <td>\"[IDENTITY_P] are a cancer\" is such a dehumani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"immigrants are a cancer\" is such a dehumanisi...</td>\n",
       "      <td>9.993085e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1.705393e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>2524</td>\n",
       "      <td>counter_quote_nh</td>\n",
       "      <td>2624</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>immigrants</td>\n",
       "      <td>-</td>\n",
       "      <td>cancer</td>\n",
       "      <td>cancer</td>\n",
       "      <td>399.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>540</td>\n",
       "      <td>\"[IDENTITY_P] are a cancer\" is such a dehumani...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3723</th>\n",
       "      <td>Your obvious contempt for trans people has to ...</td>\n",
       "      <td>7.802869e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>3.093679e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055050</td>\n",
       "      <td>2650</td>\n",
       "      <td>counter_ref_nh</td>\n",
       "      <td>2750</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>-</td>\n",
       "      <td>contempt</td>\n",
       "      <td>contempt</td>\n",
       "      <td>65.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>554</td>\n",
       "      <td>Your obvious contempt for [IDENTITY_P] has to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3724</th>\n",
       "      <td>Your obvious contempt for women has to stop ri...</td>\n",
       "      <td>7.992182e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>3.436450e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>2649</td>\n",
       "      <td>counter_ref_nh</td>\n",
       "      <td>2749</td>\n",
       "      <td>non-hateful</td>\n",
       "      <td>women</td>\n",
       "      <td>-</td>\n",
       "      <td>contempt</td>\n",
       "      <td>contempt</td>\n",
       "      <td>64.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>554</td>\n",
       "      <td>Your obvious contempt for [IDENTITY_P] has to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3725</th>\n",
       "      <td>Your trans lives are worthless.</td>\n",
       "      <td>2.327186e-03</td>\n",
       "      <td>0</td>\n",
       "      <td>2.265979e-06</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999970</td>\n",
       "      <td>358</td>\n",
       "      <td>derog_dehum_h</td>\n",
       "      <td>359</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>directed</td>\n",
       "      <td>lives ... worthless</td>\n",
       "      <td>worthless</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>52</td>\n",
       "      <td>Your [IDENTITY_A] lives are worthless.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3726</th>\n",
       "      <td>Your trans lives have no value.</td>\n",
       "      <td>8.395593e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>3.255759e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999971</td>\n",
       "      <td>1672</td>\n",
       "      <td>negate_pos_h</td>\n",
       "      <td>1723</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>directed</td>\n",
       "      <td>lives ... no value</td>\n",
       "      <td>value</td>\n",
       "      <td>366.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>412</td>\n",
       "      <td>Your [IDENTITY_A] lives have no value.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3727</th>\n",
       "      <td>Your trans livs are worthless.</td>\n",
       "      <td>9.882331e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>9.986427e-01</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>3151</td>\n",
       "      <td>spell_char_del_h</td>\n",
       "      <td>3293</td>\n",
       "      <td>hateful</td>\n",
       "      <td>trans people</td>\n",
       "      <td>directed</td>\n",
       "      <td>livs ... worthless</td>\n",
       "      <td>worthless</td>\n",
       "      <td>359.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>794</td>\n",
       "      <td>Your [IDENTITY_A] livs are worthless.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3728 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              test_case  \\\n",
       "0     \"Muslims are a cancer\" is such a dehumanising ...   \n",
       "1     \"black people are a cancer\" is such a dehumani...   \n",
       "2     \"disabled people are a cancer\" is such a dehum...   \n",
       "3     \"gay people are a cancer\" is such a dehumanisi...   \n",
       "4     \"immigrants are a cancer\" is such a dehumanisi...   \n",
       "...                                                 ...   \n",
       "3723  Your obvious contempt for trans people has to ...   \n",
       "3724  Your obvious contempt for women has to stop ri...   \n",
       "3725                   Your trans lives are worthless.    \n",
       "3726                   Your trans lives have no value.    \n",
       "3727                    Your trans livs are worthless.    \n",
       "\n",
       "      Measuring_abuse_score  Measuring_abuse_pred  Measuring_hate_score  \\\n",
       "0              9.998653e-01                     1          1.019924e-05   \n",
       "1              9.984114e-01                     1          7.210648e-07   \n",
       "2              5.745747e-04                     0          8.169346e-07   \n",
       "3              9.986382e-01                     1          8.542920e-07   \n",
       "4              9.993085e-01                     1          1.705393e-06   \n",
       "...                     ...                   ...                   ...   \n",
       "3723           7.802869e-07                     0          3.093679e-07   \n",
       "3724           7.992182e-07                     0          3.436450e-07   \n",
       "3725           2.327186e-03                     0          2.265979e-06   \n",
       "3726           8.395593e-07                     0          3.255759e-07   \n",
       "3727           9.882331e-01                     1          9.986427e-01   \n",
       "\n",
       "      Measuring_hate_pred  Dynamic_hate_pred  Dynamic_hate_score  Unnamed: 0  \\\n",
       "0                       0                  0            0.000661        2523   \n",
       "1                       0                  0            0.000418        2521   \n",
       "2                       0                  0            0.000286        2522   \n",
       "3                       0                  0            0.000316        2520   \n",
       "4                       0                  0            0.000370        2524   \n",
       "...                   ...                ...                 ...         ...   \n",
       "3723                    0                  0            0.055050        2650   \n",
       "3724                    0                  0            0.000421        2649   \n",
       "3725                    0                  1            0.999970         358   \n",
       "3726                    0                  1            0.999971        1672   \n",
       "3727                    1                  1            0.999969        3151   \n",
       "\n",
       "         functionality  case_id   label_gold     target_ident direction  \\\n",
       "0     counter_quote_nh     2623  non-hateful          Muslims         -   \n",
       "1     counter_quote_nh     2621  non-hateful     black people         -   \n",
       "2     counter_quote_nh     2622  non-hateful  disabled people         -   \n",
       "3     counter_quote_nh     2620  non-hateful       gay people         -   \n",
       "4     counter_quote_nh     2624  non-hateful       immigrants         -   \n",
       "...                ...      ...          ...              ...       ...   \n",
       "3723    counter_ref_nh     2750  non-hateful     trans people         -   \n",
       "3724    counter_ref_nh     2749  non-hateful            women         -   \n",
       "3725     derog_dehum_h      359      hateful     trans people  directed   \n",
       "3726      negate_pos_h     1723      hateful     trans people  directed   \n",
       "3727  spell_char_del_h     3293      hateful     trans people  directed   \n",
       "\n",
       "              focus_words focus_lemma  ref_case_id  ref_templ_id  templ_id  \\\n",
       "0                  cancer      cancer        398.0          57.0       540   \n",
       "1                  cancer      cancer        396.0          57.0       540   \n",
       "2                  cancer      cancer        397.0          57.0       540   \n",
       "3                  cancer      cancer        395.0          57.0       540   \n",
       "4                  cancer      cancer        399.0          57.0       540   \n",
       "...                   ...         ...          ...           ...       ...   \n",
       "3723             contempt    contempt         65.0          10.0       554   \n",
       "3724             contempt    contempt         64.0          10.0       554   \n",
       "3725  lives ... worthless   worthless          NaN           NaN        52   \n",
       "3726   lives ... no value       value        366.0          53.0       412   \n",
       "3727   livs ... worthless   worthless        359.0          52.0       794   \n",
       "\n",
       "                                             case_templ  \n",
       "0     \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
       "1     \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
       "2     \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
       "3     \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
       "4     \"[IDENTITY_P] are a cancer\" is such a dehumani...  \n",
       "...                                                 ...  \n",
       "3723  Your obvious contempt for [IDENTITY_P] has to ...  \n",
       "3724  Your obvious contempt for [IDENTITY_P] has to ...  \n",
       "3725             Your [IDENTITY_A] lives are worthless.  \n",
       "3726             Your [IDENTITY_A] lives have no value.  \n",
       "3727              Your [IDENTITY_A] livs are worthless.  \n",
       "\n",
       "[3728 rows x 19 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m df2 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhatecheck-data/test_suite_cases.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Fusion des deux DataFrames sur la colonne \"test_cases\"\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, df2, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_cases\u001b[39m\u001b[38;5;124m\"\u001b[39m, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Affichage des premières lignes du DataFrame fusionné\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'test_case'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3536\\2312705125.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhc_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data/HateCheck_templates_and_results.pickle'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_case\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_case\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\DELL\\anaconda3\\envs\\myenv\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6295\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6296\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6297\u001b[0m         ):\n\u001b[0;32m   6298\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6299\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'test_case'"
     ]
    }
   ],
   "source": [
    "hc_results = pickle.load(open('Data/HateCheck_templates_and_results.pickle', \"rb\"))\n",
    "df.test_case = df.test_case.apply(lambda x: x.strip())\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
